{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99061,"status":"ok","timestamp":1686069509784,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"zh_GBPkojG0w","outputId":"10d55245-b1b2-4431-ed1a-cc7b37e08e8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option('display.max_rows', None)\n","pd.options.mode.chained_assignment = None  # default='warn' surpress chain assignment warning\n","\n","from pathlib import Path\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#import nltk\n","#nltk.download('all') \n","\n","import numpy as np\n","\n","# import re                                  \n","# import string\n","# from nltk.corpus import wordnet\n","# from nltk.corpus import brown\n","# from nltk.corpus import stopwords \n","# from nltk.stem import PorterStemmer\n","# from nltk.tokenize import TweetTokenizer\n","# from nltk.stem import WordNetLemmatizer\n","# from sklearn.feature_extraction.text import CountVectorizer\n","# from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import coo_matrix, hstack\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","#from textblob import Word\n","\n","#from collections import Counter\n","\n","#import gensim\n","\n","from numpy import savetxt\n","import numpy as np\n","import scipy.sparse\n","\n","# XGBoost\n","import xgboost as xgb\n","from xgboost import XGBClassifier\n","\n","# sklearn \n","from sklearn import model_selection\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import f1_score\n","from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n","from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.compose import ColumnTransformer\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# matplotlib and seaborn for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#import torch\n","\n","# turn off userwarnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","#pip install tokenization"]},{"cell_type":"markdown","metadata":{"id":"DkADw6uebf9X"},"source":["#### File paths"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4065,"status":"ok","timestamp":1686069513828,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"f1T9r2hujtRu"},"outputs":[],"source":["from scipy.sparse import load_npz\n","\n","# Paths to features from each extraction method\n","# NepalEQ_hashtag_arr_matrix_path = '/content/drive/MyDrive/NepalEQ_hashtag_arr_matrix.npy'\n","# QueenslandFLD_hashtag_arr_matrix_path = '/content/drive/MyDrive/QueenslandFLD_hashtag_arr_matrix.npy'\n","# NepalEQ_retweet_arr_matrix_path = '/content/drive/MyDrive/NepalEQ_retweet_arr_matrix.npy'\n","# QueenslandFLD_retweet_arr_matrix_path = '/content/drive/MyDrive/QueenslandFLD_retweet_arr_matrix.npy'\n","# NepalEQ_df_em_1gram_matrix_path = '/content/drive/MyDrive/NepalEQ_df_em_1gram_matrix.npy'\n","# QueenslandFLD_df_em_1gram_matrix_path = '/content/drive/MyDrive/QueenslandFLD_df_em_1gram_matrix.npy'\n","# NepalEQ_POS_matrix_path = '/content/drive/MyDrive/NepalEQ_POS_matrix.npy'\n","# QueenslandFLD_POS_matrix_path = '/content/drive/MyDrive/QueenslandFLD_POS_matrix.npy'\n","NepalEQ_df_embeddings_fasttext_matrix_path = '/content/drive/MyDrive/NepalEQ_df_embeddings_fasttext_matrix.npy'\n","# QueenslandFLD_df_embeddings_fasttext_matrix_path = '/content/drive/MyDrive/QueenslandFLD_df_embeddings_fasttext_matrix.npy'\n","NepalEQ_df_embeddings_word2vec_matrix_path = '/content/drive/MyDrive/NepalEQ_df_embeddings_word2vec_matrix.npy'\n","# QueenslandFLD_df_embeddings_word2vec_matrix_path = '/content/drive/MyDrive/QueenslandFLD_df_embeddings_word2vec_matrix.npy'\n","NepalEQ_df_embeddings_glove_matrix_path = '/content/drive/MyDrive/NepalEQ_df_embeddings_glove_matrix.npy'\n","# QueenslandFLD_df_embeddings_glove_matrix_path = '/content/drive/MyDrive/QueenslandFLD_df_embeddings_glove_matrix.npy'\n","NepalEQ_df_bert_input_matrix_path = '/content/drive/MyDrive/NepalEQ_df_bert_input_matrix.npy'\n","# QueenslandFLD_df_bert_input_matrix_path = '/content/drive/MyDrive/QueenslandFLD_df_bert_input_matrix.npy'\n","# NepalEQ_df_tfidf_1gram_matrix_path = '/content/drive/MyDrive/NepalEQ_df_tfidf_1gram_matrix.npy'\n","# QueenslandFLD_df_tfidf_1gram_matrix_path = '/content/drive/MyDrive/QueenslandFLD_df_tfidf_1gram_matrix.npy'\n","# NepalEQ_TFIDF_HT_RT_POS_FT_hstack_path = '/content/drive/MyDrive/NepalEQ_TFIDF_HT_RT_POS_FT_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_RT_POS_FT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_RT_POS_FT_hstack.npy'\n","# NepalEQ_TFIDF_HT_RT_POS_w2v_hstack_path = '/content/drive/MyDrive/NepalEQ_TFIDF_HT_RT_POS_w2v_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_RT_POS_w2v_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_RT_POS_w2v_hstack.npy'\n","# NepalEQ_TFIDF_HT_RT_POS_glove_hstack_path = '/content/drive/MyDrive/NepalEQ_TFIDF_HT_RT_POS_glove_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_RT_POS_glove_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_RT_POS_glove_hstack.npy'\n","# NepalEQ_TFIDF_HT_RT_POS_bert_hstack_path = '/content/drive/MyDrive/NepalEQ_TFIDF_HT_RT_POS_bert_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_RT_POS_bert_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_RT_POS_bert_hstack.npy'\n","# NepalEQ_TFIDF_HT_POS_hstack_path = '/content/drive/MyDrive/NepalEQ_TFIDF_HT_POS_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_POS_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_POS_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_hstack.npy'\n","# QueenslandFLD_HT_POS_hstack_path = '/content/drive/MyDrive/QueenslandFLD_HT_POS_hstack.npy'\n","# QueenslandFLD_HT_FT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_HT_FT_hstack.npy'\n","# QueenslandFLD_HT_glove_hstack_path = '/content/drive/MyDrive/QueenslandFLD_HT_glove_hstack.npy'\n","# QueenslandFLD_HT_w2v_hstack_path = '/content/drive/MyDrive/QueenslandFLD_HT_w2v_hstack.npy'\n","# QueenslandFLD_HT_bert_hstack_path = '/content/drive/MyDrive/QueenslandFLD_HT_bert_hstack.npy'\n","# QueenslandFLD_POS_FT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_POS_FT_hstack.npy'\n","# QueenslandFLD_POS_glove_hstack_path = '/content/drive/MyDrive/QueenslandFLD_POS_glove_hstack.npy'\n","# QueenslandFLD_POS_w2v_hstack_path = '/content/drive/MyDrive/QueenslandFLD_POS_w2v_hstack.npy'\n","# QueenslandFLD_POS_bert_hstack_path = '/content/drive/MyDrive/QueenslandFLD_POS_bert_hstack.npy'\n","# QueenslandFLD_TFIDF_POS_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_POS_hstack.npy'\n","# QueenslandFLD_TFIDF_FT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_FT_hstack.npy'\n","# QueenslandFLD_TFIDF_glove_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_glove_hstack.npy'\n","# QueenslandFLD_TFIDF_w2v_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_w2v_hstack.npy'\n","# QueenslandFLD_TFIDF_bert_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_bert_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_POS_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_POS_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_FT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_FT_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_glove_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_glove_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_w2v_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_w2v_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_bert_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_bert_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_POS_FT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_POS_FT_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_POS_glove_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_POS_glove_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_POS_w2v_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_POS_w2v_hstack.npy'\n","# QueenslandFLD_TFIDF_HT_POS_BERT_hstack_path = '/content/drive/MyDrive/QueenslandFLD_TFIDF_HT_POS_BERT_hstack.npy'\n","\n","\n","\n","\n","# Paths to traget variables \n","NepalEQ_df_target_path = '/content/drive/MyDrive/NepalEQ_df_label.npy'\n","QueenslandFLD_df_target = '/content/drive/MyDrive/QueenslandFLD_df_label.npy'\n","\n","# load features\n","# NepalEQ_hashtag = np.load(NepalEQ_hashtag_arr_matrix_path)\n","# QueenslandFLD_hashtag = np.load(QueenslandFLD_hashtag_arr_matrix_path)\n","# NepalEQ_retweet = np.load(NepalEQ_retweet_arr_matrix_path)\n","# QueenslandFLD_retweet = np.load(QueenslandFLD_retweet_arr_matrix_path)\n","# NepalEQ_df_em_1gram = np.load(NepalEQ_df_em_1gram_matrix_path)\n","# QueenslandFLD_df_em_1gram = np.load(QueenslandFLD_df_em_1gram_matrix_path)\n","# NepalEQ_POS = np.load(NepalEQ_POS_matrix_path)\n","# QueenslandFLD_POS = np.load(QueenslandFLD_POS_matrix_path)\n","NepalEQ_df_embeddings_fasttext = np.load(NepalEQ_df_embeddings_fasttext_matrix_path)\n","# QueenslandFLD_df_embeddings_fasttext = np.load(QueenslandFLD_df_embeddings_fasttext_matrix_path)\n","NepalEQ_df_embeddings_word2vec = np.load(NepalEQ_df_embeddings_word2vec_matrix_path)\n","# QueenslandFLD_df_embeddings_word2vec = np.load(QueenslandFLD_df_embeddings_word2vec_matrix_path)\n","NepalEQ_df_embeddings_glove = np.load(NepalEQ_df_embeddings_glove_matrix_path)\n","# QueenslandFLD_df_embeddings_glove = np.load(QueenslandFLD_df_embeddings_glove_matrix_path)\n","NepalEQ_df_bert_input = np.load(NepalEQ_df_bert_input_matrix_path)\n","# QueenslandFLD_df_bert_input = np.load(QueenslandFLD_df_bert_input_matrix_path)\n","# NepalEQ_df_tfidf_1gram = np.load(NepalEQ_df_tfidf_1gram_matrix_path)\n","# QueenslandFLD_df_tfidf_1gram = np.load(QueenslandFLD_df_tfidf_1gram_matrix_path)\n","# NepalEQ_TFIDF_HT_RT_POS_FT = np.load(NepalEQ_TFIDF_HT_RT_POS_FT_hstack_path)\n","# QueenslandFLD_TFIDF_HT_RT_POS_FT = np.load(QueenslandFLD_TFIDF_HT_RT_POS_FT_hstack_path)\n","# NepalEQ_TFIDF_HT_RT_POS_w2v = np.load(NepalEQ_TFIDF_HT_RT_POS_w2v_hstack_path)\n","# QueenslandFLD_TFIDF_HT_RT_POS_w2v = np.load(QueenslandFLD_TFIDF_HT_RT_POS_w2v_hstack_path)\n","# NepalEQ_TFIDF_HT_RT_POS_glove = np.load(NepalEQ_TFIDF_HT_RT_POS_glove_hstack_path)\n","# QueenslandFLD_TFIDF_HT_RT_POS_glove = np.load(QueenslandFLD_TFIDF_HT_RT_POS_glove_hstack_path)\n","# NepalEQ_TFIDF_HT_RT_POS_bert = np.load(NepalEQ_TFIDF_HT_RT_POS_bert_hstack_path)\n","# QueenslandFLD_TFIDF_HT_RT_POS_bert = np.load(QueenslandFLD_TFIDF_HT_RT_POS_bert_hstack_path)\n","# NepalEQ_TFIDF_HT_POS = np.load(NepalEQ_TFIDF_HT_POS_hstack_path)\n","# QueenslandFLD_TFIDF_HT_POS = np.load(QueenslandFLD_TFIDF_HT_POS_hstack_path)\n","# QueenslandFLD_TFIDF_HT_hstack = np.load(QueenslandFLD_TFIDF_HT_hstack_path)\n","# QueenslandFLD_HT_POS_hstack = np.load(QueenslandFLD_HT_POS_hstack_path)\n","# QueenslandFLD_HT_FT_hstack = np.load(QueenslandFLD_HT_FT_hstack_path)\n","# QueenslandFLD_HT_glove_hstack = np.load(QueenslandFLD_HT_glove_hstack_path)\n","# QueenslandFLD_HT_w2v_hstack = np.load(QueenslandFLD_HT_w2v_hstack_path)\n","# QueenslandFLD_HT_bert_hstack = np.load(QueenslandFLD_HT_bert_hstack_path)\n","# QueenslandFLD_POS_FT_hstack = np.load(QueenslandFLD_POS_FT_hstack_path)\n","# QueenslandFLD_POS_glove_hstack = np.load(QueenslandFLD_POS_glove_hstack_path)\n","# QueenslandFLD_POS_w2v_hstack = np.load(QueenslandFLD_POS_w2v_hstack_path)\n","# QueenslandFLD_POS_bert_hstack = np.load(QueenslandFLD_POS_bert_hstack_path)\n","# QueenslandFLD_TFIDF_POS_hstack = np.load(QueenslandFLD_TFIDF_POS_hstack_path)\n","# QueenslandFLD_TFIDF_FT_hstack = np.load(QueenslandFLD_TFIDF_FT_hstack_path)\n","# QueenslandFLD_TFIDF_glove_hstack = np.load(QueenslandFLD_TFIDF_glove_hstack_path)\n","# QueenslandFLD_TFIDF_w2v_hstack = np.load(QueenslandFLD_TFIDF_w2v_hstack_path)\n","# QueenslandFLD_TFIDF_bert_hstack = np.load(QueenslandFLD_TFIDF_bert_hstack_path)\n","# QueenslandFLD_TFIDF_HT_POS_hstack = np.load(QueenslandFLD_TFIDF_HT_POS_hstack_path)\n","# QueenslandFLD_TFIDF_HT_FT_hstack = np.load(QueenslandFLD_TFIDF_HT_FT_hstack_path)\n","# QueenslandFLD_TFIDF_HT_glove_hstack = np.load(QueenslandFLD_TFIDF_HT_glove_hstack_path)\n","# QueenslandFLD_TFIDF_HT_w2v_hstack = np.load(QueenslandFLD_TFIDF_HT_w2v_hstack_path)\n","# QueenslandFLD_TFIDF_HT_bert_hstack = np.load(QueenslandFLD_TFIDF_HT_bert_hstack_path)\n","# QueenslandFLD_TFIDF_HT_POS_FT_hstack = np.load(QueenslandFLD_TFIDF_HT_POS_FT_hstack_path)\n","# QueenslandFLD_TFIDF_HT_POS_glove_hstack = np.load(QueenslandFLD_TFIDF_HT_POS_glove_hstack_path)\n","# QueenslandFLD_TFIDF_HT_POS_w2v_hstack = np.load(QueenslandFLD_TFIDF_HT_POS_w2v_hstack_path)\n","# QueenslandFLD_TFIDF_HT_POS_BERT_hstack = np.load(QueenslandFLD_TFIDF_HT_POS_BERT_hstack_path)\n","\n","\n","# load targets\n","NepalEQ_df_target = np.load(NepalEQ_df_target_path)\n","# QueenslandFLD_target = np.load(QueenslandFLD_df_target)\n","\n","\n","df_list = [NepalEQ_df_embeddings_word2vec, NepalEQ_df_embeddings_fasttext, \n","           NepalEQ_df_embeddings_glove, NepalEQ_df_bert_input]\n","df_names = ['NepalEQ_df_embeddings_word2vec', 'NepalEQ_df_embeddings_fasttext', \n","            'NepalEQ_df_embeddings_glove', 'NepalEQ_df_bert_input']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9ax-BpPZGAwK"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 11s 29ms/step - loss: 0.7151 - accuracy: 0.4997 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7064 - accuracy: 0.4991 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7049 - accuracy: 0.5051 - val_loss: 0.6933 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7082 - accuracy: 0.4943 - val_loss: 0.6940 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7070 - accuracy: 0.4937 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7042 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7050 - accuracy: 0.5094 - val_loss: 0.6951 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7025 - accuracy: 0.5160 - val_loss: 0.6946 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7074 - accuracy: 0.4964 - val_loss: 0.6993 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7025 - accuracy: 0.5173 - val_loss: 0.7016 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7033 - accuracy: 0.5080 - val_loss: 0.6985 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7069 - accuracy: 0.4893 - val_loss: 0.6979 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7024 - accuracy: 0.5134 - val_loss: 0.7012 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7032 - accuracy: 0.5100 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7056 - accuracy: 0.4897 - val_loss: 0.6978 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7035 - accuracy: 0.5009 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7037 - accuracy: 0.4945 - val_loss: 0.7081 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7008 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7035 - accuracy: 0.5026 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7027 - accuracy: 0.5117 - val_loss: 0.6965 - val_accuracy: 0.4773\n","65/65 [==============================] - 1s 5ms/step\n","Epoch 1/20\n","76/76 [==============================] - 10s 55ms/step - loss: 0.6942 - accuracy: 0.4997 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6930 - accuracy: 0.5131 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6931 - accuracy: 0.5169 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6922 - accuracy: 0.5179 - val_loss: 0.6967 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6936 - accuracy: 0.5154 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 3s 40ms/step - loss: 0.6931 - accuracy: 0.5208 - val_loss: 0.6939 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6927 - accuracy: 0.5177 - val_loss: 0.6933 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6933 - accuracy: 0.5103 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 3s 40ms/step - loss: 0.6929 - accuracy: 0.5183 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6927 - accuracy: 0.5216 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.6922 - accuracy: 0.5204 - val_loss: 0.6947 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 3s 40ms/step - loss: 0.6929 - accuracy: 0.5183 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6929 - accuracy: 0.5202 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 3s 40ms/step - loss: 0.6923 - accuracy: 0.5210 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 5s 70ms/step - loss: 0.6925 - accuracy: 0.5192 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6928 - accuracy: 0.5179 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6926 - accuracy: 0.5231 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6925 - accuracy: 0.5169 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.6925 - accuracy: 0.5194 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6929 - accuracy: 0.5200 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 14ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 25ms/step - loss: 0.7174 - accuracy: 0.5053 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7060 - accuracy: 0.5094 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7078 - accuracy: 0.4991 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7049 - accuracy: 0.5123 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7096 - accuracy: 0.4962 - val_loss: 0.6952 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7086 - accuracy: 0.4931 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7087 - accuracy: 0.4974 - val_loss: 0.7002 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7100 - accuracy: 0.4945 - val_loss: 0.7049 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7069 - accuracy: 0.5098 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7081 - accuracy: 0.4962 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7062 - accuracy: 0.5067 - val_loss: 0.7009 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7103 - accuracy: 0.4980 - val_loss: 0.7272 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7078 - accuracy: 0.4968 - val_loss: 0.7122 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7082 - accuracy: 0.4982 - val_loss: 0.6962 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7071 - accuracy: 0.5013 - val_loss: 0.7038 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7059 - accuracy: 0.5142 - val_loss: 0.7237 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7065 - accuracy: 0.5042 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7066 - accuracy: 0.5028 - val_loss: 0.7045 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7054 - accuracy: 0.5028 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7064 - accuracy: 0.5076 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 7ms/step\n","Epoch 1/20\n","76/76 [==============================] - 5s 36ms/step - loss: 0.7313 - accuracy: 0.5117 - val_loss: 0.6941 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7253 - accuracy: 0.5092 - val_loss: 0.6934 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7221 - accuracy: 0.5034 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7217 - accuracy: 0.5121 - val_loss: 0.6938 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7186 - accuracy: 0.5123 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7266 - accuracy: 0.4924 - val_loss: 0.6992 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7156 - accuracy: 0.5053 - val_loss: 0.6957 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7186 - accuracy: 0.5009 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7208 - accuracy: 0.4991 - val_loss: 0.6951 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7155 - accuracy: 0.4910 - val_loss: 0.7150 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 3s 33ms/step - loss: 0.7177 - accuracy: 0.5003 - val_loss: 0.7267 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 3s 33ms/step - loss: 0.7181 - accuracy: 0.5007 - val_loss: 0.7245 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7129 - accuracy: 0.5049 - val_loss: 0.7458 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7169 - accuracy: 0.4974 - val_loss: 0.6990 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7100 - accuracy: 0.5127 - val_loss: 0.7105 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7164 - accuracy: 0.4914 - val_loss: 0.6947 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7115 - accuracy: 0.5080 - val_loss: 0.7312 - val_accuracy: 0.4773\n","Epoch 18/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7120 - accuracy: 0.5005 - val_loss: 0.7664 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7121 - accuracy: 0.5053 - val_loss: 0.7102 - val_accuracy: 0.4773\n","Epoch 20/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7104 - accuracy: 0.5001 - val_loss: 0.7213 - val_accuracy: 0.4773\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 10s 65ms/step - loss: 0.6939 - accuracy: 0.5096 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6937 - accuracy: 0.5086 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6938 - accuracy: 0.5165 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 69ms/step - loss: 0.6932 - accuracy: 0.5158 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6930 - accuracy: 0.5131 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6936 - accuracy: 0.5119 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6925 - accuracy: 0.5187 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6934 - accuracy: 0.5117 - val_loss: 0.6932 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6930 - accuracy: 0.5202 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6927 - accuracy: 0.5150 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6929 - accuracy: 0.5200 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 4s 49ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6928 - accuracy: 0.5148 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6924 - accuracy: 0.5181 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6932 - accuracy: 0.5185 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6927 - accuracy: 0.5216 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6925 - accuracy: 0.5185 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.6931 - accuracy: 0.5185 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.6928 - accuracy: 0.5210 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 14ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 33ms/step - loss: 0.7345 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.7253 - accuracy: 0.4976 - val_loss: 0.6947 - val_accuracy: 0.4773\n","Epoch 3/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7294 - accuracy: 0.4889 - val_loss: 0.6948 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7262 - accuracy: 0.4970 - val_loss: 0.6971 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7248 - accuracy: 0.5088 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7246 - accuracy: 0.4945 - val_loss: 0.6940 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7219 - accuracy: 0.5020 - val_loss: 0.7107 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7239 - accuracy: 0.5028 - val_loss: 0.6959 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7159 - accuracy: 0.5057 - val_loss: 0.7265 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7212 - accuracy: 0.5045 - val_loss: 0.7423 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7182 - accuracy: 0.5007 - val_loss: 0.7272 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7171 - accuracy: 0.4966 - val_loss: 0.7772 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7182 - accuracy: 0.4976 - val_loss: 0.6935 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7155 - accuracy: 0.5067 - val_loss: 0.7669 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7141 - accuracy: 0.4999 - val_loss: 0.7096 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7154 - accuracy: 0.4980 - val_loss: 0.6943 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7154 - accuracy: 0.4995 - val_loss: 0.7623 - val_accuracy: 0.4773\n","Epoch 18/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7142 - accuracy: 0.5063 - val_loss: 0.7259 - val_accuracy: 0.4773\n","Epoch 19/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7138 - accuracy: 0.4933 - val_loss: 0.7020 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7161 - accuracy: 0.4968 - val_loss: 0.8680 - val_accuracy: 0.4773\n","65/65 [==============================] - 1s 5ms/step\n","Epoch 1/20\n","76/76 [==============================] - 6s 31ms/step - loss: 0.7184 - accuracy: 0.5071 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7068 - accuracy: 0.5140 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7079 - accuracy: 0.5092 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7084 - accuracy: 0.5040 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7064 - accuracy: 0.5045 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 3s 36ms/step - loss: 0.7063 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7065 - accuracy: 0.5045 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7069 - accuracy: 0.5051 - val_loss: 0.6950 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7058 - accuracy: 0.5173 - val_loss: 0.6947 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7070 - accuracy: 0.5084 - val_loss: 0.6974 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7076 - accuracy: 0.5047 - val_loss: 0.6999 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7046 - accuracy: 0.5119 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 3s 37ms/step - loss: 0.7063 - accuracy: 0.5067 - val_loss: 0.6960 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7047 - accuracy: 0.5103 - val_loss: 0.7209 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7055 - accuracy: 0.4999 - val_loss: 0.7018 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7013 - accuracy: 0.5216 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7069 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7029 - accuracy: 0.5134 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7044 - accuracy: 0.5109 - val_loss: 0.6971 - val_accuracy: 0.4773\n","Epoch 20/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.7039 - accuracy: 0.5057 - val_loss: 0.6922 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 9ms/step\n","Epoch 1/20\n","76/76 [==============================] - 10s 75ms/step - loss: 0.6934 - accuracy: 0.5165 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 6s 75ms/step - loss: 0.6931 - accuracy: 0.5144 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 4s 54ms/step - loss: 0.6928 - accuracy: 0.5115 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6928 - accuracy: 0.5163 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 6s 76ms/step - loss: 0.6927 - accuracy: 0.5115 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6923 - accuracy: 0.5192 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.6929 - accuracy: 0.5144 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 6s 72ms/step - loss: 0.6925 - accuracy: 0.5177 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 4s 54ms/step - loss: 0.6927 - accuracy: 0.5148 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.6927 - accuracy: 0.5158 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6931 - accuracy: 0.5189 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6924 - accuracy: 0.5225 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.6929 - accuracy: 0.5235 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 6s 82ms/step - loss: 0.6926 - accuracy: 0.5196 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6924 - accuracy: 0.5198 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6928 - accuracy: 0.5173 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6929 - accuracy: 0.5221 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6927 - accuracy: 0.5216 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6924 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 20ms/step\n","Epoch 1/20\n","76/76 [==============================] - 5s 33ms/step - loss: 0.7159 - accuracy: 0.5057 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7079 - accuracy: 0.5105 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7082 - accuracy: 0.5024 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7106 - accuracy: 0.4964 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7071 - accuracy: 0.5121 - val_loss: 0.6959 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7095 - accuracy: 0.4984 - val_loss: 0.6937 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7087 - accuracy: 0.5034 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7130 - accuracy: 0.4941 - val_loss: 0.7166 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7088 - accuracy: 0.5127 - val_loss: 0.7691 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7098 - accuracy: 0.5076 - val_loss: 0.7153 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7093 - accuracy: 0.5038 - val_loss: 0.7041 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7075 - accuracy: 0.5074 - val_loss: 0.6954 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7109 - accuracy: 0.5049 - val_loss: 0.7152 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7078 - accuracy: 0.5080 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7101 - accuracy: 0.5018 - val_loss: 0.6975 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7038 - accuracy: 0.5171 - val_loss: 0.7042 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7110 - accuracy: 0.5005 - val_loss: 0.7480 - val_accuracy: 0.4773\n","Epoch 18/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7062 - accuracy: 0.5055 - val_loss: 0.7019 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7100 - accuracy: 0.5032 - val_loss: 0.6999 - val_accuracy: 0.4773\n","Epoch 20/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7092 - accuracy: 0.5067 - val_loss: 0.6954 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 7ms/step\n","Epoch 1/20\n","76/76 [==============================] - 9s 81ms/step - loss: 0.7825 - accuracy: 0.4955 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7548 - accuracy: 0.5024 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7464 - accuracy: 0.5086 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.7495 - accuracy: 0.4993 - val_loss: 0.7102 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7466 - accuracy: 0.4999 - val_loss: 0.6998 - val_accuracy: 0.4773\n","Epoch 6/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7451 - accuracy: 0.4864 - val_loss: 0.7033 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7390 - accuracy: 0.5009 - val_loss: 0.7231 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7418 - accuracy: 0.4906 - val_loss: 0.7110 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7328 - accuracy: 0.5049 - val_loss: 0.7663 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7320 - accuracy: 0.5082 - val_loss: 0.6933 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7311 - accuracy: 0.5125 - val_loss: 0.7072 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.7244 - accuracy: 0.5121 - val_loss: 0.7134 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.7317 - accuracy: 0.4962 - val_loss: 0.7045 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7272 - accuracy: 0.5049 - val_loss: 0.7224 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7223 - accuracy: 0.5140 - val_loss: 0.8027 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7258 - accuracy: 0.4980 - val_loss: 0.6939 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7265 - accuracy: 0.5016 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7255 - accuracy: 0.5001 - val_loss: 0.7243 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7216 - accuracy: 0.5154 - val_loss: 0.7226 - val_accuracy: 0.4773\n","Epoch 20/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7298 - accuracy: 0.4953 - val_loss: 0.6923 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 10ms/step\n","Epoch 1/20\n","76/76 [==============================] - 11s 99ms/step - loss: 0.6948 - accuracy: 0.5069 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.6945 - accuracy: 0.5098 - val_loss: 0.6934 - val_accuracy: 0.4773\n","Epoch 3/20\n","76/76 [==============================] - 5s 70ms/step - loss: 0.6930 - accuracy: 0.5258 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 6s 78ms/step - loss: 0.6930 - accuracy: 0.5171 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 72ms/step - loss: 0.6933 - accuracy: 0.5189 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 7s 91ms/step - loss: 0.6932 - accuracy: 0.5158 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6930 - accuracy: 0.5210 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.6931 - accuracy: 0.5177 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 6s 84ms/step - loss: 0.6931 - accuracy: 0.5123 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6926 - accuracy: 0.5187 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 6s 79ms/step - loss: 0.6928 - accuracy: 0.5165 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 6s 79ms/step - loss: 0.6934 - accuracy: 0.5131 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6928 - accuracy: 0.5160 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 6s 83ms/step - loss: 0.6930 - accuracy: 0.5175 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6928 - accuracy: 0.5169 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 6s 82ms/step - loss: 0.6931 - accuracy: 0.5150 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 6s 81ms/step - loss: 0.6925 - accuracy: 0.5192 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.6925 - accuracy: 0.5216 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 6s 82ms/step - loss: 0.6928 - accuracy: 0.5086 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.6921 - accuracy: 0.5225 - val_loss: 0.6923 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 20ms/step\n","Epoch 1/20\n","76/76 [==============================] - 8s 66ms/step - loss: 0.7708 - accuracy: 0.5117 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7702 - accuracy: 0.4893 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7648 - accuracy: 0.4935 - val_loss: 0.6974 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.7664 - accuracy: 0.4924 - val_loss: 0.6986 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7565 - accuracy: 0.4976 - val_loss: 0.7292 - val_accuracy: 0.4773\n","Epoch 6/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7514 - accuracy: 0.5047 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.7469 - accuracy: 0.4982 - val_loss: 0.7026 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7479 - accuracy: 0.4924 - val_loss: 0.7328 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7392 - accuracy: 0.4881 - val_loss: 0.6939 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.7356 - accuracy: 0.4953 - val_loss: 0.7825 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.7272 - accuracy: 0.5059 - val_loss: 0.7548 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7289 - accuracy: 0.5059 - val_loss: 0.7188 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7297 - accuracy: 0.4989 - val_loss: 0.7103 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7283 - accuracy: 0.5051 - val_loss: 0.7078 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.7325 - accuracy: 0.5013 - val_loss: 0.6982 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7306 - accuracy: 0.5059 - val_loss: 0.6942 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.7321 - accuracy: 0.5001 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7324 - accuracy: 0.5055 - val_loss: 0.7695 - val_accuracy: 0.4773\n","Epoch 19/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7352 - accuracy: 0.4993 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7334 - accuracy: 0.4953 - val_loss: 0.6941 - val_accuracy: 0.4773\n","65/65 [==============================] - 1s 11ms/step\n","Epoch 1/20\n","76/76 [==============================] - 5s 34ms/step - loss: 0.7139 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7064 - accuracy: 0.5156 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7061 - accuracy: 0.5045 - val_loss: 0.6932 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7089 - accuracy: 0.4895 - val_loss: 0.6942 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7030 - accuracy: 0.5067 - val_loss: 0.6943 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7061 - accuracy: 0.5038 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7056 - accuracy: 0.5086 - val_loss: 0.6960 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7046 - accuracy: 0.5080 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7060 - accuracy: 0.5003 - val_loss: 0.7044 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7063 - accuracy: 0.4997 - val_loss: 0.7257 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7054 - accuracy: 0.4970 - val_loss: 0.6949 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7037 - accuracy: 0.5020 - val_loss: 0.7364 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7061 - accuracy: 0.4984 - val_loss: 0.6956 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7048 - accuracy: 0.4964 - val_loss: 0.7449 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7040 - accuracy: 0.4999 - val_loss: 0.7007 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7048 - accuracy: 0.5003 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7048 - accuracy: 0.4922 - val_loss: 0.7169 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7044 - accuracy: 0.4943 - val_loss: 0.7028 - val_accuracy: 0.4773\n","Epoch 19/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7008 - accuracy: 0.5061 - val_loss: 0.6933 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7024 - accuracy: 0.5061 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 10s 78ms/step - loss: 0.6930 - accuracy: 0.5092 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6942 - accuracy: 0.5109 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6931 - accuracy: 0.5134 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 4s 53ms/step - loss: 0.6931 - accuracy: 0.5150 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6928 - accuracy: 0.5138 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.6927 - accuracy: 0.5156 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6930 - accuracy: 0.5187 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.6930 - accuracy: 0.5183 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 4s 52ms/step - loss: 0.6924 - accuracy: 0.5181 - val_loss: 0.6934 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6932 - accuracy: 0.5156 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6929 - accuracy: 0.5239 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6931 - accuracy: 0.5171 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 51ms/step - loss: 0.6924 - accuracy: 0.5185 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6924 - accuracy: 0.5204 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6923 - accuracy: 0.5160 - val_loss: 0.6932 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6933 - accuracy: 0.5082 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6926 - accuracy: 0.5150 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6923 - accuracy: 0.5241 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6926 - accuracy: 0.5189 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 70ms/step - loss: 0.6930 - accuracy: 0.5223 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 15ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 25ms/step - loss: 0.7142 - accuracy: 0.5088 - val_loss: 0.6934 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7070 - accuracy: 0.5084 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7059 - accuracy: 0.5084 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7090 - accuracy: 0.4947 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7091 - accuracy: 0.4835 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7052 - accuracy: 0.5080 - val_loss: 0.6951 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7075 - accuracy: 0.4964 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7063 - accuracy: 0.5100 - val_loss: 0.6962 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7075 - accuracy: 0.5078 - val_loss: 0.6997 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7057 - accuracy: 0.5119 - val_loss: 0.6986 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7082 - accuracy: 0.5055 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7060 - accuracy: 0.5096 - val_loss: 0.6990 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7091 - accuracy: 0.4953 - val_loss: 0.6944 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7077 - accuracy: 0.4989 - val_loss: 0.7249 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7058 - accuracy: 0.5036 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7069 - accuracy: 0.4978 - val_loss: 0.7197 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7082 - accuracy: 0.4939 - val_loss: 0.7147 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7070 - accuracy: 0.4976 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7063 - accuracy: 0.5076 - val_loss: 0.7349 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7042 - accuracy: 0.5173 - val_loss: 0.7307 - val_accuracy: 0.4773\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 37ms/step - loss: 0.7369 - accuracy: 0.5092 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7283 - accuracy: 0.4947 - val_loss: 0.6975 - val_accuracy: 0.4773\n","Epoch 3/20\n","76/76 [==============================] - 2s 33ms/step - loss: 0.7208 - accuracy: 0.5109 - val_loss: 0.6952 - val_accuracy: 0.4773\n","Epoch 4/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7228 - accuracy: 0.5018 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7189 - accuracy: 0.5098 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7245 - accuracy: 0.4889 - val_loss: 0.6934 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7179 - accuracy: 0.5065 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7174 - accuracy: 0.5003 - val_loss: 0.7001 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 3s 37ms/step - loss: 0.7159 - accuracy: 0.4980 - val_loss: 0.6980 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.7160 - accuracy: 0.5001 - val_loss: 0.7029 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 3s 34ms/step - loss: 0.7163 - accuracy: 0.4968 - val_loss: 0.7092 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7171 - accuracy: 0.4987 - val_loss: 0.7261 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7172 - accuracy: 0.5030 - val_loss: 0.7204 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7156 - accuracy: 0.5026 - val_loss: 0.7440 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 3s 34ms/step - loss: 0.7133 - accuracy: 0.4893 - val_loss: 0.6996 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 2s 33ms/step - loss: 0.7126 - accuracy: 0.5047 - val_loss: 0.7004 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7134 - accuracy: 0.5018 - val_loss: 0.7053 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7107 - accuracy: 0.5107 - val_loss: 0.7296 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7156 - accuracy: 0.4968 - val_loss: 0.7002 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7087 - accuracy: 0.5030 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 8ms/step\n","Epoch 1/20\n","76/76 [==============================] - 9s 58ms/step - loss: 0.6941 - accuracy: 0.5142 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6937 - accuracy: 0.5160 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.6932 - accuracy: 0.5171 - val_loss: 0.6932 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6923 - accuracy: 0.5216 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6931 - accuracy: 0.5096 - val_loss: 0.6933 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.6927 - accuracy: 0.5223 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6925 - accuracy: 0.5198 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6925 - accuracy: 0.5245 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6931 - accuracy: 0.5167 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6930 - accuracy: 0.5194 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 4s 51ms/step - loss: 0.6926 - accuracy: 0.5189 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6925 - accuracy: 0.5189 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6928 - accuracy: 0.5208 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6925 - accuracy: 0.5194 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 4s 50ms/step - loss: 0.6930 - accuracy: 0.5200 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6928 - accuracy: 0.5200 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6927 - accuracy: 0.5212 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 5s 73ms/step - loss: 0.6926 - accuracy: 0.5154 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6927 - accuracy: 0.5200 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.6929 - accuracy: 0.5196 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 15ms/step\n","Epoch 1/20\n","76/76 [==============================] - 6s 35ms/step - loss: 0.7371 - accuracy: 0.5034 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7258 - accuracy: 0.5084 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7268 - accuracy: 0.5045 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7274 - accuracy: 0.5009 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.7258 - accuracy: 0.4976 - val_loss: 0.7108 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7221 - accuracy: 0.5055 - val_loss: 0.6998 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7217 - accuracy: 0.5140 - val_loss: 0.7123 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7213 - accuracy: 0.5005 - val_loss: 0.7521 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7226 - accuracy: 0.5005 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7143 - accuracy: 0.5181 - val_loss: 0.6939 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7172 - accuracy: 0.5057 - val_loss: 0.7255 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.7172 - accuracy: 0.5074 - val_loss: 0.7091 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7159 - accuracy: 0.5111 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7133 - accuracy: 0.5090 - val_loss: 0.6991 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7177 - accuracy: 0.4924 - val_loss: 0.7348 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7155 - accuracy: 0.4991 - val_loss: 0.6975 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7157 - accuracy: 0.4962 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.7162 - accuracy: 0.4920 - val_loss: 0.7010 - val_accuracy: 0.4773\n","Epoch 19/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7160 - accuracy: 0.4910 - val_loss: 0.7608 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7173 - accuracy: 0.4897 - val_loss: 0.6922 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 5ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 30ms/step - loss: 0.7178 - accuracy: 0.5038 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.7074 - accuracy: 0.4960 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7097 - accuracy: 0.4941 - val_loss: 0.6938 - val_accuracy: 0.4773\n","Epoch 4/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7100 - accuracy: 0.4922 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7063 - accuracy: 0.5032 - val_loss: 0.6953 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7071 - accuracy: 0.5020 - val_loss: 0.6979 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7097 - accuracy: 0.5055 - val_loss: 0.6989 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7047 - accuracy: 0.5175 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 3s 36ms/step - loss: 0.7090 - accuracy: 0.4935 - val_loss: 0.7066 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7073 - accuracy: 0.4968 - val_loss: 0.7357 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7071 - accuracy: 0.5131 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.7064 - accuracy: 0.5142 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7046 - accuracy: 0.5131 - val_loss: 0.7020 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7057 - accuracy: 0.5018 - val_loss: 0.6940 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7076 - accuracy: 0.4949 - val_loss: 0.7103 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 3s 33ms/step - loss: 0.7058 - accuracy: 0.4945 - val_loss: 0.6932 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 3s 34ms/step - loss: 0.7078 - accuracy: 0.4924 - val_loss: 0.6956 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7054 - accuracy: 0.4982 - val_loss: 0.6952 - val_accuracy: 0.4773\n","Epoch 19/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7029 - accuracy: 0.5074 - val_loss: 0.7018 - val_accuracy: 0.4773\n","Epoch 20/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7048 - accuracy: 0.5105 - val_loss: 0.6930 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 8ms/step\n","Epoch 1/20\n","76/76 [==============================] - 10s 88ms/step - loss: 0.6944 - accuracy: 0.5038 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6933 - accuracy: 0.5221 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 6s 74ms/step - loss: 0.6930 - accuracy: 0.5165 - val_loss: 0.6935 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 5s 71ms/step - loss: 0.6931 - accuracy: 0.5109 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 6s 78ms/step - loss: 0.6926 - accuracy: 0.5216 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6927 - accuracy: 0.5107 - val_loss: 0.6945 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6930 - accuracy: 0.5192 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 6s 77ms/step - loss: 0.6925 - accuracy: 0.5187 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 5s 59ms/step - loss: 0.6923 - accuracy: 0.5200 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6920 - accuracy: 0.5301 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 7s 87ms/step - loss: 0.6931 - accuracy: 0.5165 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 6s 75ms/step - loss: 0.6923 - accuracy: 0.5202 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.6923 - accuracy: 0.5237 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 6s 77ms/step - loss: 0.6930 - accuracy: 0.5227 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6926 - accuracy: 0.5206 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.6928 - accuracy: 0.5208 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 6s 76ms/step - loss: 0.6927 - accuracy: 0.5225 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6927 - accuracy: 0.5202 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 70ms/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6922 - val_accuracy: 0.5227\n","65/65 [==============================] - 3s 37ms/step\n","Epoch 1/20\n","76/76 [==============================] - 5s 30ms/step - loss: 0.7150 - accuracy: 0.5063 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7066 - accuracy: 0.5057 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7078 - accuracy: 0.5047 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7055 - accuracy: 0.5152 - val_loss: 0.6963 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 3s 34ms/step - loss: 0.7123 - accuracy: 0.4955 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7088 - accuracy: 0.4931 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7078 - accuracy: 0.5047 - val_loss: 0.6947 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7073 - accuracy: 0.5216 - val_loss: 0.6946 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7064 - accuracy: 0.5049 - val_loss: 0.7052 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7087 - accuracy: 0.5154 - val_loss: 0.7307 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7099 - accuracy: 0.5076 - val_loss: 0.7360 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7104 - accuracy: 0.4976 - val_loss: 0.6967 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 32ms/step - loss: 0.7093 - accuracy: 0.5026 - val_loss: 0.7089 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7083 - accuracy: 0.5090 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7109 - accuracy: 0.4999 - val_loss: 0.7027 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7069 - accuracy: 0.5105 - val_loss: 0.7977 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7076 - accuracy: 0.5076 - val_loss: 0.7373 - val_accuracy: 0.4773\n","Epoch 18/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7087 - accuracy: 0.5026 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7088 - accuracy: 0.4976 - val_loss: 0.6986 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.7095 - accuracy: 0.5051 - val_loss: 0.6949 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 9ms/step\n","Epoch 1/20\n","76/76 [==============================] - 7s 71ms/step - loss: 0.7901 - accuracy: 0.4881 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7580 - accuracy: 0.4916 - val_loss: 0.7082 - val_accuracy: 0.4773\n","Epoch 3/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7517 - accuracy: 0.4972 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.7475 - accuracy: 0.5042 - val_loss: 0.6977 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7472 - accuracy: 0.4949 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7417 - accuracy: 0.4982 - val_loss: 0.7124 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.7405 - accuracy: 0.4953 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 5s 69ms/step - loss: 0.7364 - accuracy: 0.5053 - val_loss: 0.7012 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7331 - accuracy: 0.5028 - val_loss: 0.7152 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7349 - accuracy: 0.4972 - val_loss: 0.7132 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 5s 71ms/step - loss: 0.7293 - accuracy: 0.4974 - val_loss: 0.7064 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7283 - accuracy: 0.5049 - val_loss: 0.7855 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7270 - accuracy: 0.5034 - val_loss: 0.7173 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.7293 - accuracy: 0.4993 - val_loss: 0.6941 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7266 - accuracy: 0.4937 - val_loss: 0.6985 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 5s 69ms/step - loss: 0.7257 - accuracy: 0.5011 - val_loss: 0.7143 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7259 - accuracy: 0.5024 - val_loss: 0.7245 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7235 - accuracy: 0.5049 - val_loss: 0.6992 - val_accuracy: 0.4773\n","Epoch 19/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7260 - accuracy: 0.5063 - val_loss: 0.6980 - val_accuracy: 0.4773\n","Epoch 20/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7257 - accuracy: 0.4900 - val_loss: 0.6926 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 11ms/step\n","Epoch 1/20\n","76/76 [==============================] - 12s 105ms/step - loss: 0.6940 - accuracy: 0.5069 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6938 - accuracy: 0.5131 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 6s 74ms/step - loss: 0.6933 - accuracy: 0.5144 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 6s 79ms/step - loss: 0.6927 - accuracy: 0.5169 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6935 - accuracy: 0.5150 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 6s 86ms/step - loss: 0.6925 - accuracy: 0.5183 - val_loss: 0.6934 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6932 - accuracy: 0.5125 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 6s 80ms/step - loss: 0.6929 - accuracy: 0.5185 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 6s 80ms/step - loss: 0.6931 - accuracy: 0.5245 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6928 - accuracy: 0.5231 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 6s 83ms/step - loss: 0.6924 - accuracy: 0.5204 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.6920 - accuracy: 0.5229 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6929 - accuracy: 0.5125 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 6s 85ms/step - loss: 0.6927 - accuracy: 0.5158 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 5s 72ms/step - loss: 0.6930 - accuracy: 0.5146 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 6s 74ms/step - loss: 0.6927 - accuracy: 0.5229 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 6s 72ms/step - loss: 0.6923 - accuracy: 0.5204 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.6927 - accuracy: 0.5198 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 6s 79ms/step - loss: 0.6928 - accuracy: 0.5204 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 6s 74ms/step - loss: 0.6923 - accuracy: 0.5223 - val_loss: 0.6926 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 22ms/step\n","Epoch 1/20\n","76/76 [==============================] - 7s 73ms/step - loss: 0.7805 - accuracy: 0.5117 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7643 - accuracy: 0.5100 - val_loss: 0.7034 - val_accuracy: 0.4773\n","Epoch 3/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7664 - accuracy: 0.5001 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7619 - accuracy: 0.5051 - val_loss: 0.6943 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.7613 - accuracy: 0.4931 - val_loss: 0.6964 - val_accuracy: 0.4773\n","Epoch 6/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7510 - accuracy: 0.5040 - val_loss: 0.6945 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7501 - accuracy: 0.5038 - val_loss: 0.6978 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7442 - accuracy: 0.4987 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7377 - accuracy: 0.5034 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7353 - accuracy: 0.4949 - val_loss: 0.7000 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7295 - accuracy: 0.5061 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7296 - accuracy: 0.5001 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7309 - accuracy: 0.4995 - val_loss: 0.7186 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7317 - accuracy: 0.4964 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7316 - accuracy: 0.5067 - val_loss: 0.7082 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7353 - accuracy: 0.4881 - val_loss: 0.7113 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7268 - accuracy: 0.5071 - val_loss: 0.7193 - val_accuracy: 0.4773\n","Epoch 18/20\n","76/76 [==============================] - 5s 59ms/step - loss: 0.7307 - accuracy: 0.5005 - val_loss: 0.7018 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.7308 - accuracy: 0.5078 - val_loss: 0.6974 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7307 - accuracy: 0.5071 - val_loss: 0.7211 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 12ms/step\n","Epoch 1/20\n","76/76 [==============================] - 5s 37ms/step - loss: 0.7100 - accuracy: 0.5098 - val_loss: 0.6936 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7102 - accuracy: 0.4972 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7042 - accuracy: 0.5111 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7058 - accuracy: 0.5022 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7055 - accuracy: 0.5125 - val_loss: 0.6973 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7044 - accuracy: 0.5049 - val_loss: 0.6942 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7049 - accuracy: 0.5053 - val_loss: 0.6932 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7038 - accuracy: 0.5121 - val_loss: 0.7152 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7041 - accuracy: 0.5078 - val_loss: 0.6960 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7056 - accuracy: 0.4904 - val_loss: 0.7136 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7049 - accuracy: 0.4995 - val_loss: 0.7092 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7043 - accuracy: 0.5069 - val_loss: 0.7044 - val_accuracy: 0.4768\n","Epoch 13/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7038 - accuracy: 0.4984 - val_loss: 0.7189 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7040 - accuracy: 0.4982 - val_loss: 0.6944 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7039 - accuracy: 0.5069 - val_loss: 0.7166 - val_accuracy: 0.4768\n","Epoch 16/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7013 - accuracy: 0.5131 - val_loss: 0.6955 - val_accuracy: 0.4768\n","Epoch 17/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.7029 - accuracy: 0.5115 - val_loss: 0.7089 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7050 - accuracy: 0.5026 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7026 - accuracy: 0.5028 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7012 - accuracy: 0.5169 - val_loss: 0.6947 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 10s 79ms/step - loss: 0.6933 - accuracy: 0.5171 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6928 - accuracy: 0.5214 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6940 - accuracy: 0.5185 - val_loss: 0.6936 - val_accuracy: 0.4773\n","Epoch 4/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.6935 - accuracy: 0.5136 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.6927 - accuracy: 0.5125 - val_loss: 0.6941 - val_accuracy: 0.4773\n","Epoch 6/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6930 - accuracy: 0.5115 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 49ms/step - loss: 0.6931 - accuracy: 0.5183 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6930 - accuracy: 0.5225 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6929 - accuracy: 0.5223 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6927 - accuracy: 0.5212 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6926 - accuracy: 0.5231 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6925 - accuracy: 0.5210 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 50ms/step - loss: 0.6929 - accuracy: 0.5214 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6928 - accuracy: 0.5212 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6924 - accuracy: 0.5218 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6929 - accuracy: 0.5165 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6925 - accuracy: 0.5198 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6926 - accuracy: 0.5208 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6926 - accuracy: 0.5187 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6927 - accuracy: 0.5231 - val_loss: 0.6923 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 15ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 24ms/step - loss: 0.7140 - accuracy: 0.5009 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7047 - accuracy: 0.5103 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7065 - accuracy: 0.5047 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7051 - accuracy: 0.5020 - val_loss: 0.6953 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7083 - accuracy: 0.4937 - val_loss: 0.6934 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7056 - accuracy: 0.5001 - val_loss: 0.6960 - val_accuracy: 0.4768\n","Epoch 7/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7073 - accuracy: 0.5057 - val_loss: 0.7021 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7090 - accuracy: 0.4995 - val_loss: 0.6948 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7049 - accuracy: 0.5090 - val_loss: 0.6935 - val_accuracy: 0.4768\n","Epoch 10/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7064 - accuracy: 0.5018 - val_loss: 0.6990 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 1s 16ms/step - loss: 0.7066 - accuracy: 0.5076 - val_loss: 0.6952 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 1s 16ms/step - loss: 0.7065 - accuracy: 0.4999 - val_loss: 0.7007 - val_accuracy: 0.4768\n","Epoch 13/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7056 - accuracy: 0.5042 - val_loss: 0.7197 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 1s 16ms/step - loss: 0.7064 - accuracy: 0.5053 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7040 - accuracy: 0.5069 - val_loss: 0.7397 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7044 - accuracy: 0.5094 - val_loss: 0.6940 - val_accuracy: 0.4768\n","Epoch 17/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7096 - accuracy: 0.4982 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.7045 - accuracy: 0.5177 - val_loss: 0.7259 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7077 - accuracy: 0.4895 - val_loss: 0.6959 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7044 - accuracy: 0.5090 - val_loss: 0.7072 - val_accuracy: 0.5227\n","65/65 [==============================] - 0s 4ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 32ms/step - loss: 0.7356 - accuracy: 0.4916 - val_loss: 0.6936 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7238 - accuracy: 0.5057 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7221 - accuracy: 0.5040 - val_loss: 0.6946 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7143 - accuracy: 0.5096 - val_loss: 0.6946 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7224 - accuracy: 0.4991 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7196 - accuracy: 0.5005 - val_loss: 0.7014 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7151 - accuracy: 0.5063 - val_loss: 0.7465 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7190 - accuracy: 0.4964 - val_loss: 0.7021 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7186 - accuracy: 0.5001 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7128 - accuracy: 0.5121 - val_loss: 0.7216 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7134 - accuracy: 0.4987 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7171 - accuracy: 0.4980 - val_loss: 0.7406 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7135 - accuracy: 0.5009 - val_loss: 0.6938 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7148 - accuracy: 0.5049 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7135 - accuracy: 0.4937 - val_loss: 0.7149 - val_accuracy: 0.4768\n","Epoch 16/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7107 - accuracy: 0.5098 - val_loss: 0.6938 - val_accuracy: 0.4783\n","Epoch 17/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.7148 - accuracy: 0.4858 - val_loss: 0.6985 - val_accuracy: 0.4768\n","Epoch 18/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7136 - accuracy: 0.5005 - val_loss: 0.6965 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7108 - accuracy: 0.5071 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7105 - accuracy: 0.5082 - val_loss: 0.6922 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 8ms/step\n","Epoch 1/20\n","76/76 [==============================] - 7s 51ms/step - loss: 0.6940 - accuracy: 0.5123 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.6937 - accuracy: 0.5156 - val_loss: 0.6937 - val_accuracy: 0.4773\n","Epoch 3/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.6935 - accuracy: 0.5084 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6932 - accuracy: 0.5192 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6929 - accuracy: 0.5214 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6931 - accuracy: 0.5165 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.6929 - accuracy: 0.5225 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6929 - accuracy: 0.5189 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6929 - accuracy: 0.5183 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.6930 - accuracy: 0.5206 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.6928 - accuracy: 0.5223 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.6923 - accuracy: 0.5181 - val_loss: 0.6934 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6931 - accuracy: 0.5223 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.6926 - accuracy: 0.5233 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.6928 - accuracy: 0.5204 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.6930 - accuracy: 0.5202 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.6920 - accuracy: 0.5218 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6925 - accuracy: 0.5233 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.6929 - accuracy: 0.5127 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.6922 - accuracy: 0.5192 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 11ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 31ms/step - loss: 0.7349 - accuracy: 0.5090 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7268 - accuracy: 0.5007 - val_loss: 0.6941 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7329 - accuracy: 0.4926 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7258 - accuracy: 0.5123 - val_loss: 0.7069 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7258 - accuracy: 0.5092 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7253 - accuracy: 0.5134 - val_loss: 0.6936 - val_accuracy: 0.4768\n","Epoch 7/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7260 - accuracy: 0.5049 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.7267 - accuracy: 0.4955 - val_loss: 0.7077 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7228 - accuracy: 0.5069 - val_loss: 0.7125 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7205 - accuracy: 0.5074 - val_loss: 0.7099 - val_accuracy: 0.4768\n","Epoch 11/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7175 - accuracy: 0.4984 - val_loss: 0.8444 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7221 - accuracy: 0.4941 - val_loss: 0.7210 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7174 - accuracy: 0.5051 - val_loss: 0.7339 - val_accuracy: 0.4768\n","Epoch 14/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7150 - accuracy: 0.5038 - val_loss: 0.7521 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7160 - accuracy: 0.4980 - val_loss: 0.7037 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.7157 - accuracy: 0.5063 - val_loss: 0.6990 - val_accuracy: 0.4768\n","Epoch 17/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7112 - accuracy: 0.5179 - val_loss: 0.7021 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7146 - accuracy: 0.4984 - val_loss: 0.6955 - val_accuracy: 0.4768\n","Epoch 19/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.7142 - accuracy: 0.5003 - val_loss: 0.6945 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.7151 - accuracy: 0.5049 - val_loss: 0.7054 - val_accuracy: 0.4768\n","65/65 [==============================] - 1s 7ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 27ms/step - loss: 0.7173 - accuracy: 0.5111 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7084 - accuracy: 0.5018 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7065 - accuracy: 0.5030 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7094 - accuracy: 0.5005 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7065 - accuracy: 0.5140 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7069 - accuracy: 0.5109 - val_loss: 0.6952 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7089 - accuracy: 0.5042 - val_loss: 0.6940 - val_accuracy: 0.4768\n","Epoch 8/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7076 - accuracy: 0.5119 - val_loss: 0.6935 - val_accuracy: 0.4783\n","Epoch 9/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7086 - accuracy: 0.5005 - val_loss: 0.6933 - val_accuracy: 0.4768\n","Epoch 10/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7052 - accuracy: 0.5080 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7087 - accuracy: 0.4962 - val_loss: 0.6952 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7066 - accuracy: 0.5024 - val_loss: 0.7010 - val_accuracy: 0.4768\n","Epoch 13/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7086 - accuracy: 0.5053 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.7061 - accuracy: 0.4984 - val_loss: 0.6982 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7073 - accuracy: 0.5059 - val_loss: 0.7300 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7067 - accuracy: 0.4922 - val_loss: 0.7088 - val_accuracy: 0.4768\n","Epoch 17/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7059 - accuracy: 0.4997 - val_loss: 0.6938 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7060 - accuracy: 0.5051 - val_loss: 0.6932 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7041 - accuracy: 0.5154 - val_loss: 0.7304 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7039 - accuracy: 0.5084 - val_loss: 0.6958 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 7ms/step\n","Epoch 1/20\n","76/76 [==============================] - 7s 54ms/step - loss: 0.6931 - accuracy: 0.5189 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6936 - accuracy: 0.5144 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6925 - accuracy: 0.5179 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6932 - accuracy: 0.5171 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 4s 49ms/step - loss: 0.6931 - accuracy: 0.5113 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 4s 51ms/step - loss: 0.6931 - accuracy: 0.5163 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6935 - accuracy: 0.5210 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6930 - accuracy: 0.5181 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 4s 49ms/step - loss: 0.6928 - accuracy: 0.5202 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 4s 51ms/step - loss: 0.6923 - accuracy: 0.5183 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6922 - accuracy: 0.5274 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6929 - accuracy: 0.5233 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 52ms/step - loss: 0.6930 - accuracy: 0.5109 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 4s 51ms/step - loss: 0.6930 - accuracy: 0.5194 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6927 - accuracy: 0.5214 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.6923 - accuracy: 0.5225 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6928 - accuracy: 0.5210 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6921 - accuracy: 0.5214 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6928 - accuracy: 0.5192 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6928 - accuracy: 0.5210 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 16ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 25ms/step - loss: 0.7178 - accuracy: 0.5036 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7057 - accuracy: 0.5092 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7061 - accuracy: 0.5067 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7102 - accuracy: 0.5036 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7074 - accuracy: 0.5167 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7075 - accuracy: 0.5040 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7081 - accuracy: 0.5111 - val_loss: 0.6934 - val_accuracy: 0.4783\n","Epoch 8/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7103 - accuracy: 0.5065 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7095 - accuracy: 0.5045 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7104 - accuracy: 0.5067 - val_loss: 0.7296 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7122 - accuracy: 0.4964 - val_loss: 0.7100 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.7109 - accuracy: 0.5040 - val_loss: 0.6975 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7105 - accuracy: 0.4968 - val_loss: 0.7461 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7082 - accuracy: 0.5117 - val_loss: 0.7027 - val_accuracy: 0.4768\n","Epoch 15/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7084 - accuracy: 0.5042 - val_loss: 0.6959 - val_accuracy: 0.4768\n","Epoch 16/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.7085 - accuracy: 0.5038 - val_loss: 0.7282 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7068 - accuracy: 0.5156 - val_loss: 0.7045 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.7096 - accuracy: 0.5030 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7083 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.7101 - accuracy: 0.4962 - val_loss: 0.6922 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 7ms/step\n","Epoch 1/20\n","76/76 [==============================] - 6s 68ms/step - loss: 0.7749 - accuracy: 0.5059 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7520 - accuracy: 0.5158 - val_loss: 0.6932 - val_accuracy: 0.4768\n","Epoch 3/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7485 - accuracy: 0.5123 - val_loss: 0.6973 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.7442 - accuracy: 0.5036 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7433 - accuracy: 0.5042 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.7433 - accuracy: 0.5016 - val_loss: 0.7102 - val_accuracy: 0.4773\n","Epoch 7/20\n","76/76 [==============================] - 5s 68ms/step - loss: 0.7368 - accuracy: 0.5018 - val_loss: 0.7225 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7413 - accuracy: 0.5003 - val_loss: 0.7273 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7371 - accuracy: 0.4937 - val_loss: 0.7370 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.7340 - accuracy: 0.5063 - val_loss: 0.7575 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7336 - accuracy: 0.4984 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7326 - accuracy: 0.4939 - val_loss: 0.6985 - val_accuracy: 0.4768\n","Epoch 13/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7326 - accuracy: 0.5007 - val_loss: 0.7264 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7250 - accuracy: 0.5040 - val_loss: 0.7470 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.7303 - accuracy: 0.5040 - val_loss: 0.7295 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.7244 - accuracy: 0.5053 - val_loss: 0.7301 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.7265 - accuracy: 0.4955 - val_loss: 0.6936 - val_accuracy: 0.4768\n","Epoch 18/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7197 - accuracy: 0.5103 - val_loss: 0.6958 - val_accuracy: 0.4768\n","Epoch 19/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.7289 - accuracy: 0.4875 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.7194 - accuracy: 0.5142 - val_loss: 0.7001 - val_accuracy: 0.4768\n","65/65 [==============================] - 1s 11ms/step\n","Epoch 1/20\n","76/76 [==============================] - 9s 80ms/step - loss: 0.6941 - accuracy: 0.5177 - val_loss: 0.6973 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6940 - accuracy: 0.5111 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6936 - accuracy: 0.5113 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6934 - accuracy: 0.5105 - val_loss: 0.6939 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6935 - accuracy: 0.5177 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6928 - accuracy: 0.5241 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.6928 - accuracy: 0.5160 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.6930 - accuracy: 0.5192 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6931 - accuracy: 0.5150 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.6926 - accuracy: 0.5237 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6923 - accuracy: 0.5235 - val_loss: 0.6944 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6929 - accuracy: 0.5113 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6930 - accuracy: 0.5158 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6926 - accuracy: 0.5202 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6926 - accuracy: 0.5202 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.6924 - accuracy: 0.5237 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6929 - accuracy: 0.5208 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.6924 - accuracy: 0.5150 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6926 - accuracy: 0.5221 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6932 - accuracy: 0.5177 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 17ms/step\n","Epoch 1/20\n","76/76 [==============================] - 5s 59ms/step - loss: 0.7766 - accuracy: 0.4997 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.7609 - accuracy: 0.5016 - val_loss: 0.6998 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.7676 - accuracy: 0.4989 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.7575 - accuracy: 0.5009 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.7608 - accuracy: 0.4984 - val_loss: 0.6996 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.7530 - accuracy: 0.4914 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.7475 - accuracy: 0.5061 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.7409 - accuracy: 0.5096 - val_loss: 0.7049 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.7391 - accuracy: 0.4995 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.7302 - accuracy: 0.5063 - val_loss: 0.6946 - val_accuracy: 0.4768\n","Epoch 11/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.7317 - accuracy: 0.4955 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.7261 - accuracy: 0.5204 - val_loss: 0.7025 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.7316 - accuracy: 0.4943 - val_loss: 0.7878 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.7267 - accuracy: 0.5171 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.7301 - accuracy: 0.5040 - val_loss: 0.6974 - val_accuracy: 0.4768\n","Epoch 16/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.7310 - accuracy: 0.4951 - val_loss: 0.6992 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.7308 - accuracy: 0.5183 - val_loss: 0.6934 - val_accuracy: 0.4768\n","Epoch 18/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.7318 - accuracy: 0.5001 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.7339 - accuracy: 0.4997 - val_loss: 0.6999 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.7357 - accuracy: 0.4999 - val_loss: 0.6923 - val_accuracy: 0.5227\n","65/65 [==============================] - 1s 9ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 21ms/step - loss: 0.6051 - accuracy: 0.6767 - val_loss: 0.6811 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.4143 - accuracy: 0.8099 - val_loss: 0.6619 - val_accuracy: 0.5531\n","Epoch 3/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.3135 - accuracy: 0.8673 - val_loss: 0.6324 - val_accuracy: 0.7251\n","Epoch 4/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.2231 - accuracy: 0.9064 - val_loss: 0.6048 - val_accuracy: 0.6870\n","Epoch 5/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.1644 - accuracy: 0.9341 - val_loss: 0.7791 - val_accuracy: 0.5290\n","Epoch 6/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.1251 - accuracy: 0.9507 - val_loss: 0.5870 - val_accuracy: 0.6908\n","Epoch 7/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0985 - accuracy: 0.9615 - val_loss: 1.4960 - val_accuracy: 0.5836\n","Epoch 8/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0813 - accuracy: 0.9704 - val_loss: 0.9200 - val_accuracy: 0.6667\n","Epoch 9/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0649 - accuracy: 0.9783 - val_loss: 1.8161 - val_accuracy: 0.6256\n","Epoch 10/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0593 - accuracy: 0.9783 - val_loss: 1.2853 - val_accuracy: 0.6686\n","Epoch 11/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0480 - accuracy: 0.9814 - val_loss: 2.6844 - val_accuracy: 0.5681\n","Epoch 12/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.0426 - accuracy: 0.9849 - val_loss: 1.6594 - val_accuracy: 0.6787\n","Epoch 13/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.0405 - accuracy: 0.9838 - val_loss: 1.8545 - val_accuracy: 0.6541\n","Epoch 14/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.0371 - accuracy: 0.9876 - val_loss: 2.1494 - val_accuracy: 0.6734\n","Epoch 15/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0334 - accuracy: 0.9882 - val_loss: 1.8153 - val_accuracy: 0.6570\n","Epoch 16/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.0303 - accuracy: 0.9894 - val_loss: 1.8030 - val_accuracy: 0.6792\n","Epoch 17/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0275 - accuracy: 0.9913 - val_loss: 2.4261 - val_accuracy: 0.6594\n","Epoch 18/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0232 - accuracy: 0.9930 - val_loss: 7.8338 - val_accuracy: 0.5522\n","Epoch 19/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0257 - accuracy: 0.9913 - val_loss: 2.3128 - val_accuracy: 0.6676\n","Epoch 20/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.0192 - accuracy: 0.9930 - val_loss: 2.6732 - val_accuracy: 0.6652\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 7s 55ms/step - loss: 0.6462 - accuracy: 0.6258 - val_loss: 0.5308 - val_accuracy: 0.7425\n","Epoch 2/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.4715 - accuracy: 0.7817 - val_loss: 0.5178 - val_accuracy: 0.7536\n","Epoch 3/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.4109 - accuracy: 0.8223 - val_loss: 0.5400 - val_accuracy: 0.7396\n","Epoch 4/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.3576 - accuracy: 0.8439 - val_loss: 0.5558 - val_accuracy: 0.7415\n","Epoch 5/20\n","76/76 [==============================] - 3s 37ms/step - loss: 0.3118 - accuracy: 0.8755 - val_loss: 0.6097 - val_accuracy: 0.7333\n","Epoch 6/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.2778 - accuracy: 0.8907 - val_loss: 0.5998 - val_accuracy: 0.7314\n","Epoch 7/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.2485 - accuracy: 0.9035 - val_loss: 0.6494 - val_accuracy: 0.7246\n","Epoch 8/20\n","76/76 [==============================] - 3s 34ms/step - loss: 0.2214 - accuracy: 0.9153 - val_loss: 0.7153 - val_accuracy: 0.7169\n","Epoch 9/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.1992 - accuracy: 0.9294 - val_loss: 0.7647 - val_accuracy: 0.7174\n","Epoch 10/20\n","76/76 [==============================] - 3s 38ms/step - loss: 0.1832 - accuracy: 0.9302 - val_loss: 0.8269 - val_accuracy: 0.7000\n","Epoch 11/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.1608 - accuracy: 0.9364 - val_loss: 0.9088 - val_accuracy: 0.7024\n","Epoch 12/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.1488 - accuracy: 0.9439 - val_loss: 0.9828 - val_accuracy: 0.6957\n","Epoch 13/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.1363 - accuracy: 0.9468 - val_loss: 1.0714 - val_accuracy: 0.6942\n","Epoch 14/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.1213 - accuracy: 0.9565 - val_loss: 1.1597 - val_accuracy: 0.6947\n","Epoch 15/20\n","76/76 [==============================] - 3s 40ms/step - loss: 0.1151 - accuracy: 0.9578 - val_loss: 1.1670 - val_accuracy: 0.6836\n","Epoch 16/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.1041 - accuracy: 0.9617 - val_loss: 1.2188 - val_accuracy: 0.6826\n","Epoch 17/20\n","76/76 [==============================] - 3s 34ms/step - loss: 0.1037 - accuracy: 0.9640 - val_loss: 1.2508 - val_accuracy: 0.6749\n","Epoch 18/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.0972 - accuracy: 0.9660 - val_loss: 1.3399 - val_accuracy: 0.6792\n","Epoch 19/20\n","76/76 [==============================] - 3s 35ms/step - loss: 0.0885 - accuracy: 0.9665 - val_loss: 1.3320 - val_accuracy: 0.6821\n","Epoch 20/20\n","76/76 [==============================] - 3s 41ms/step - loss: 0.0862 - accuracy: 0.9702 - val_loss: 1.3651 - val_accuracy: 0.6749\n","65/65 [==============================] - 2s 16ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 22ms/step - loss: 0.6301 - accuracy: 0.6388 - val_loss: 0.6738 - val_accuracy: 0.5372\n","Epoch 2/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.4078 - accuracy: 0.8192 - val_loss: 0.6661 - val_accuracy: 0.5362\n","Epoch 3/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.2804 - accuracy: 0.8853 - val_loss: 0.6242 - val_accuracy: 0.7386\n","Epoch 4/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.1874 - accuracy: 0.9236 - val_loss: 0.6283 - val_accuracy: 0.6290\n","Epoch 5/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.1258 - accuracy: 0.9509 - val_loss: 0.5756 - val_accuracy: 0.7106\n","Epoch 6/20\n","76/76 [==============================] - 1s 18ms/step - loss: 0.0875 - accuracy: 0.9648 - val_loss: 1.0995 - val_accuracy: 0.5773\n","Epoch 7/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0670 - accuracy: 0.9729 - val_loss: 1.1772 - val_accuracy: 0.5435\n","Epoch 8/20\n","76/76 [==============================] - 1s 19ms/step - loss: 0.0519 - accuracy: 0.9805 - val_loss: 0.9773 - val_accuracy: 0.6435\n","Epoch 9/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0399 - accuracy: 0.9845 - val_loss: 1.1107 - val_accuracy: 0.6816\n","Epoch 10/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.0317 - accuracy: 0.9882 - val_loss: 2.9930 - val_accuracy: 0.5981\n","Epoch 11/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0298 - accuracy: 0.9909 - val_loss: 1.4006 - val_accuracy: 0.6966\n","Epoch 12/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0262 - accuracy: 0.9907 - val_loss: 1.8665 - val_accuracy: 0.6763\n","Epoch 13/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 1.5693 - val_accuracy: 0.6855\n","Epoch 14/20\n","76/76 [==============================] - 1s 20ms/step - loss: 0.0190 - accuracy: 0.9938 - val_loss: 1.9056 - val_accuracy: 0.6797\n","Epoch 15/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0170 - accuracy: 0.9948 - val_loss: 1.7866 - val_accuracy: 0.6845\n","Epoch 16/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 1.9966 - val_accuracy: 0.6720\n","Epoch 17/20\n","76/76 [==============================] - 1s 17ms/step - loss: 0.0157 - accuracy: 0.9952 - val_loss: 2.2571 - val_accuracy: 0.6580\n","Epoch 18/20\n","76/76 [==============================] - 2s 20ms/step - loss: 0.0152 - accuracy: 0.9959 - val_loss: 2.2965 - val_accuracy: 0.6720\n","Epoch 19/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0146 - accuracy: 0.9971 - val_loss: 2.1838 - val_accuracy: 0.6850\n","Epoch 20/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 3.2127 - val_accuracy: 0.6362\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 4s 32ms/step - loss: 0.6855 - accuracy: 0.6579 - val_loss: 0.7450 - val_accuracy: 0.4783\n","Epoch 2/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.4109 - accuracy: 0.8109 - val_loss: 0.6492 - val_accuracy: 0.7449\n","Epoch 3/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.2994 - accuracy: 0.8691 - val_loss: 0.6888 - val_accuracy: 0.4942\n","Epoch 4/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.2031 - accuracy: 0.9165 - val_loss: 0.6383 - val_accuracy: 0.6343\n","Epoch 5/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.1428 - accuracy: 0.9437 - val_loss: 0.6314 - val_accuracy: 0.6329\n","Epoch 6/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.1061 - accuracy: 0.9600 - val_loss: 0.5995 - val_accuracy: 0.6865\n","Epoch 7/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0854 - accuracy: 0.9665 - val_loss: 0.7068 - val_accuracy: 0.6787\n","Epoch 8/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.0704 - accuracy: 0.9749 - val_loss: 1.1768 - val_accuracy: 0.6614\n","Epoch 9/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.0603 - accuracy: 0.9789 - val_loss: 1.0691 - val_accuracy: 0.6918\n","Epoch 10/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.0519 - accuracy: 0.9816 - val_loss: 2.6589 - val_accuracy: 0.5594\n","Epoch 11/20\n","76/76 [==============================] - 2s 31ms/step - loss: 0.0433 - accuracy: 0.9843 - val_loss: 1.5767 - val_accuracy: 0.6691\n","Epoch 12/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.0391 - accuracy: 0.9876 - val_loss: 2.5723 - val_accuracy: 0.5976\n","Epoch 13/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 1.7528 - val_accuracy: 0.6739\n","Epoch 14/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0361 - accuracy: 0.9899 - val_loss: 2.0108 - val_accuracy: 0.6874\n","Epoch 15/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0298 - accuracy: 0.9909 - val_loss: 4.5655 - val_accuracy: 0.5903\n","Epoch 16/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.0243 - accuracy: 0.9938 - val_loss: 3.5570 - val_accuracy: 0.6005\n","Epoch 17/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.0251 - accuracy: 0.9915 - val_loss: 2.1849 - val_accuracy: 0.6618\n","Epoch 18/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0232 - accuracy: 0.9936 - val_loss: 2.7021 - val_accuracy: 0.6792\n","Epoch 19/20\n","76/76 [==============================] - 2s 30ms/step - loss: 0.0210 - accuracy: 0.9936 - val_loss: 2.6020 - val_accuracy: 0.6454\n","Epoch 20/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0202 - accuracy: 0.9928 - val_loss: 2.6676 - val_accuracy: 0.6367\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 7s 52ms/step - loss: 0.6303 - accuracy: 0.6368 - val_loss: 0.5223 - val_accuracy: 0.7435\n","Epoch 2/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.4631 - accuracy: 0.7927 - val_loss: 0.5135 - val_accuracy: 0.7614\n","Epoch 3/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.3916 - accuracy: 0.8339 - val_loss: 0.5232 - val_accuracy: 0.7565\n","Epoch 4/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.3325 - accuracy: 0.8654 - val_loss: 0.5369 - val_accuracy: 0.7527\n","Epoch 5/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.2846 - accuracy: 0.8853 - val_loss: 0.6817 - val_accuracy: 0.6995\n","Epoch 6/20\n","76/76 [==============================] - 4s 49ms/step - loss: 0.2447 - accuracy: 0.9056 - val_loss: 0.6932 - val_accuracy: 0.7251\n","Epoch 7/20\n","76/76 [==============================] - 3s 45ms/step - loss: 0.2129 - accuracy: 0.9151 - val_loss: 0.7199 - val_accuracy: 0.7208\n","Epoch 8/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.1872 - accuracy: 0.9323 - val_loss: 0.7756 - val_accuracy: 0.7169\n","Epoch 9/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.1654 - accuracy: 0.9402 - val_loss: 0.9464 - val_accuracy: 0.7087\n","Epoch 10/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.1504 - accuracy: 0.9468 - val_loss: 0.9040 - val_accuracy: 0.7019\n","Epoch 11/20\n","76/76 [==============================] - 3s 44ms/step - loss: 0.1285 - accuracy: 0.9524 - val_loss: 1.0920 - val_accuracy: 0.6894\n","Epoch 12/20\n","76/76 [==============================] - 3s 40ms/step - loss: 0.1210 - accuracy: 0.9549 - val_loss: 1.0624 - val_accuracy: 0.6894\n","Epoch 13/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.1089 - accuracy: 0.9609 - val_loss: 1.1600 - val_accuracy: 0.6937\n","Epoch 14/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.0978 - accuracy: 0.9656 - val_loss: 1.2891 - val_accuracy: 0.6821\n","Epoch 15/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.0913 - accuracy: 0.9660 - val_loss: 1.3893 - val_accuracy: 0.6739\n","Epoch 16/20\n","76/76 [==============================] - 3s 46ms/step - loss: 0.0873 - accuracy: 0.9675 - val_loss: 1.3100 - val_accuracy: 0.6816\n","Epoch 17/20\n","76/76 [==============================] - 3s 43ms/step - loss: 0.0764 - accuracy: 0.9729 - val_loss: 1.5133 - val_accuracy: 0.6729\n","Epoch 18/20\n","76/76 [==============================] - 3s 39ms/step - loss: 0.0730 - accuracy: 0.9741 - val_loss: 1.4028 - val_accuracy: 0.6734\n","Epoch 19/20\n","76/76 [==============================] - 3s 42ms/step - loss: 0.0647 - accuracy: 0.9768 - val_loss: 1.6451 - val_accuracy: 0.6671\n","Epoch 20/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.0638 - accuracy: 0.9785 - val_loss: 1.6088 - val_accuracy: 0.6696\n","65/65 [==============================] - 1s 12ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 31ms/step - loss: 0.6681 - accuracy: 0.6507 - val_loss: 0.6712 - val_accuracy: 0.6546\n","Epoch 2/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.3933 - accuracy: 0.8258 - val_loss: 0.6519 - val_accuracy: 0.6816\n","Epoch 3/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.2469 - accuracy: 0.8975 - val_loss: 0.6789 - val_accuracy: 0.5406\n","Epoch 4/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.1405 - accuracy: 0.9462 - val_loss: 0.6382 - val_accuracy: 0.6053\n","Epoch 5/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0949 - accuracy: 0.9642 - val_loss: 0.8047 - val_accuracy: 0.5420\n","Epoch 6/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.0688 - accuracy: 0.9718 - val_loss: 0.7666 - val_accuracy: 0.6029\n","Epoch 7/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0571 - accuracy: 0.9805 - val_loss: 1.1774 - val_accuracy: 0.5618\n","Epoch 8/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0397 - accuracy: 0.9851 - val_loss: 0.9076 - val_accuracy: 0.6768\n","Epoch 9/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0352 - accuracy: 0.9880 - val_loss: 1.6941 - val_accuracy: 0.6517\n","Epoch 10/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.0348 - accuracy: 0.9882 - val_loss: 1.3564 - val_accuracy: 0.6821\n","Epoch 11/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 1.5541 - val_accuracy: 0.6715\n","Epoch 12/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0232 - accuracy: 0.9923 - val_loss: 1.8148 - val_accuracy: 0.6778\n","Epoch 13/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 5.9545 - val_accuracy: 0.5048\n","Epoch 14/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 2.2031 - val_accuracy: 0.6560\n","Epoch 15/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.0173 - accuracy: 0.9952 - val_loss: 2.6617 - val_accuracy: 0.6300\n","Epoch 16/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0215 - accuracy: 0.9936 - val_loss: 2.1649 - val_accuracy: 0.6783\n","Epoch 17/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0150 - accuracy: 0.9967 - val_loss: 2.4790 - val_accuracy: 0.6478\n","Epoch 18/20\n","76/76 [==============================] - 2s 29ms/step - loss: 0.0163 - accuracy: 0.9957 - val_loss: 2.8599 - val_accuracy: 0.6444\n","Epoch 19/20\n","76/76 [==============================] - 2s 28ms/step - loss: 0.0145 - accuracy: 0.9957 - val_loss: 3.8877 - val_accuracy: 0.5894\n","Epoch 20/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.0138 - accuracy: 0.9971 - val_loss: 4.0869 - val_accuracy: 0.6415\n","65/65 [==============================] - 1s 7ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 27ms/step - loss: 0.6173 - accuracy: 0.6712 - val_loss: 0.6714 - val_accuracy: 0.6357\n","Epoch 2/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.4152 - accuracy: 0.8116 - val_loss: 0.6557 - val_accuracy: 0.6884\n","Epoch 3/20\n","76/76 [==============================] - 2s 27ms/step - loss: 0.3141 - accuracy: 0.8691 - val_loss: 0.6287 - val_accuracy: 0.6768\n","Epoch 4/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.2296 - accuracy: 0.9085 - val_loss: 0.6056 - val_accuracy: 0.6705\n","Epoch 5/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.1671 - accuracy: 0.9331 - val_loss: 0.6104 - val_accuracy: 0.6609\n","Epoch 6/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.1298 - accuracy: 0.9484 - val_loss: 0.7005 - val_accuracy: 0.6319\n","Epoch 7/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.1041 - accuracy: 0.9567 - val_loss: 1.1679 - val_accuracy: 0.6164\n","Epoch 8/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0813 - accuracy: 0.9708 - val_loss: 1.1322 - val_accuracy: 0.6372\n","Epoch 9/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.0656 - accuracy: 0.9756 - val_loss: 1.2542 - val_accuracy: 0.6710\n","Epoch 10/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.0563 - accuracy: 0.9799 - val_loss: 1.2981 - val_accuracy: 0.6763\n","Epoch 11/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.0513 - accuracy: 0.9814 - val_loss: 3.1769 - val_accuracy: 0.6106\n","Epoch 12/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0444 - accuracy: 0.9849 - val_loss: 1.9097 - val_accuracy: 0.6527\n","Epoch 13/20\n","76/76 [==============================] - 2s 22ms/step - loss: 0.0417 - accuracy: 0.9851 - val_loss: 5.5035 - val_accuracy: 0.5710\n","Epoch 14/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.0343 - accuracy: 0.9878 - val_loss: 2.2391 - val_accuracy: 0.6459\n","Epoch 15/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0354 - accuracy: 0.9876 - val_loss: 1.7896 - val_accuracy: 0.6841\n","Epoch 16/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.0325 - accuracy: 0.9899 - val_loss: 2.4752 - val_accuracy: 0.6512\n","Epoch 17/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0284 - accuracy: 0.9911 - val_loss: 2.7508 - val_accuracy: 0.6435\n","Epoch 18/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.0294 - accuracy: 0.9903 - val_loss: 2.3184 - val_accuracy: 0.6691\n","Epoch 19/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.0257 - accuracy: 0.9909 - val_loss: 2.3264 - val_accuracy: 0.6604\n","Epoch 20/20\n","76/76 [==============================] - 2s 26ms/step - loss: 0.0203 - accuracy: 0.9919 - val_loss: 3.8151 - val_accuracy: 0.6232\n","65/65 [==============================] - 1s 6ms/step\n","Epoch 1/20\n","76/76 [==============================] - 7s 54ms/step - loss: 0.6934 - accuracy: 0.5100 - val_loss: 0.6938 - val_accuracy: 0.4778\n","Epoch 2/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.6936 - accuracy: 0.5198 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 4s 49ms/step - loss: 0.6930 - accuracy: 0.5243 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6932 - accuracy: 0.5183 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6928 - accuracy: 0.5165 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.6930 - accuracy: 0.5200 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6928 - accuracy: 0.5144 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6926 - accuracy: 0.5200 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6928 - accuracy: 0.5156 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 4s 54ms/step - loss: 0.6925 - accuracy: 0.5163 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6926 - accuracy: 0.5227 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6928 - accuracy: 0.5165 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.6926 - accuracy: 0.5208 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 4s 52ms/step - loss: 0.6929 - accuracy: 0.5144 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6929 - accuracy: 0.5189 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6926 - accuracy: 0.5177 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.6922 - accuracy: 0.5216 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6929 - accuracy: 0.5204 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6917 - accuracy: 0.5171 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 4s 47ms/step - loss: 0.6926 - accuracy: 0.5233 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 22ms/step\n","Epoch 1/20\n","76/76 [==============================] - 3s 27ms/step - loss: 0.6190 - accuracy: 0.6589 - val_loss: 0.6730 - val_accuracy: 0.7386\n","Epoch 2/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.4217 - accuracy: 0.8053 - val_loss: 0.6564 - val_accuracy: 0.6048\n","Epoch 3/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.2973 - accuracy: 0.8758 - val_loss: 0.8237 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.2033 - accuracy: 0.9176 - val_loss: 0.6285 - val_accuracy: 0.6097\n","Epoch 5/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.1376 - accuracy: 0.9482 - val_loss: 0.6386 - val_accuracy: 0.6348\n","Epoch 6/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.0954 - accuracy: 0.9629 - val_loss: 0.6142 - val_accuracy: 0.6754\n","Epoch 7/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.0687 - accuracy: 0.9768 - val_loss: 1.0287 - val_accuracy: 0.6295\n","Epoch 8/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0556 - accuracy: 0.9801 - val_loss: 4.0833 - val_accuracy: 0.5242\n","Epoch 9/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0442 - accuracy: 0.9841 - val_loss: 1.2608 - val_accuracy: 0.6420\n","Epoch 10/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 3.7182 - val_accuracy: 0.5739\n","Epoch 11/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0304 - accuracy: 0.9888 - val_loss: 2.3506 - val_accuracy: 0.6000\n","Epoch 12/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0266 - accuracy: 0.9919 - val_loss: 2.9823 - val_accuracy: 0.6266\n","Epoch 13/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0268 - accuracy: 0.9930 - val_loss: 1.5680 - val_accuracy: 0.6865\n","Epoch 14/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0212 - accuracy: 0.9938 - val_loss: 5.7893 - val_accuracy: 0.5589\n","Epoch 15/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.0196 - accuracy: 0.9930 - val_loss: 1.8930 - val_accuracy: 0.6768\n","Epoch 16/20\n","76/76 [==============================] - 2s 25ms/step - loss: 0.0211 - accuracy: 0.9934 - val_loss: 2.3214 - val_accuracy: 0.6792\n","Epoch 17/20\n","76/76 [==============================] - 2s 24ms/step - loss: 0.0184 - accuracy: 0.9948 - val_loss: 2.0471 - val_accuracy: 0.6792\n","Epoch 18/20\n","76/76 [==============================] - 2s 23ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 2.1670 - val_accuracy: 0.6739\n","Epoch 19/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0148 - accuracy: 0.9961 - val_loss: 2.9183 - val_accuracy: 0.6633\n","Epoch 20/20\n","76/76 [==============================] - 2s 21ms/step - loss: 0.0114 - accuracy: 0.9959 - val_loss: 2.6097 - val_accuracy: 0.6454\n","65/65 [==============================] - 0s 5ms/step\n","Epoch 1/20\n","76/76 [==============================] - 6s 68ms/step - loss: 0.8168 - accuracy: 0.6426 - val_loss: 0.6729 - val_accuracy: 0.5232\n","Epoch 2/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.4341 - accuracy: 0.8004 - val_loss: 0.7029 - val_accuracy: 0.4787\n","Epoch 3/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.3007 - accuracy: 0.8710 - val_loss: 0.8674 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.2031 - accuracy: 0.9163 - val_loss: 0.6122 - val_accuracy: 0.6580\n","Epoch 5/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.1378 - accuracy: 0.9462 - val_loss: 0.8706 - val_accuracy: 0.5251\n","Epoch 6/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.1133 - accuracy: 0.9573 - val_loss: 0.7784 - val_accuracy: 0.6618\n","Epoch 7/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.0857 - accuracy: 0.9677 - val_loss: 0.8442 - val_accuracy: 0.6918\n","Epoch 8/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0651 - accuracy: 0.9770 - val_loss: 4.0933 - val_accuracy: 0.4928\n","Epoch 9/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0596 - accuracy: 0.9797 - val_loss: 1.7110 - val_accuracy: 0.6454\n","Epoch 10/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.0501 - accuracy: 0.9816 - val_loss: 4.4718 - val_accuracy: 0.5884\n","Epoch 11/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.0485 - accuracy: 0.9830 - val_loss: 4.5578 - val_accuracy: 0.5928\n","Epoch 12/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0454 - accuracy: 0.9855 - val_loss: 2.7300 - val_accuracy: 0.6179\n","Epoch 13/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.0441 - accuracy: 0.9878 - val_loss: 11.0974 - val_accuracy: 0.5285\n","Epoch 14/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.0347 - accuracy: 0.9880 - val_loss: 2.8039 - val_accuracy: 0.6681\n","Epoch 15/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0349 - accuracy: 0.9903 - val_loss: 6.0381 - val_accuracy: 0.5309\n","Epoch 16/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0276 - accuracy: 0.9911 - val_loss: 2.3151 - val_accuracy: 0.6609\n","Epoch 17/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0297 - accuracy: 0.9907 - val_loss: 3.9066 - val_accuracy: 0.6493\n","Epoch 18/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0267 - accuracy: 0.9938 - val_loss: 3.5124 - val_accuracy: 0.6556\n","Epoch 19/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.0298 - accuracy: 0.9925 - val_loss: 6.2999 - val_accuracy: 0.5348\n","Epoch 20/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.0198 - accuracy: 0.9936 - val_loss: 2.8197 - val_accuracy: 0.6812\n","65/65 [==============================] - 1s 10ms/step\n","Epoch 1/20\n","76/76 [==============================] - 8s 71ms/step - loss: 0.6937 - accuracy: 0.5177 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.6938 - accuracy: 0.5125 - val_loss: 0.6932 - val_accuracy: 0.4783\n","Epoch 3/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6937 - accuracy: 0.5096 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.6924 - accuracy: 0.5233 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 63ms/step - loss: 0.6930 - accuracy: 0.5169 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6932 - accuracy: 0.5189 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.6931 - accuracy: 0.5194 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6924 - accuracy: 0.5250 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6930 - accuracy: 0.5183 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.6927 - accuracy: 0.5229 - val_loss: 0.6935 - val_accuracy: 0.4783\n","Epoch 12/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6931 - accuracy: 0.5038 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6929 - accuracy: 0.5154 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6924 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 5s 64ms/step - loss: 0.6925 - accuracy: 0.5198 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6926 - accuracy: 0.5202 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6925 - accuracy: 0.5206 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 5s 62ms/step - loss: 0.6925 - accuracy: 0.5218 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6926 - accuracy: 0.5189 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.6925 - accuracy: 0.5196 - val_loss: 0.6921 - val_accuracy: 0.5227\n","65/65 [==============================] - 2s 24ms/step\n","Epoch 1/20\n","76/76 [==============================] - 6s 59ms/step - loss: 0.7398 - accuracy: 0.6455 - val_loss: 0.7559 - val_accuracy: 0.4773\n","Epoch 2/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.4025 - accuracy: 0.8184 - val_loss: 0.7656 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.2413 - accuracy: 0.8977 - val_loss: 0.7396 - val_accuracy: 0.5251\n","Epoch 4/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.1499 - accuracy: 0.9441 - val_loss: 1.3175 - val_accuracy: 0.5232\n","Epoch 5/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.1002 - accuracy: 0.9600 - val_loss: 0.8621 - val_accuracy: 0.5802\n","Epoch 6/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.0763 - accuracy: 0.9691 - val_loss: 0.8761 - val_accuracy: 0.5899\n","Epoch 7/20\n","76/76 [==============================] - 5s 59ms/step - loss: 0.0623 - accuracy: 0.9766 - val_loss: 4.1678 - val_accuracy: 0.4797\n","Epoch 8/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.0482 - accuracy: 0.9857 - val_loss: 0.9609 - val_accuracy: 0.6903\n","Epoch 9/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.0451 - accuracy: 0.9836 - val_loss: 3.3805 - val_accuracy: 0.5758\n","Epoch 10/20\n","76/76 [==============================] - 4s 57ms/step - loss: 0.0398 - accuracy: 0.9870 - val_loss: 1.4649 - val_accuracy: 0.6860\n","Epoch 11/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 2.2210 - val_accuracy: 0.6507\n","Epoch 12/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.0270 - accuracy: 0.9940 - val_loss: 2.7641 - val_accuracy: 0.6763\n","Epoch 13/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.0251 - accuracy: 0.9934 - val_loss: 3.7795 - val_accuracy: 0.6444\n","Epoch 14/20\n","76/76 [==============================] - 4s 55ms/step - loss: 0.0267 - accuracy: 0.9938 - val_loss: 2.2887 - val_accuracy: 0.6932\n","Epoch 15/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.0190 - accuracy: 0.9957 - val_loss: 2.5209 - val_accuracy: 0.6918\n","Epoch 16/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.0230 - accuracy: 0.9944 - val_loss: 3.4931 - val_accuracy: 0.6150\n","Epoch 17/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.0173 - accuracy: 0.9961 - val_loss: 12.8472 - val_accuracy: 0.5329\n","Epoch 18/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.0245 - accuracy: 0.9954 - val_loss: 5.6129 - val_accuracy: 0.6203\n","Epoch 19/20\n","76/76 [==============================] - 4s 58ms/step - loss: 0.0173 - accuracy: 0.9971 - val_loss: 6.8560 - val_accuracy: 0.5203\n","Epoch 20/20\n","76/76 [==============================] - 4s 56ms/step - loss: 0.0248 - accuracy: 0.9948 - val_loss: 5.7172 - val_accuracy: 0.6077\n","65/65 [==============================] - 1s 11ms/step\n"]}],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","\n","y = NepalEQ_df_target\n","VS = [200, 200, 300, 300]\n","DPL = [100, 200, 100, 300]\n","final_metrics_list = []  # List to store the metrics for each iteration\n","metrics_list = []  # List to store the metrics for each iteration\n","for X, n in zip(df_list, df_names):\n","  y = NepalEQ_df_target\n","\n","  # split the data into train and test sets\n","  X_text_train, X_text_test, y_train, y_test =train_test_split(X, y,test_size=0.3,stratify=y)\n","\n","  num_classes = 2\n","\n","  y_train = keras.utils.to_categorical(y_train, num_classes)\n","  y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","  \n","\n","  for vs, dpl in zip(VS, DPL):\n","    \n","    MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","    MAX_SEQUENCE_LENGTH = vs # Maximum length of the text sequence\n","    EMBEDDING_DIM = dpl # Size of the word embeddings\n","    NUM_CLASSES = 2 # Number of unique classes\n","\n","    # pad the text sequences\n","    X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","    X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","    ### LSTM\n","\n","    # define the text input layer\n","    text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","    text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","    text_LSTM = CuDNNLSTM(EMBEDDING_DIM, input_shape=(None,1), return_sequences=True, name='text_LSTM')(text_embed)\n","    text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","    text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","    text_flatten = Flatten(name='text_flatten')(text_norm)\n","    output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","    # define the model\n","    model = Model(inputs=[text_input], outputs=[output])\n","\n","    # set batch size\n","    batch_size = 64\n","\n","    # initiate RMSprop optimizer\n","    opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","    # Let's train the model using RMSprop\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=opt,\n","                  metrics=['accuracy'])\n","\n","\n","    # train the model\n","    model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","              validation_data=(np.array(X_text_test), np.array(y_test)),\n","              epochs=20, batch_size=batch_size)\n","    \n","\n","\n","    # Compute the metrics\n","    y_pred = np.argmax(model.predict(X_text_test), axis=1)\n","    y_test_binary = np.argmax(y_test, axis=1)\n","    accuracy = accuracy_score(y_test_binary, y_pred)\n","    precision = precision_score(y_test_binary, y_pred)\n","    recall = recall_score(y_test_binary, y_pred)\n","    f1 = f1_score(y_test_binary, y_pred)\n","    roc_auc = roc_auc_score(y_test_binary, y_pred)\n","\n","    metrics_list.append({'name': n,'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1, 'ROC AUC': roc_auc})\n","\n","    \n","    \n","\n","\n","\n","    ### Bi-LSTM\n","\n","    # define the text input layer\n","    text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","    text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","    text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","    text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","    text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","    text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","    output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","    # define the model\n","    model = Model(inputs=[text_input], outputs=[output])\n","\n","    # set batch size\n","    batch_size = 64\n","\n","    # initiate RMSprop optimizer\n","    opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","    # Let's train the model using RMSprop\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=opt,\n","                  metrics=['accuracy'])\n","\n","\n","    # train the model\n","    model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","              validation_data=(np.array(X_text_test), np.array(y_test)),\n","              epochs=20, batch_size=batch_size)\n","    \n","\n","    # Compute the metrics\n","    y_pred = np.argmax(model.predict(X_text_test), axis=1)\n","    y_test_binary = np.argmax(y_test, axis=1)\n","    accuracy = accuracy_score(y_test_binary, y_pred)\n","    precision = precision_score(y_test_binary, y_pred)\n","    recall = recall_score(y_test_binary, y_pred)\n","    f1 = f1_score(y_test_binary, y_pred)\n","    roc_auc = roc_auc_score(y_test_binary, y_pred)\n","\n","    metrics_list.append({'name': n,'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1, 'ROC AUC': roc_auc})\n","\n","    \n","    \n","\n","\n","\n","    ### GRU\n","\n","    # define the text input layer\n","    text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","    text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","    text_LSTM = CuDNNGRU(EMBEDDING_DIM, input_shape=(None,1), return_sequences=True, name='text_LSTM')(text_embed)\n","    text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","    text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","    text_flatten = Flatten(name='text_flatten')(text_norm)\n","    output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","    # define the model\n","    model = Model(inputs=[text_input], outputs=[output])\n","\n","    # set batch size\n","    batch_size = 64\n","\n","    # initiate RMSprop optimizer\n","    opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","    # Let's train the model using RMSprop\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=opt,\n","                  metrics=['accuracy'])\n","\n","\n","    # train the model\n","    model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","              validation_data=(np.array(X_text_test), np.array(y_test)),\n","              epochs=20, batch_size=batch_size)\n","    \n","\n","    # Compute the metrics\n","    y_pred = np.argmax(model.predict(X_text_test), axis=1)\n","    y_test_binary = np.argmax(y_test, axis=1)\n","    accuracy = accuracy_score(y_test_binary, y_pred)\n","    precision = precision_score(y_test_binary, y_pred)\n","    recall = recall_score(y_test_binary, y_pred)\n","    f1 = f1_score(y_test_binary, y_pred)\n","    roc_auc = roc_auc_score(y_test_binary, y_pred)\n","\n","    metrics_list.append({'name': n,'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1': f1, 'ROC AUC': roc_auc})\n","\n","    \n","\n","# Create the final dataframe\n","metrics_df = pd.DataFrame(metrics_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"F9CjbsHf1sP5"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-23202cc2-bc61-4a52-b4e1-8f9cdeede2ff\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003ename\u003c/th\u003e\n","      \u003cth\u003eAccuracy\u003c/th\u003e\n","      \u003cth\u003ePrecision\u003c/th\u003e\n","      \u003cth\u003eRecall\u003c/th\u003e\n","      \u003cth\u003eF1\u003c/th\u003e\n","      \u003cth\u003eROC AUC\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e1.000000\u003c/td\u003e\n","      \u003ctd\u003e0.646174\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e1.000000\u003c/td\u003e\n","      \u003ctd\u003e0.646174\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e5\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e1.000000\u003c/td\u003e\n","      \u003ctd\u003e0.646174\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e6\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e8\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e9\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e10\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e11\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_word2vec\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e1.000000\u003c/td\u003e\n","      \u003ctd\u003e0.646174\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e12\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e13\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e14\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e0.477295\u003c/td\u003e\n","      \u003ctd\u003e1.000000\u003c/td\u003e\n","      \u003ctd\u003e0.646174\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e15\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e16\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e17\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e18\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e19\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e20\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e21\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e22\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e23\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_fasttext\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e24\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e25\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e26\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e27\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e28\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e29\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.476812\u003c/td\u003e\n","      \u003ctd\u003e0.477042\u003c/td\u003e\n","      \u003ctd\u003e0.998988\u003c/td\u003e\n","      \u003ctd\u003e0.645731\u003c/td\u003e\n","      \u003ctd\u003e0.499494\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e30\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e31\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e32\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e33\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.476812\u003c/td\u003e\n","      \u003ctd\u003e0.477042\u003c/td\u003e\n","      \u003ctd\u003e0.998988\u003c/td\u003e\n","      \u003ctd\u003e0.645731\u003c/td\u003e\n","      \u003ctd\u003e0.499494\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e34\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e35\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_embeddings_glove\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e36\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.665217\u003c/td\u003e\n","      \u003ctd\u003e0.742998\u003c/td\u003e\n","      \u003ctd\u003e0.456478\u003c/td\u003e\n","      \u003ctd\u003e0.565517\u003c/td\u003e\n","      \u003ctd\u003e0.656150\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e37\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.674879\u003c/td\u003e\n","      \u003ctd\u003e0.661208\u003c/td\u003e\n","      \u003ctd\u003e0.653846\u003c/td\u003e\n","      \u003ctd\u003e0.657506\u003c/td\u003e\n","      \u003ctd\u003e0.673966\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e38\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.636232\u003c/td\u003e\n","      \u003ctd\u003e0.753780\u003c/td\u003e\n","      \u003ctd\u003e0.353239\u003c/td\u003e\n","      \u003ctd\u003e0.481048\u003c/td\u003e\n","      \u003ctd\u003e0.623939\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e39\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.636715\u003c/td\u003e\n","      \u003ctd\u003e0.588589\u003c/td\u003e\n","      \u003ctd\u003e0.793522\u003c/td\u003e\n","      \u003ctd\u003e0.675862\u003c/td\u003e\n","      \u003ctd\u003e0.643526\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e40\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.669565\u003c/td\u003e\n","      \u003ctd\u003e0.639706\u003c/td\u003e\n","      \u003ctd\u003e0.704453\u003c/td\u003e\n","      \u003ctd\u003e0.670520\u003c/td\u003e\n","      \u003ctd\u003e0.671081\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e41\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.641546\u003c/td\u003e\n","      \u003ctd\u003e0.788732\u003c/td\u003e\n","      \u003ctd\u003e0.340081\u003c/td\u003e\n","      \u003ctd\u003e0.475248\u003c/td\u003e\n","      \u003ctd\u003e0.628451\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e42\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.623188\u003c/td\u003e\n","      \u003ctd\u003e0.776596\u003c/td\u003e\n","      \u003ctd\u003e0.295547\u003c/td\u003e\n","      \u003ctd\u003e0.428152\u003c/td\u003e\n","      \u003ctd\u003e0.608956\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e43\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e44\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.645411\u003c/td\u003e\n","      \u003ctd\u003e0.596358\u003c/td\u003e\n","      \u003ctd\u003e0.795547\u003c/td\u003e\n","      \u003ctd\u003e0.681700\u003c/td\u003e\n","      \u003ctd\u003e0.651932\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e45\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.681159\u003c/td\u003e\n","      \u003ctd\u003e0.699029\u003c/td\u003e\n","      \u003ctd\u003e0.582996\u003c/td\u003e\n","      \u003ctd\u003e0.635762\u003c/td\u003e\n","      \u003ctd\u003e0.676895\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e46\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.522705\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.000000\u003c/td\u003e\n","      \u003ctd\u003e0.500000\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e47\u003c/th\u003e\n","      \u003ctd\u003eNepalEQ_df_bert_input\u003c/td\u003e\n","      \u003ctd\u003e0.607729\u003c/td\u003e\n","      \u003ctd\u003e0.838462\u003c/td\u003e\n","      \u003ctd\u003e0.220648\u003c/td\u003e\n","      \u003ctd\u003e0.349359\u003c/td\u003e\n","      \u003ctd\u003e0.590915\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-23202cc2-bc61-4a52-b4e1-8f9cdeede2ff')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-23202cc2-bc61-4a52-b4e1-8f9cdeede2ff button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-23202cc2-bc61-4a52-b4e1-8f9cdeede2ff');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["                              name  Accuracy  Precision    Recall        F1  \\\n","0   NepalEQ_df_embeddings_word2vec  0.477295   0.477295  1.000000  0.646174   \n","1   NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","2   NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","3   NepalEQ_df_embeddings_word2vec  0.477295   0.477295  1.000000  0.646174   \n","4   NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","5   NepalEQ_df_embeddings_word2vec  0.477295   0.477295  1.000000  0.646174   \n","6   NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","7   NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","8   NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","9   NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","10  NepalEQ_df_embeddings_word2vec  0.522705   0.000000  0.000000  0.000000   \n","11  NepalEQ_df_embeddings_word2vec  0.477295   0.477295  1.000000  0.646174   \n","12  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","13  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","14  NepalEQ_df_embeddings_fasttext  0.477295   0.477295  1.000000  0.646174   \n","15  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","16  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","17  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","18  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","19  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","20  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","21  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","22  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","23  NepalEQ_df_embeddings_fasttext  0.522705   0.000000  0.000000  0.000000   \n","24     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","25     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","26     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","27     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","28     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","29     NepalEQ_df_embeddings_glove  0.476812   0.477042  0.998988  0.645731   \n","30     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","31     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","32     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","33     NepalEQ_df_embeddings_glove  0.476812   0.477042  0.998988  0.645731   \n","34     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","35     NepalEQ_df_embeddings_glove  0.522705   0.000000  0.000000  0.000000   \n","36           NepalEQ_df_bert_input  0.665217   0.742998  0.456478  0.565517   \n","37           NepalEQ_df_bert_input  0.674879   0.661208  0.653846  0.657506   \n","38           NepalEQ_df_bert_input  0.636232   0.753780  0.353239  0.481048   \n","39           NepalEQ_df_bert_input  0.636715   0.588589  0.793522  0.675862   \n","40           NepalEQ_df_bert_input  0.669565   0.639706  0.704453  0.670520   \n","41           NepalEQ_df_bert_input  0.641546   0.788732  0.340081  0.475248   \n","42           NepalEQ_df_bert_input  0.623188   0.776596  0.295547  0.428152   \n","43           NepalEQ_df_bert_input  0.522705   0.000000  0.000000  0.000000   \n","44           NepalEQ_df_bert_input  0.645411   0.596358  0.795547  0.681700   \n","45           NepalEQ_df_bert_input  0.681159   0.699029  0.582996  0.635762   \n","46           NepalEQ_df_bert_input  0.522705   0.000000  0.000000  0.000000   \n","47           NepalEQ_df_bert_input  0.607729   0.838462  0.220648  0.349359   \n","\n","     ROC AUC  \n","0   0.500000  \n","1   0.500000  \n","2   0.500000  \n","3   0.500000  \n","4   0.500000  \n","5   0.500000  \n","6   0.500000  \n","7   0.500000  \n","8   0.500000  \n","9   0.500000  \n","10  0.500000  \n","11  0.500000  \n","12  0.500000  \n","13  0.500000  \n","14  0.500000  \n","15  0.500000  \n","16  0.500000  \n","17  0.500000  \n","18  0.500000  \n","19  0.500000  \n","20  0.500000  \n","21  0.500000  \n","22  0.500000  \n","23  0.500000  \n","24  0.500000  \n","25  0.500000  \n","26  0.500000  \n","27  0.500000  \n","28  0.500000  \n","29  0.499494  \n","30  0.500000  \n","31  0.500000  \n","32  0.500000  \n","33  0.499494  \n","34  0.500000  \n","35  0.500000  \n","36  0.656150  \n","37  0.673966  \n","38  0.623939  \n","39  0.643526  \n","40  0.671081  \n","41  0.628451  \n","42  0.608956  \n","43  0.500000  \n","44  0.651932  \n","45  0.676895  \n","46  0.500000  \n","47  0.590915  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["metrics_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tDUQJDhDSDiX"},"outputs":[],"source":["# Create the final dataframe\n","# final_metrics_df = pd.DataFrame(final_metrics_list)\n","\n","# Save the dataframe as a CSV file\n","# final_metrics_df.to_csv('Queensland_DL_metrics.csv', index=False)\n","metrics_df.to_csv('NepalEQ_DL_metrics.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UljL6KAxiXt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Nt3IqYNIkGYU"},"source":["### Hashtags"]},{"cell_type":"markdown","metadata":{"id":"dHtt6SuWDrSz"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oh4NzQHVbK6N"},"outputs":[],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_hashtag, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fxly-AL4bK6P"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":786020,"status":"ok","timestamp":1678531236446,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"l1Laa6By5Nxy","outputId":"bd27cdc1-5d0a-4dc8-e4e2-d22dab85ed19"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 55s 603ms/step - loss: 0.6959 - accuracy: 0.5169 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 40s 523ms/step - loss: 0.6909 - accuracy: 0.5241 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 38s 503ms/step - loss: 0.6865 - accuracy: 0.5129 - val_loss: 0.6909 - val_accuracy: 0.5232\n","Epoch 4/20\n","76/76 [==============================] - 37s 490ms/step - loss: 0.6832 - accuracy: 0.5260 - val_loss: 0.6900 - val_accuracy: 0.5237\n","Epoch 5/20\n","76/76 [==============================] - 39s 519ms/step - loss: 0.6823 - accuracy: 0.5156 - val_loss: 0.6893 - val_accuracy: 0.5246\n","Epoch 6/20\n","76/76 [==============================] - 36s 479ms/step - loss: 0.6815 - accuracy: 0.5202 - val_loss: 0.6891 - val_accuracy: 0.5246\n","Epoch 7/20\n","76/76 [==============================] - 37s 485ms/step - loss: 0.6802 - accuracy: 0.5225 - val_loss: 0.6881 - val_accuracy: 0.5232\n","Epoch 8/20\n","76/76 [==============================] - 37s 492ms/step - loss: 0.6803 - accuracy: 0.5223 - val_loss: 0.6884 - val_accuracy: 0.5261\n","Epoch 9/20\n","76/76 [==============================] - 40s 520ms/step - loss: 0.6790 - accuracy: 0.5196 - val_loss: 0.6899 - val_accuracy: 0.5261\n","Epoch 10/20\n","76/76 [==============================] - 39s 518ms/step - loss: 0.6775 - accuracy: 0.5266 - val_loss: 0.6925 - val_accuracy: 0.5251\n","Epoch 11/20\n","76/76 [==============================] - 38s 500ms/step - loss: 0.6780 - accuracy: 0.5274 - val_loss: 0.6967 - val_accuracy: 0.4947\n","Epoch 12/20\n","76/76 [==============================] - 39s 508ms/step - loss: 0.6770 - accuracy: 0.5243 - val_loss: 0.6968 - val_accuracy: 0.5256\n","Epoch 13/20\n","76/76 [==============================] - 40s 522ms/step - loss: 0.6769 - accuracy: 0.5258 - val_loss: 0.6981 - val_accuracy: 0.4957\n","Epoch 14/20\n","76/76 [==============================] - 36s 477ms/step - loss: 0.6758 - accuracy: 0.5252 - val_loss: 0.7026 - val_accuracy: 0.5251\n","Epoch 15/20\n","76/76 [==============================] - 40s 525ms/step - loss: 0.6761 - accuracy: 0.5208 - val_loss: 0.6993 - val_accuracy: 0.5261\n","Epoch 16/20\n","76/76 [==============================] - 38s 501ms/step - loss: 0.6748 - accuracy: 0.5293 - val_loss: 0.7042 - val_accuracy: 0.4952\n","Epoch 17/20\n","76/76 [==============================] - 38s 504ms/step - loss: 0.6746 - accuracy: 0.5202 - val_loss: 0.7030 - val_accuracy: 0.5242\n","Epoch 18/20\n","76/76 [==============================] - 38s 496ms/step - loss: 0.6748 - accuracy: 0.5258 - val_loss: 0.7075 - val_accuracy: 0.5251\n","Epoch 19/20\n","76/76 [==============================] - 39s 515ms/step - loss: 0.6741 - accuracy: 0.5283 - val_loss: 0.7093 - val_accuracy: 0.5246\n","Epoch 20/20\n","76/76 [==============================] - 39s 517ms/step - loss: 0.6745 - accuracy: 0.5256 - val_loss: 0.7076 - val_accuracy: 0.5246\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f968e8ceb80\u003e"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2712,"status":"ok","timestamp":1678531239150,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"tD8k84IS7wLX","outputId":"aa04e901-39fd-478d-f92e-9a5e8b9c333c"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 33ms/step\n","precision: [0.52437811 0.53333333]\n","recall: [0.974122   0.03238866]\n","fscore: [0.68175938 0.0610687 ]\n","support: [1082  988]\n","accuracy: 0.5246376811594203\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.97      0.68      1082\n","           1       0.53      0.03      0.06       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.53      0.50      0.37      2070\n","weighted avg       0.53      0.52      0.39      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":745519,"status":"ok","timestamp":1678533908690,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"hXjjzhQN50TL","outputId":"cfc8bd2b-2f9b-425d-c8b0-596587bba8ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 52s 656ms/step - loss: 0.7008 - accuracy: 0.4939 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 36s 479ms/step - loss: 0.6896 - accuracy: 0.5138 - val_loss: 0.6918 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 36s 467ms/step - loss: 0.6855 - accuracy: 0.5283 - val_loss: 0.6911 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 35s 467ms/step - loss: 0.6819 - accuracy: 0.5247 - val_loss: 0.6902 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 38s 497ms/step - loss: 0.6781 - accuracy: 0.5250 - val_loss: 0.6905 - val_accuracy: 0.4957\n","Epoch 6/20\n","76/76 [==============================] - 33s 434ms/step - loss: 0.6768 - accuracy: 0.5289 - val_loss: 0.6893 - val_accuracy: 0.5242\n","Epoch 7/20\n","76/76 [==============================] - 37s 488ms/step - loss: 0.6744 - accuracy: 0.5347 - val_loss: 0.7210 - val_accuracy: 0.5251\n","Epoch 8/20\n","76/76 [==============================] - 35s 462ms/step - loss: 0.6738 - accuracy: 0.5276 - val_loss: 0.6955 - val_accuracy: 0.5232\n","Epoch 9/20\n","76/76 [==============================] - 35s 456ms/step - loss: 0.6736 - accuracy: 0.5260 - val_loss: 0.7009 - val_accuracy: 0.5246\n","Epoch 10/20\n","76/76 [==============================] - 37s 483ms/step - loss: 0.6715 - accuracy: 0.5272 - val_loss: 0.7082 - val_accuracy: 0.5237\n","Epoch 11/20\n","76/76 [==============================] - 33s 432ms/step - loss: 0.6720 - accuracy: 0.5347 - val_loss: 0.7191 - val_accuracy: 0.4937\n","Epoch 12/20\n","76/76 [==============================] - 37s 480ms/step - loss: 0.6717 - accuracy: 0.5320 - val_loss: 0.7282 - val_accuracy: 0.4952\n","Epoch 13/20\n","76/76 [==============================] - 35s 458ms/step - loss: 0.6709 - accuracy: 0.5171 - val_loss: 0.7260 - val_accuracy: 0.5232\n","Epoch 14/20\n","76/76 [==============================] - 35s 464ms/step - loss: 0.6694 - accuracy: 0.5382 - val_loss: 0.7321 - val_accuracy: 0.5242\n","Epoch 15/20\n","76/76 [==============================] - 37s 481ms/step - loss: 0.6694 - accuracy: 0.5320 - val_loss: 0.7360 - val_accuracy: 0.5232\n","Epoch 16/20\n","76/76 [==============================] - 33s 436ms/step - loss: 0.6687 - accuracy: 0.5328 - val_loss: 0.7387 - val_accuracy: 0.5232\n","Epoch 17/20\n","76/76 [==============================] - 36s 475ms/step - loss: 0.6689 - accuracy: 0.5343 - val_loss: 0.7412 - val_accuracy: 0.5232\n","Epoch 18/20\n","76/76 [==============================] - 36s 469ms/step - loss: 0.6693 - accuracy: 0.5328 - val_loss: 0.7432 - val_accuracy: 0.5232\n","Epoch 19/20\n","76/76 [==============================] - 35s 460ms/step - loss: 0.6682 - accuracy: 0.5374 - val_loss: 0.7430 - val_accuracy: 0.5237\n","Epoch 20/20\n","76/76 [==============================] - 37s 486ms/step - loss: 0.6671 - accuracy: 0.5330 - val_loss: 0.7466 - val_accuracy: 0.5232\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f968d627df0\u003e"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_hashtag, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2467,"status":"ok","timestamp":1678533911134,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"cx-pKoWQ7YKJ","outputId":"ea723838-2d16-4bbf-a90e-2aa391acc174"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 32ms/step\n","precision: [0.5235732  0.50909091]\n","recall: [0.97504621 0.02834008]\n","fscore: [0.68130449 0.05369128]\n","support: [1082  988]\n","accuracy: 0.5231884057971015\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.98      0.68      1082\n","           1       0.51      0.03      0.05       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.52      0.50      0.37      2070\n","weighted avg       0.52      0.52      0.38      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121600,"status":"ok","timestamp":1678536794138,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"pTR59rjZ7YQE","outputId":"8944b3d2-7bbb-43a0-9d0b-6586023d3c9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 13s 94ms/step - loss: 0.6938 - accuracy: 0.5111 - val_loss: 0.6942 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6926 - accuracy: 0.5167 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 8s 102ms/step - loss: 0.6931 - accuracy: 0.5163 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6928 - accuracy: 0.5204 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6927 - accuracy: 0.5260 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 6/20\n","76/76 [==============================] - 6s 80ms/step - loss: 0.6933 - accuracy: 0.5158 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 5s 66ms/step - loss: 0.6925 - accuracy: 0.5187 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 8/20\n","76/76 [==============================] - 5s 72ms/step - loss: 0.6931 - accuracy: 0.5212 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 9/20\n","76/76 [==============================] - 5s 72ms/step - loss: 0.6930 - accuracy: 0.5229 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 10/20\n","76/76 [==============================] - 5s 61ms/step - loss: 0.6930 - accuracy: 0.5212 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 11/20\n","76/76 [==============================] - 8s 103ms/step - loss: 0.6930 - accuracy: 0.5202 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 12/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6923 - accuracy: 0.5204 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 13/20\n","76/76 [==============================] - 5s 67ms/step - loss: 0.6925 - accuracy: 0.5237 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 14/20\n","76/76 [==============================] - 7s 89ms/step - loss: 0.6930 - accuracy: 0.5171 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 15/20\n","76/76 [==============================] - 5s 60ms/step - loss: 0.6922 - accuracy: 0.5221 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 16/20\n","76/76 [==============================] - 5s 71ms/step - loss: 0.6928 - accuracy: 0.5146 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 17/20\n","76/76 [==============================] - 5s 69ms/step - loss: 0.6928 - accuracy: 0.5144 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 18/20\n","76/76 [==============================] - 4s 59ms/step - loss: 0.6918 - accuracy: 0.5241 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 19/20\n","76/76 [==============================] - 8s 106ms/step - loss: 0.6930 - accuracy: 0.5160 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 6s 78ms/step - loss: 0.6928 - accuracy: 0.5229 - val_loss: 0.6922 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f59b4122e80\u003e"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3089,"status":"ok","timestamp":1678536797202,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Hht92k0YGNcc","outputId":"29eaacc1-d052-46a5-d30b-d0c65fce6fdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 22ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"yKAzK9vPH5li"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29887,"status":"ok","timestamp":1678536838243,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"dve-TSiha7Pn","outputId":"c439b22a-5d8e-4615-f9c3-3e1c74451b51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","66/66 [==============================] - 4s 29ms/step - loss: 0.9250 - accuracy: 0.4987 - val_loss: 0.8315 - val_accuracy: 0.5399\n","Epoch 2/20\n","66/66 [==============================] - 1s 22ms/step - loss: 0.8494 - accuracy: 0.4970 - val_loss: 0.7379 - val_accuracy: 0.5399\n","Epoch 3/20\n","66/66 [==============================] - 1s 22ms/step - loss: 0.7983 - accuracy: 0.5113 - val_loss: 0.7086 - val_accuracy: 0.4601\n","Epoch 4/20\n","66/66 [==============================] - 2s 24ms/step - loss: 0.7941 - accuracy: 0.4989 - val_loss: 0.7664 - val_accuracy: 0.4601\n","Epoch 5/20\n","66/66 [==============================] - 1s 18ms/step - loss: 0.7673 - accuracy: 0.5061 - val_loss: 0.7006 - val_accuracy: 0.4601\n","Epoch 6/20\n","66/66 [==============================] - 1s 18ms/step - loss: 0.7647 - accuracy: 0.4968 - val_loss: 0.6961 - val_accuracy: 0.4601\n","Epoch 7/20\n","66/66 [==============================] - 1s 17ms/step - loss: 0.7524 - accuracy: 0.4928 - val_loss: 0.6897 - val_accuracy: 0.5399\n","Epoch 8/20\n","66/66 [==============================] - 1s 17ms/step - loss: 0.7352 - accuracy: 0.5106 - val_loss: 0.7285 - val_accuracy: 0.4601\n","Epoch 9/20\n","66/66 [==============================] - 1s 17ms/step - loss: 0.7265 - accuracy: 0.5293 - val_loss: 0.7032 - val_accuracy: 0.4601\n","Epoch 10/20\n","66/66 [==============================] - 1s 17ms/step - loss: 0.7201 - accuracy: 0.5151 - val_loss: 0.7332 - val_accuracy: 0.4601\n","Epoch 11/20\n","66/66 [==============================] - 1s 17ms/step - loss: 0.7143 - accuracy: 0.5146 - val_loss: 0.7470 - val_accuracy: 0.4756\n","Epoch 12/20\n","66/66 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5398 - val_loss: 0.7275 - val_accuracy: 0.5066\n","Epoch 13/20\n","66/66 [==============================] - 1s 21ms/step - loss: 0.6939 - accuracy: 0.5198 - val_loss: 0.6942 - val_accuracy: 0.5105\n","Epoch 14/20\n","66/66 [==============================] - 1s 22ms/step - loss: 0.6841 - accuracy: 0.5246 - val_loss: 0.6757 - val_accuracy: 0.5122\n","Epoch 15/20\n","66/66 [==============================] - 2s 25ms/step - loss: 0.6789 - accuracy: 0.5234 - val_loss: 0.6708 - val_accuracy: 0.5476\n","Epoch 16/20\n","66/66 [==============================] - 2s 23ms/step - loss: 0.6766 - accuracy: 0.5288 - val_loss: 0.6700 - val_accuracy: 0.5476\n","Epoch 17/20\n","66/66 [==============================] - 1s 23ms/step - loss: 0.6744 - accuracy: 0.5331 - val_loss: 0.6701 - val_accuracy: 0.5460\n","Epoch 18/20\n","66/66 [==============================] - 2s 24ms/step - loss: 0.6733 - accuracy: 0.5343 - val_loss: 0.6714 - val_accuracy: 0.5476\n","Epoch 19/20\n","66/66 [==============================] - 1s 16ms/step - loss: 0.6689 - accuracy: 0.5419 - val_loss: 0.6724 - val_accuracy: 0.5465\n","Epoch 20/20\n","66/66 [==============================] - 1s 17ms/step - loss: 0.6699 - accuracy: 0.5312 - val_loss: 0.6698 - val_accuracy: 0.5487\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f59b81b7c70\u003e"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_hashtag, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1678536842432,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"_1tk8ehEa7Po","outputId":"a1a2baaf-dc9e-422b-c241-cac88d88607a"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 0s 3ms/step\n","precision: [0.80769231 0.54494382]\n","recall: [0.02527076 0.99487179]\n","fscore: [0.04900817 0.70417423]\n","support: [831 975]\n","accuracy: 0.5487264673311185\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.03      0.05       831\n","           1       0.54      0.99      0.70       975\n","\n","   micro avg       0.55      0.55      0.55      1806\n","   macro avg       0.68      0.51      0.38      1806\n","weighted avg       0.67      0.55      0.40      1806\n"," samples avg       0.55      0.55      0.55      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1359493,"status":"ok","timestamp":1678539827183,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"3cRvuj4ta7Po","outputId":"2fd66d66-0be7-4fc6-bbd0-86f4c0b6f118"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","66/66 [==============================] - 87s 1s/step - loss: 0.6750 - accuracy: 0.5324 - val_loss: 0.6902 - val_accuracy: 0.5493\n","Epoch 2/20\n","66/66 [==============================] - 65s 984ms/step - loss: 0.6589 - accuracy: 0.5393 - val_loss: 0.6862 - val_accuracy: 0.5415\n","Epoch 3/20\n","66/66 [==============================] - 68s 1s/step - loss: 0.6543 - accuracy: 0.5502 - val_loss: 0.6854 - val_accuracy: 0.5515\n","Epoch 4/20\n","66/66 [==============================] - 64s 969ms/step - loss: 0.6519 - accuracy: 0.5474 - val_loss: 0.6819 - val_accuracy: 0.5504\n","Epoch 5/20\n","66/66 [==============================] - 64s 966ms/step - loss: 0.6502 - accuracy: 0.5478 - val_loss: 0.6786 - val_accuracy: 0.5498\n","Epoch 6/20\n","66/66 [==============================] - 66s 997ms/step - loss: 0.6486 - accuracy: 0.5455 - val_loss: 0.6744 - val_accuracy: 0.5493\n","Epoch 7/20\n","66/66 [==============================] - 65s 981ms/step - loss: 0.6472 - accuracy: 0.5469 - val_loss: 0.6693 - val_accuracy: 0.5493\n","Epoch 8/20\n","66/66 [==============================] - 67s 1s/step - loss: 0.6470 - accuracy: 0.5528 - val_loss: 0.6678 - val_accuracy: 0.5476\n","Epoch 9/20\n","66/66 [==============================] - 63s 961ms/step - loss: 0.6450 - accuracy: 0.5542 - val_loss: 0.6668 - val_accuracy: 0.5482\n","Epoch 10/20\n","66/66 [==============================] - 64s 972ms/step - loss: 0.6434 - accuracy: 0.5549 - val_loss: 0.6672 - val_accuracy: 0.5493\n","Epoch 11/20\n","66/66 [==============================] - 68s 1s/step - loss: 0.6432 - accuracy: 0.5538 - val_loss: 0.6691 - val_accuracy: 0.5493\n","Epoch 12/20\n","66/66 [==============================] - 66s 1s/step - loss: 0.6423 - accuracy: 0.5602 - val_loss: 0.6714 - val_accuracy: 0.5504\n","Epoch 13/20\n","66/66 [==============================] - 66s 1s/step - loss: 0.6417 - accuracy: 0.5526 - val_loss: 0.6730 - val_accuracy: 0.5504\n","Epoch 14/20\n","66/66 [==============================] - 65s 981ms/step - loss: 0.6416 - accuracy: 0.5488 - val_loss: 0.6741 - val_accuracy: 0.5504\n","Epoch 15/20\n","66/66 [==============================] - 68s 1s/step - loss: 0.6407 - accuracy: 0.5580 - val_loss: 0.6850 - val_accuracy: 0.5476\n","Epoch 16/20\n","66/66 [==============================] - 66s 1s/step - loss: 0.6411 - accuracy: 0.5519 - val_loss: 0.6760 - val_accuracy: 0.5504\n","Epoch 17/20\n","66/66 [==============================] - 66s 1000ms/step - loss: 0.6401 - accuracy: 0.5545 - val_loss: 0.6774 - val_accuracy: 0.5504\n","Epoch 18/20\n","66/66 [==============================] - 66s 1s/step - loss: 0.6402 - accuracy: 0.5578 - val_loss: 0.6781 - val_accuracy: 0.5504\n","Epoch 19/20\n","66/66 [==============================] - 66s 993ms/step - loss: 0.6385 - accuracy: 0.5654 - val_loss: 0.6800 - val_accuracy: 0.5493\n","Epoch 20/20\n","66/66 [==============================] - 66s 1s/step - loss: 0.6395 - accuracy: 0.5587 - val_loss: 0.6814 - val_accuracy: 0.5493\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa464074e50\u003e"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_hashtag, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5035,"status":"ok","timestamp":1678539832194,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"2eAUUeS_a7Po","outputId":"c39e102f-81e4-47df-fc21-f3504de685b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 5s 70ms/step\n","precision: [0.70731707 0.54560907]\n","recall: [0.03489771 0.98769231]\n","fscore: [0.06651376 0.70291971]\n","support: [831 975]\n","accuracy: 0.5492801771871539\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.03      0.07       831\n","           1       0.55      0.99      0.70       975\n","\n","   micro avg       0.55      0.55      0.55      1806\n","   macro avg       0.63      0.51      0.38      1806\n","weighted avg       0.62      0.55      0.41      1806\n"," samples avg       0.55      0.55      0.55      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1239424,"status":"ok","timestamp":1678541082564,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"D4xzyYWZa7Po","outputId":"a6d410d6-1cf1-44d7-be3e-70d84a3e2c70"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","66/66 [==============================] - 67s 940ms/step - loss: 0.6762 - accuracy: 0.5355 - val_loss: 0.6891 - val_accuracy: 0.5515\n","Epoch 2/20\n","66/66 [==============================] - 64s 970ms/step - loss: 0.6617 - accuracy: 0.5490 - val_loss: 0.6849 - val_accuracy: 0.5493\n","Epoch 3/20\n","66/66 [==============================] - 61s 920ms/step - loss: 0.6571 - accuracy: 0.5488 - val_loss: 0.6796 - val_accuracy: 0.5493\n","Epoch 4/20\n","66/66 [==============================] - 62s 943ms/step - loss: 0.6544 - accuracy: 0.5557 - val_loss: 0.6763 - val_accuracy: 0.5487\n","Epoch 5/20\n","66/66 [==============================] - 61s 924ms/step - loss: 0.6527 - accuracy: 0.5521 - val_loss: 0.6718 - val_accuracy: 0.5487\n","Epoch 6/20\n","66/66 [==============================] - 62s 936ms/step - loss: 0.6503 - accuracy: 0.5559 - val_loss: 0.6676 - val_accuracy: 0.5271\n","Epoch 7/20\n","66/66 [==============================] - 61s 921ms/step - loss: 0.6483 - accuracy: 0.5512 - val_loss: 0.6602 - val_accuracy: 0.5498\n","Epoch 8/20\n","66/66 [==============================] - 60s 918ms/step - loss: 0.6473 - accuracy: 0.5585 - val_loss: 0.6575 - val_accuracy: 0.5504\n","Epoch 9/20\n","66/66 [==============================] - 63s 959ms/step - loss: 0.6467 - accuracy: 0.5547 - val_loss: 0.6580 - val_accuracy: 0.5498\n","Epoch 10/20\n","66/66 [==============================] - 61s 933ms/step - loss: 0.6457 - accuracy: 0.5587 - val_loss: 0.6583 - val_accuracy: 0.5271\n","Epoch 11/20\n","66/66 [==============================] - 61s 929ms/step - loss: 0.6439 - accuracy: 0.5595 - val_loss: 0.6635 - val_accuracy: 0.5493\n","Epoch 12/20\n","66/66 [==============================] - 61s 925ms/step - loss: 0.6430 - accuracy: 0.5526 - val_loss: 0.6702 - val_accuracy: 0.5493\n","Epoch 13/20\n","66/66 [==============================] - 61s 934ms/step - loss: 0.6421 - accuracy: 0.5668 - val_loss: 0.6714 - val_accuracy: 0.5260\n","Epoch 14/20\n","66/66 [==============================] - 64s 967ms/step - loss: 0.6429 - accuracy: 0.5611 - val_loss: 0.6734 - val_accuracy: 0.5255\n","Epoch 15/20\n","66/66 [==============================] - 65s 996ms/step - loss: 0.6419 - accuracy: 0.5495 - val_loss: 0.6720 - val_accuracy: 0.5493\n","Epoch 16/20\n","66/66 [==============================] - 61s 921ms/step - loss: 0.6419 - accuracy: 0.5557 - val_loss: 0.6775 - val_accuracy: 0.5482\n","Epoch 17/20\n","66/66 [==============================] - 60s 906ms/step - loss: 0.6405 - accuracy: 0.5599 - val_loss: 0.6822 - val_accuracy: 0.5487\n","Epoch 18/20\n","66/66 [==============================] - 61s 922ms/step - loss: 0.6400 - accuracy: 0.5552 - val_loss: 0.6804 - val_accuracy: 0.5493\n","Epoch 19/20\n","66/66 [==============================] - 61s 915ms/step - loss: 0.6387 - accuracy: 0.5592 - val_loss: 0.6911 - val_accuracy: 0.5482\n","Epoch 20/20\n","66/66 [==============================] - 61s 921ms/step - loss: 0.6387 - accuracy: 0.5701 - val_loss: 0.6867 - val_accuracy: 0.5266\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa460187b80\u003e"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_hashtag, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5629,"status":"ok","timestamp":1678541088164,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ed4J3k03a7Po","outputId":"dd334caf-0bac-4014-9fbb-5e8f0db35648"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 5s 77ms/step\n","precision: [0.49269184 0.86585366]\n","recall: [0.97352587 0.14564103]\n","fscore: [0.65426607 0.24934153]\n","support: [831 975]\n","accuracy: 0.526578073089701\n","              precision    recall  f1-score   support\n","\n","           0       0.49      0.97      0.65       831\n","           1       0.87      0.15      0.25       975\n","\n","   micro avg       0.53      0.53      0.53      1806\n","   macro avg       0.68      0.56      0.45      1806\n","weighted avg       0.69      0.53      0.44      1806\n"," samples avg       0.53      0.53      0.53      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32234,"status":"ok","timestamp":1678616912476,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"qIvyETpYa7Pp","outputId":"586b7ff2-e18a-4661-8d33-7b0f4646a4d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 12s 94ms/step - loss: 0.6918 - accuracy: 0.5267 - val_loss: 0.6916 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 6s 87ms/step - loss: 0.6908 - accuracy: 0.5374 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 4s 57ms/step - loss: 0.6906 - accuracy: 0.5343 - val_loss: 0.6937 - val_accuracy: 0.5044\n","Epoch 4/5\n","66/66 [==============================] - 4s 56ms/step - loss: 0.6896 - accuracy: 0.5367 - val_loss: 0.6852 - val_accuracy: 0.5050\n","Epoch 5/5\n","66/66 [==============================] - 5s 83ms/step - loss: 0.6756 - accuracy: 0.5338 - val_loss: 0.6682 - val_accuracy: 0.5432\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc2102084c0\u003e"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_hashtag, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2550,"status":"ok","timestamp":1678616915021,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"sziHDch4a7Pp","outputId":"236e2d33-1320-42d0-8cd3-fa3c9fa3dea5"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 22ms/step\n","precision: [0.71428571 0.54185268]\n","recall: [0.01203369 0.99589744]\n","fscore: [0.02366864 0.70184315]\n","support: [831 975]\n","accuracy: 0.5431893687707641\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.01      0.02       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.63      0.50      0.36      1806\n","weighted avg       0.62      0.54      0.39      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"cvWwTK7-IIzR"},"source":["### Retweets"]},{"cell_type":"markdown","metadata":{"id":"aM9jXFsZRT4Y"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327613,"status":"ok","timestamp":1678541431129,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"cjE8BrgBa8A-","outputId":"15bcad90-ef09-42a7-9259-4bcb5f3cb365"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 68s 855ms/step - loss: 0.9046 - accuracy: 0.4962 - val_loss: 0.7656 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 63s 833ms/step - loss: 0.8228 - accuracy: 0.5024 - val_loss: 0.7355 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 66s 868ms/step - loss: 0.7949 - accuracy: 0.5018 - val_loss: 0.7010 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 64s 843ms/step - loss: 0.7757 - accuracy: 0.5016 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 63s 838ms/step - loss: 0.7599 - accuracy: 0.4995 - val_loss: 0.6935 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa4617ef100\u003e"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_retweet, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4512,"status":"ok","timestamp":1678541445054,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"MthDhqrAa8A_","outputId":"d43dbf41-b653-4b8d-d92a-2359b40a1c89"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 60ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457191,"status":"ok","timestamp":1678541917166,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"0CX8XxO7a8A_","outputId":"a8834fbd-3f23-4e5c-c386-c66724800fd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 109s 1s/step - loss: 0.6969 - accuracy: 0.5119 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 77s 1s/step - loss: 0.6929 - accuracy: 0.5171 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 75s 986ms/step - loss: 0.6933 - accuracy: 0.5221 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 78s 1s/step - loss: 0.6930 - accuracy: 0.5198 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 80s 1s/step - loss: 0.6929 - accuracy: 0.5227 - val_loss: 0.6923 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa46144cd30\u003e"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11019,"status":"ok","timestamp":1678541928177,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"vOkCv_aaa8A_","outputId":"952ee695-7fca-442f-ec42-e678a5d01844"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 7s 99ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":372672,"status":"ok","timestamp":1678542322379,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"rLl2jn99a8A_","outputId":"920decee-60c2-4ea0-932c-61335c7aaf79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 80s 978ms/step - loss: 0.6984 - accuracy: 0.5119 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 71s 927ms/step - loss: 0.6928 - accuracy: 0.5154 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 73s 969ms/step - loss: 0.6930 - accuracy: 0.5218 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 73s 958ms/step - loss: 0.6929 - accuracy: 0.5163 - val_loss: 0.6933 - val_accuracy: 0.4773\n","Epoch 5/5\n","76/76 [==============================] - 73s 964ms/step - loss: 0.6930 - accuracy: 0.5192 - val_loss: 0.6926 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa45055d310\u003e"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_retweet, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6074,"status":"ok","timestamp":1678542328447,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"o1K_Zl3pa8BA","outputId":"9b6ffdd2-9673-4310-fc25-beb59458a37c"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 6s 77ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35971,"status":"ok","timestamp":1678616822775,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"L3GK4vyQa8BA","outputId":"48aa66e1-5e7e-4c83-e610-a1c44c6ea43d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='text_input'), name='text_input', description=\"created by layer 'text_input'\"), but it was called on an input with incompatible shape (None, 1).\n","WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='text_input'), name='text_input', description=\"created by layer 'text_input'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"name":"stdout","output_type":"stream","text":["76/76 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.5204"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='text_input'), name='text_input', description=\"created by layer 'text_input'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"name":"stdout","output_type":"stream","text":["\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\u0008\r76/76 [==============================] - 20s 45ms/step - loss: 0.6926 - accuracy: 0.5204 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 1s 16ms/step - loss: 0.6925 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 1s 16ms/step - loss: 0.6923 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 1s 15ms/step - loss: 0.6921 - accuracy: 0.5227 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 1s 19ms/step - loss: 0.6921 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc218a7ea60\u003e"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_retweet, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3001,"status":"ok","timestamp":1678616825771,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"c2g2ODB1a8BA","outputId":"15c8f880-cafc-46c9-ddfb-b5a0a1ea7996"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 300) for input KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name='text_input'), name='text_input', description=\"created by layer 'text_input'\"), but it was called on an input with incompatible shape (None, 1).\n"]},{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 5ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"YSCUfQ1MRT4a"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328256,"status":"ok","timestamp":1678542720686,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"D4CiTTTja8t5","outputId":"2c156f11-b861-4ab7-c6b0-183bd2ffd53e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 64s 931ms/step - loss: 0.9297 - accuracy: 0.5082 - val_loss: 0.6955 - val_accuracy: 0.4601\n","Epoch 2/5\n","66/66 [==============================] - 57s 860ms/step - loss: 0.8421 - accuracy: 0.5165 - val_loss: 0.7255 - val_accuracy: 0.4601\n","Epoch 3/5\n","66/66 [==============================] - 55s 835ms/step - loss: 0.8179 - accuracy: 0.5117 - val_loss: 0.7497 - val_accuracy: 0.4601\n","Epoch 4/5\n","66/66 [==============================] - 56s 851ms/step - loss: 0.8109 - accuracy: 0.5004 - val_loss: 0.7462 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 55s 843ms/step - loss: 0.7895 - accuracy: 0.5136 - val_loss: 0.7121 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa4500818e0\u003e"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_retweet, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2138,"status":"ok","timestamp":1678542722807,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"N_5cSnaza8t6","outputId":"8e6d413b-c2d7-4e0c-bac3-9bfcfa395853"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 55ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389654,"status":"ok","timestamp":1678543112458,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"25pdGVZSa8t6","outputId":"137eb8b0-fa6b-4baf-e9f4-dff112f9e0b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 74s 1s/step - loss: 0.6942 - accuracy: 0.5277 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 64s 967ms/step - loss: 0.6913 - accuracy: 0.5367 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 66s 1s/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6918 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 65s 990ms/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6906 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 66s 989ms/step - loss: 0.6906 - accuracy: 0.5364 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa44dcfe130\u003e"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5380,"status":"ok","timestamp":1678543117814,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"rGWhwX7Ta8t7","outputId":"65683bc2-3a65-47c8-e533-9748d48f0b3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 5s 73ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":390825,"status":"ok","timestamp":1678601070378,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"yNwkIDlma8t7","outputId":"31d282b3-72b3-4d9a-b8e7-d713b8781843"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 74s 1s/step - loss: 0.6987 - accuracy: 0.5243 - val_loss: 0.6933 - val_accuracy: 0.4601\n","Epoch 2/5\n","66/66 [==============================] - 69s 1s/step - loss: 0.6910 - accuracy: 0.5390 - val_loss: 0.6925 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 66s 1s/step - loss: 0.6913 - accuracy: 0.5374 - val_loss: 0.6919 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 86s 1s/step - loss: 0.6900 - accuracy: 0.5400 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 68s 1s/step - loss: 0.6903 - accuracy: 0.5398 - val_loss: 0.6912 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f65c7d9b2b0\u003e"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_retweet, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6805,"status":"ok","timestamp":1678601077159,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"1pafD4CDa8t7","outputId":"ea983819-595a-4128-c6b0-d218457b67ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 5s 75ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39922,"status":"ok","timestamp":1678616990129,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"hYwwhZWva8t7","outputId":"5580a6c8-8127-48aa-a64c-b7472032863f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 14s 139ms/step - loss: 0.6918 - accuracy: 0.5388 - val_loss: 0.6941 - val_accuracy: 0.4601\n","Epoch 2/5\n","66/66 [==============================] - 5s 81ms/step - loss: 0.6917 - accuracy: 0.5333 - val_loss: 0.6911 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 5s 77ms/step - loss: 0.6913 - accuracy: 0.5360 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 7s 106ms/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6905 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 7s 103ms/step - loss: 0.6900 - accuracy: 0.5386 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc19b0029d0\u003e"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_retweet, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3646,"status":"ok","timestamp":1678616993752,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"OI3Nj4tka8t8","outputId":"a2f629ef-0f04-453b-c33c-8dc3c08ecb15"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 41ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"WRDmMeU3II-c"},"source":["### BOW"]},{"cell_type":"markdown","metadata":{"id":"h3t0vnVNRXDl"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391445,"status":"ok","timestamp":1678544092202,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"TJRAqnRRa9Rz","outputId":"6911e977-606c-44a6-d705-0a5152ee7039"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 68s 870ms/step - loss: 0.8792 - accuracy: 0.4993 - val_loss: 0.6937 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 67s 887ms/step - loss: 0.8232 - accuracy: 0.5001 - val_loss: 0.6997 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 65s 862ms/step - loss: 0.7875 - accuracy: 0.5074 - val_loss: 0.7042 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 65s 864ms/step - loss: 0.7846 - accuracy: 0.4953 - val_loss: 0.6948 - val_accuracy: 0.4773\n","Epoch 5/5\n","76/76 [==============================] - 68s 895ms/step - loss: 0.7522 - accuracy: 0.5136 - val_loss: 0.6921 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa44b62d6a0\u003e"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_em_1gram, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5448,"status":"ok","timestamp":1678544097620,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"f2HX8Lf2a9R0","outputId":"ea950e35-eee8-42a3-ec0a-29e8ffa99ff6"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 61ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388723,"status":"ok","timestamp":1678544486316,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"_vvo5K6Ua9R0","outputId":"7c30ee7b-da06-4f4d-9630-d06662b671b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 84s 1s/step - loss: 0.6962 - accuracy: 0.5051 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 76s 1s/step - loss: 0.6873 - accuracy: 0.5272 - val_loss: 0.6928 - val_accuracy: 0.5159\n","Epoch 3/5\n","76/76 [==============================] - 74s 970ms/step - loss: 0.6844 - accuracy: 0.5216 - val_loss: 0.6915 - val_accuracy: 0.5217\n","Epoch 4/5\n","76/76 [==============================] - 76s 1s/step - loss: 0.6813 - accuracy: 0.5357 - val_loss: 0.6907 - val_accuracy: 0.5208\n","Epoch 5/5\n","76/76 [==============================] - 78s 1s/step - loss: 0.6792 - accuracy: 0.5428 - val_loss: 0.6904 - val_accuracy: 0.5261\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa44ab1b070\u003e"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5653,"status":"ok","timestamp":1678544491962,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"eoTHoHmSa9R0","outputId":"b03675aa-2900-4206-bc17-c8feb1953864"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 72ms/step\n","precision: [0.6278481  0.50208955]\n","recall: [0.22920518 0.85121457]\n","fscore: [0.33581584 0.63161848]\n","support: [1082  988]\n","accuracy: 0.5260869565217391\n","              precision    recall  f1-score   support\n","\n","           0       0.63      0.23      0.34      1082\n","           1       0.50      0.85      0.63       988\n","\n","   micro avg       0.53      0.53      0.53      2070\n","   macro avg       0.56      0.54      0.48      2070\n","weighted avg       0.57      0.53      0.48      2070\n"," samples avg       0.53      0.53      0.53      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392213,"status":"ok","timestamp":1678544884154,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ilCk14-Ba9R0","outputId":"988d7eb6-a743-456e-b1c5-0d254244d7f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 78s 928ms/step - loss: 0.6936 - accuracy: 0.5055 - val_loss: 0.6929 - val_accuracy: 0.5169\n","Epoch 2/5\n","76/76 [==============================] - 73s 958ms/step - loss: 0.6842 - accuracy: 0.5243 - val_loss: 0.6914 - val_accuracy: 0.5237\n","Epoch 3/5\n","76/76 [==============================] - 72s 946ms/step - loss: 0.6806 - accuracy: 0.5345 - val_loss: 0.6911 - val_accuracy: 0.5251\n","Epoch 4/5\n","76/76 [==============================] - 69s 916ms/step - loss: 0.6787 - accuracy: 0.5279 - val_loss: 0.6904 - val_accuracy: 0.5246\n","Epoch 5/5\n","76/76 [==============================] - 73s 955ms/step - loss: 0.6771 - accuracy: 0.5202 - val_loss: 0.6896 - val_accuracy: 0.5246\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa43cc1b730\u003e"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_em_1gram, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5144,"status":"ok","timestamp":1678544889273,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"4kXGZE-ma9R1","outputId":"34cb1ff7-8163-4584-e7ba-bb8a45576503"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 71ms/step\n","precision: [0.52474747 0.52222222]\n","recall: [0.96025878 0.04757085]\n","fscore: [0.67864141 0.08719852]\n","support: [1082  988]\n","accuracy: 0.5246376811594203\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.96      0.68      1082\n","           1       0.52      0.05      0.09       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.52      0.50      0.38      2070\n","weighted avg       0.52      0.52      0.40      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26075,"status":"ok","timestamp":1678617052626,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"OoH-xdeba9R1","outputId":"28c32371-ec2d-4e7f-b2db-ef61ae7786c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 8s 60ms/step - loss: 0.6937 - accuracy: 0.5148 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6928 - accuracy: 0.5148 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6931 - accuracy: 0.5131 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 4s 54ms/step - loss: 0.6929 - accuracy: 0.5169 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6930 - accuracy: 0.5221 - val_loss: 0.6922 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc18c702c40\u003e"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_em_1gram, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1720,"status":"ok","timestamp":1678617054323,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"NJWfMD88a9R1","outputId":"0c75ce22-37d6-4806-b3ea-4d6dd7edcf4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 14ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"FzebOjZBRXDm"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":344808,"status":"ok","timestamp":1678545234590,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"3fsdGVGua96F","outputId":"58479959-87d8-4c3b-ba3e-aa60e654a5f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 69s 1s/step - loss: 0.8928 - accuracy: 0.5115 - val_loss: 0.6922 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 73s 1s/step - loss: 0.8574 - accuracy: 0.4932 - val_loss: 0.7467 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 60s 904ms/step - loss: 0.8043 - accuracy: 0.5094 - val_loss: 0.7227 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 57s 873ms/step - loss: 0.7850 - accuracy: 0.4966 - val_loss: 0.7453 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 56s 850ms/step - loss: 0.7789 - accuracy: 0.5039 - val_loss: 0.7150 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa43c74c910\u003e"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_em_1gram, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8888,"status":"ok","timestamp":1678545243440,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"cwnT7Xhma96G","outputId":"a31554b7-9850-4874-b8f4-0e1a56117496"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 55ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228198,"status":"ok","timestamp":1678545471605,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"5tEf5eKUa96G","outputId":"7b213f64-3fb1-4873-98b6-af16d9f8b5ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 55s 746ms/step - loss: 0.6588 - accuracy: 0.6095 - val_loss: 0.6846 - val_accuracy: 0.5410\n","Epoch 2/5\n","66/66 [==============================] - 43s 652ms/step - loss: 0.6228 - accuracy: 0.6563 - val_loss: 0.6822 - val_accuracy: 0.5952\n","Epoch 3/5\n","66/66 [==============================] - 44s 668ms/step - loss: 0.6123 - accuracy: 0.6596 - val_loss: 0.6753 - val_accuracy: 0.6539\n","Epoch 4/5\n","66/66 [==============================] - 40s 614ms/step - loss: 0.6090 - accuracy: 0.6622 - val_loss: 0.6723 - val_accuracy: 0.6484\n","Epoch 5/5\n","66/66 [==============================] - 43s 654ms/step - loss: 0.6058 - accuracy: 0.6632 - val_loss: 0.6603 - val_accuracy: 0.6495\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa43a434310\u003e"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1678545471605,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"_K0uGheaa96G","outputId":"ed02289d-7145-448f-a630-06a5e25299f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 41ms/step\n","precision: [0.75647668 0.62042254]\n","recall: [0.35138387 0.90358974]\n","fscore: [0.47986853 0.73569937]\n","support: [831 975]\n","accuracy: 0.6495016611295681\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.35      0.48       831\n","           1       0.62      0.90      0.74       975\n","\n","   micro avg       0.65      0.65      0.65      1806\n","   macro avg       0.69      0.63      0.61      1806\n","weighted avg       0.68      0.65      0.62      1806\n"," samples avg       0.65      0.65      0.65      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11281,"status":"ok","timestamp":1678545661535,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"HGFvI8naa96G","outputId":"ceff3023-1b2a-4800-a0c9-a3fb0734a552"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 43s 608ms/step - loss: 0.6606 - accuracy: 0.6091 - val_loss: 0.6854 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 41s 616ms/step - loss: 0.6228 - accuracy: 0.6516 - val_loss: 0.6792 - val_accuracy: 0.5421\n","Epoch 3/5\n","66/66 [==============================] - 37s 566ms/step - loss: 0.6156 - accuracy: 0.6554 - val_loss: 0.6729 - val_accuracy: 0.6650\n","Epoch 4/5\n","66/66 [==============================] - 38s 577ms/step - loss: 0.6109 - accuracy: 0.6568 - val_loss: 0.6693 - val_accuracy: 0.6722\n","Epoch 5/5\n","66/66 [==============================] - 39s 600ms/step - loss: 0.6088 - accuracy: 0.6596 - val_loss: 0.6579 - val_accuracy: 0.6423\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa4244b93d0\u003e"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_em_1gram, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3528,"status":"ok","timestamp":1678545665062,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ZZn4Zvw4a96H","outputId":"c1599882-2e54-4b13-80e9-83a1de80c2ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 49ms/step\n","precision: [0.83882784 0.60730594]\n","recall: [0.2755716  0.95487179]\n","fscore: [0.41485507 0.74242424]\n","support: [831 975]\n","accuracy: 0.6423034330011074\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.28      0.41       831\n","           1       0.61      0.95      0.74       975\n","\n","   micro avg       0.64      0.64      0.64      1806\n","   macro avg       0.72      0.62      0.58      1806\n","weighted avg       0.71      0.64      0.59      1806\n"," samples avg       0.64      0.64      0.64      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25145,"status":"ok","timestamp":1678617115578,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"DH6Zra04a96H","outputId":"bfa92273-95eb-4745-d65b-d1a3dbbcd071"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 7s 62ms/step - loss: 0.6921 - accuracy: 0.5279 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 3s 44ms/step - loss: 0.6916 - accuracy: 0.5390 - val_loss: 0.6923 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 3s 44ms/step - loss: 0.6915 - accuracy: 0.5357 - val_loss: 0.6901 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 3s 50ms/step - loss: 0.6907 - accuracy: 0.5376 - val_loss: 0.6908 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 3s 47ms/step - loss: 0.6909 - accuracy: 0.5383 - val_loss: 0.6898 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc17d798490\u003e"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_em_1gram, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":940,"status":"ok","timestamp":1678617116488,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"hocKX2zna96H","outputId":"578bb367-5109-477b-ea0f-f0a165ccf48e"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 15ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"2CYGXZduIJGP"},"source":["### POS"]},{"cell_type":"markdown","metadata":{"id":"X5kH4a6zRXvh"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":336044,"status":"ok","timestamp":1678591650294,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ThU0cSX2a-rL","outputId":"422ccbec-ea6e-4e6c-bf19-036a6cf94aee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 74s 906ms/step - loss: 0.8739 - accuracy: 0.5156 - val_loss: 0.7401 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 72s 938ms/step - loss: 0.8413 - accuracy: 0.4844 - val_loss: 0.7403 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 61s 797ms/step - loss: 0.7941 - accuracy: 0.5011 - val_loss: 0.7218 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 57s 750ms/step - loss: 0.7854 - accuracy: 0.4991 - val_loss: 0.7287 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 58s 771ms/step - loss: 0.7624 - accuracy: 0.5049 - val_loss: 0.7493 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f08e044b7f0\u003e"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_POS, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4386,"status":"ok","timestamp":1678591654671,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"LHExxtxca-rM","outputId":"775e5add-fb69-4931-f0e3-39956a6bd650"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 60ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328493,"status":"ok","timestamp":1678591983160,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"WwlQoPtaa-rM","outputId":"7a5b5919-5468-45c8-d6da-f1256219a596"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 68s 825ms/step - loss: 0.6959 - accuracy: 0.5268 - val_loss: 0.6919 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 64s 844ms/step - loss: 0.6821 - accuracy: 0.5612 - val_loss: 0.6910 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 63s 830ms/step - loss: 0.6779 - accuracy: 0.5788 - val_loss: 0.6899 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 63s 831ms/step - loss: 0.6768 - accuracy: 0.5838 - val_loss: 0.6890 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 66s 876ms/step - loss: 0.6760 - accuracy: 0.5765 - val_loss: 0.6892 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f08d8a3dcd0\u003e"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5403,"status":"ok","timestamp":1678591988555,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"-zrRsEN0a-rM","outputId":"0d5a9e60-8468-4b94-f068-6f7770c5b397"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 65ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329618,"status":"ok","timestamp":1678592318167,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"jychPMiba-rM","outputId":"9556ba38-ef4b-42d0-bf11-86c1aa9eadd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 67s 803ms/step - loss: 0.6928 - accuracy: 0.5312 - val_loss: 0.6916 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 60s 794ms/step - loss: 0.6802 - accuracy: 0.5749 - val_loss: 0.6910 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 60s 794ms/step - loss: 0.6782 - accuracy: 0.5740 - val_loss: 0.6901 - val_accuracy: 0.5309\n","Epoch 4/5\n","76/76 [==============================] - 60s 789ms/step - loss: 0.6778 - accuracy: 0.5722 - val_loss: 0.6918 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 63s 824ms/step - loss: 0.6770 - accuracy: 0.5711 - val_loss: 0.6870 - val_accuracy: 0.5415\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f08e2362850\u003e"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_POS, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5353,"status":"ok","timestamp":1678592323513,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"CSDBPnLLa-rN","outputId":"97581ff3-aef8-41a2-ac8f-e940007012c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 65ms/step\n","precision: [0.54276527 0.53786408]\n","recall: [0.78003697 0.28036437]\n","fscore: [0.64012135 0.36859614]\n","support: [1082  988]\n","accuracy: 0.5415458937198068\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.78      0.64      1082\n","           1       0.54      0.28      0.37       988\n","\n","   micro avg       0.54      0.54      0.54      2070\n","   macro avg       0.54      0.53      0.50      2070\n","weighted avg       0.54      0.54      0.51      2070\n"," samples avg       0.54      0.54      0.54      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24458,"status":"ok","timestamp":1678617177432,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"dwpcigpHa-rN","outputId":"07a1a464-2f0a-43f0-f238-75a14aa5a1c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 8s 57ms/step - loss: 0.6936 - accuracy: 0.5059 - val_loss: 0.6924 - val_accuracy: 0.5329\n","Epoch 2/5\n","76/76 [==============================] - 3s 46ms/step - loss: 0.6921 - accuracy: 0.5194 - val_loss: 0.6904 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 4s 53ms/step - loss: 0.6889 - accuracy: 0.5347 - val_loss: 0.6879 - val_accuracy: 0.5676\n","Epoch 4/5\n","76/76 [==============================] - 4s 49ms/step - loss: 0.6853 - accuracy: 0.5492 - val_loss: 0.6847 - val_accuracy: 0.5338\n","Epoch 5/5\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6821 - accuracy: 0.5597 - val_loss: 0.6801 - val_accuracy: 0.5739\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc17d7983a0\u003e"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_POS, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2510,"status":"ok","timestamp":1678617179933,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"1ffeSBmJa-rN","outputId":"4a6fb2b4-1ccc-48d6-c9de-e26e5e37c7b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 16ms/step\n","precision: [0.58787346 0.55686695]\n","recall: [0.61829945 0.52530364]\n","fscore: [0.6027027 0.540625 ]\n","support: [1082  988]\n","accuracy: 0.5739130434782609\n","              precision    recall  f1-score   support\n","\n","           0       0.59      0.62      0.60      1082\n","           1       0.56      0.53      0.54       988\n","\n","   micro avg       0.57      0.57      0.57      2070\n","   macro avg       0.57      0.57      0.57      2070\n","weighted avg       0.57      0.57      0.57      2070\n"," samples avg       0.57      0.57      0.57      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"zKkwap-hRXvi"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678603864407,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"S8UtZ2VrlWd4","outputId":"51482a52-cd1c-4048-b2d7-ff83b602aee4"},"outputs":[{"data":{"text/plain":["array([[18,  1,  0,  0],\n","       [12,  2,  0,  0],\n","       [ 6,  2,  0,  0],\n","       ...,\n","       [ 5,  1,  0,  0],\n","       [21,  3,  0,  0],\n","       [ 8,  1,  0,  1]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["NepalEQ_POS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337420,"status":"ok","timestamp":1678605973982,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"YQVcEX0Da_TE","outputId":"2672d83e-089d-484b-9802-cbabf2eb37e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 77s 1s/step - loss: 0.8913 - accuracy: 0.5080 - val_loss: 0.7601 - val_accuracy: 0.4601\n","Epoch 2/5\n","66/66 [==============================] - 69s 1s/step - loss: 0.8546 - accuracy: 0.4918 - val_loss: 0.7330 - val_accuracy: 0.4601\n","Epoch 3/5\n","66/66 [==============================] - 73s 1s/step - loss: 0.7934 - accuracy: 0.5101 - val_loss: 0.7453 - val_accuracy: 0.4601\n","Epoch 4/5\n","66/66 [==============================] - 61s 918ms/step - loss: 0.7928 - accuracy: 0.4966 - val_loss: 0.7751 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 56s 845ms/step - loss: 0.7633 - accuracy: 0.5065 - val_loss: 0.7563 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6d103eb80\u003e"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_POS, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3513,"status":"ok","timestamp":1678605977467,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"zm6riFAKa_TF","outputId":"36a24224-6a67-4310-c4b3-20718fea8139"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 54ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322811,"status":"ok","timestamp":1678606300261,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"QevZ-t8ja_TF","outputId":"fc69e110-49bc-4deb-8cd9-beeab029cac8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 71s 982ms/step - loss: 0.6805 - accuracy: 0.5659 - val_loss: 0.6874 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 62s 939ms/step - loss: 0.6366 - accuracy: 0.6406 - val_loss: 0.6870 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 64s 977ms/step - loss: 0.6244 - accuracy: 0.6563 - val_loss: 0.6869 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 62s 936ms/step - loss: 0.6201 - accuracy: 0.6573 - val_loss: 0.6777 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 63s 943ms/step - loss: 0.6223 - accuracy: 0.6596 - val_loss: 0.6695 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6d18258b0\u003e"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5855,"status":"ok","timestamp":1678606306091,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ahYSxVnca_TF","outputId":"8b6ee77a-a102-421f-e4be-4bd66715980b"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 92ms/step\n","precision: [0.5        0.54004449]\n","recall: [0.00481348 0.99589744]\n","fscore: [0.00953516 0.70032456]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.00      0.01       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.52      0.50      0.35      1806\n","weighted avg       0.52      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327436,"status":"ok","timestamp":1678606633524,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"e2C0z4jMa_TF","outputId":"e0994b80-80db-4ee5-c130-20676208b12b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 65s 917ms/step - loss: 0.6597 - accuracy: 0.6041 - val_loss: 0.6879 - val_accuracy: 0.6168\n","Epoch 2/5\n","66/66 [==============================] - 58s 889ms/step - loss: 0.6172 - accuracy: 0.6672 - val_loss: 0.6830 - val_accuracy: 0.6545\n","Epoch 3/5\n","66/66 [==============================] - 58s 887ms/step - loss: 0.6133 - accuracy: 0.6594 - val_loss: 0.6781 - val_accuracy: 0.6357\n","Epoch 4/5\n","66/66 [==============================] - 57s 861ms/step - loss: 0.6112 - accuracy: 0.6691 - val_loss: 0.6741 - val_accuracy: 0.6667\n","Epoch 5/5\n","66/66 [==============================] - 59s 894ms/step - loss: 0.6103 - accuracy: 0.6625 - val_loss: 0.6670 - val_accuracy: 0.6700\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6d11b7af0\u003e"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_POS, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5759,"status":"ok","timestamp":1678606639261,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"S_tw4psva_TG","outputId":"04510247-2f6a-4164-cc71-ca62d4f61d8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 4s 67ms/step\n","precision: [0.68388106 0.66238218]\n","recall: [0.52587244 0.79282051]\n","fscore: [0.59455782 0.72175537]\n","support: [831 975]\n","accuracy: 0.6699889258028793\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.53      0.59       831\n","           1       0.66      0.79      0.72       975\n","\n","   micro avg       0.67      0.67      0.67      1806\n","   macro avg       0.67      0.66      0.66      1806\n","weighted avg       0.67      0.67      0.66      1806\n"," samples avg       0.67      0.67      0.67      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24854,"status":"ok","timestamp":1678617252617,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"32GKkhfGa_TG","outputId":"c03df107-70fe-4265-cfe6-4c92982c94b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 7s 57ms/step - loss: 0.6888 - accuracy: 0.5371 - val_loss: 0.6832 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 3s 44ms/step - loss: 0.6665 - accuracy: 0.5922 - val_loss: 0.6498 - val_accuracy: 0.5986\n","Epoch 3/5\n","66/66 [==============================] - 3s 48ms/step - loss: 0.6353 - accuracy: 0.6432 - val_loss: 0.6308 - val_accuracy: 0.6473\n","Epoch 4/5\n","66/66 [==============================] - 3s 52ms/step - loss: 0.6261 - accuracy: 0.6565 - val_loss: 0.6257 - val_accuracy: 0.6445\n","Epoch 5/5\n","66/66 [==============================] - 3s 46ms/step - loss: 0.6229 - accuracy: 0.6592 - val_loss: 0.6245 - val_accuracy: 0.6484\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc14c5fe100\u003e"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_POS, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1666,"status":"ok","timestamp":1678617254263,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"EH8DFecza_TG","outputId":"61067d74-f60f-445f-c454-1654903c3f52"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 17ms/step\n","precision: [0.64202899 0.65232975]\n","recall: [0.53309266 0.74666667]\n","fscore: [0.58251151 0.69631755]\n","support: [831 975]\n","accuracy: 0.6483942414174972\n","              precision    recall  f1-score   support\n","\n","           0       0.64      0.53      0.58       831\n","           1       0.65      0.75      0.70       975\n","\n","   micro avg       0.65      0.65      0.65      1806\n","   macro avg       0.65      0.64      0.64      1806\n","weighted avg       0.65      0.65      0.64      1806\n"," samples avg       0.65      0.65      0.65      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"7BNky2MiIKUX"},"source":["### Fasttext"]},{"cell_type":"markdown","metadata":{"id":"Z2GKVUVZRYhu"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325999,"status":"ok","timestamp":1678592854786,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"z4rufrL2a_6-","outputId":"3876901b-f39b-46c0-d5cc-c3e289d69eff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 60s 755ms/step - loss: 0.9172 - accuracy: 0.5011 - val_loss: 0.7218 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 55s 722ms/step - loss: 0.8305 - accuracy: 0.5140 - val_loss: 0.6975 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 55s 721ms/step - loss: 0.8005 - accuracy: 0.5018 - val_loss: 0.7005 - val_accuracy: 0.4773\n","Epoch 4/5\n","76/76 [==============================] - 59s 775ms/step - loss: 0.7923 - accuracy: 0.5009 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 58s 761ms/step - loss: 0.7629 - accuracy: 0.5131 - val_loss: 0.6958 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f08c9993340\u003e"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_fasttext, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3533,"status":"ok","timestamp":1678592858308,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"VYl9FrEKa_6_","outputId":"04124513-365b-404e-9449-908c49514402"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 52ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389188,"status":"ok","timestamp":1678593247489,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"xCwJIu6la_6_","outputId":"83ebe255-fdd1-4017-e062-b82abc93363c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 71s 857ms/step - loss: 0.6973 - accuracy: 0.5018 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 64s 838ms/step - loss: 0.6925 - accuracy: 0.5227 - val_loss: 0.6934 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 68s 895ms/step - loss: 0.6923 - accuracy: 0.5252 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 65s 857ms/step - loss: 0.6926 - accuracy: 0.5189 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 66s 860ms/step - loss: 0.6927 - accuracy: 0.5189 - val_loss: 0.6922 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f08c99d2910\u003e"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4575,"status":"ok","timestamp":1678593252054,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"VJyRiITYa_6_","outputId":"8f753cab-be0a-46f2-ae25-6ad01c1f9eec"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 65ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327552,"status":"ok","timestamp":1678593579603,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"qQtJ1Eq1a_6_","outputId":"2fa8bf27-9d71-4545-9a6b-ce660ef610e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 66s 802ms/step - loss: 0.6958 - accuracy: 0.5107 - val_loss: 0.6952 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 63s 826ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6935 - val_accuracy: 0.4773\n","Epoch 3/5\n","76/76 [==============================] - 61s 800ms/step - loss: 0.6926 - accuracy: 0.5198 - val_loss: 0.6935 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 61s 805ms/step - loss: 0.6927 - accuracy: 0.5198 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 63s 822ms/step - loss: 0.6926 - accuracy: 0.5200 - val_loss: 0.6924 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f08c907d4f0\u003e"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_fasttext, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5673,"status":"ok","timestamp":1678593585270,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"BX41huFRa_6_","outputId":"6fb61787-9b86-4c10-b1a2-631738bbdec3"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 64ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24141,"status":"ok","timestamp":1678617306087,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"QvViDkcKa_7A","outputId":"f65f1606-4ecb-439e-8b18-8ba38462ed79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 7s 63ms/step - loss: 0.6938 - accuracy: 0.5142 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6931 - accuracy: 0.5131 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6926 - accuracy: 0.5125 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 5s 65ms/step - loss: 0.6931 - accuracy: 0.5231 - val_loss: 0.6930 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc1485aa310\u003e"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_fasttext, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1661,"status":"ok","timestamp":1678617307733,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"5PdypQdfa_7A","outputId":"05a4296c-ce42-4786-be09-a427d6b24df6"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 14ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"sDzjLU1ZRYhv"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1678593853788,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"NMTT02QMbAg5","outputId":"0b73b24d-d1de-4fdd-fc6a-697412ffe6a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 52s 757ms/step - loss: 0.8631 - accuracy: 0.4977 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 50s 750ms/step - loss: 0.8090 - accuracy: 0.5023 - val_loss: 0.6903 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 48s 731ms/step - loss: 0.7828 - accuracy: 0.4966 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 57s 865ms/step - loss: 0.7719 - accuracy: 0.5037 - val_loss: 0.7048 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 51s 773ms/step - loss: 0.7592 - accuracy: 0.5075 - val_loss: 0.7110 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f08c831fe80\u003e"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_fasttext, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":0,"status":"ok","timestamp":1678593853789,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"k3s4eF0mbAg6","outputId":"12268a6f-ca62-4e45-a996-20eda0e988fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 31ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271379,"status":"ok","timestamp":1678596101710,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"FHzkD3vFbAg6","outputId":"a416b119-f485-4918-b5b0-6304a76e2341"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 61s 816ms/step - loss: 0.6926 - accuracy: 0.5224 - val_loss: 0.6903 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 52s 791ms/step - loss: 0.6916 - accuracy: 0.5379 - val_loss: 0.6907 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 53s 798ms/step - loss: 0.6907 - accuracy: 0.5395 - val_loss: 0.6911 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 53s 796ms/step - loss: 0.6909 - accuracy: 0.5395 - val_loss: 0.6901 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 50s 760ms/step - loss: 0.6909 - accuracy: 0.5395 - val_loss: 0.6904 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f6607987a30\u003e"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_fasttext, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10775,"status":"ok","timestamp":1678596112461,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"KJbk_Hr9bAg6","outputId":"48fde50e-83b1-49a6-aada-9c32353463f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 95ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330963,"status":"ok","timestamp":1678596462207,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"yzcAJ2JHbAg6","outputId":"e7e58f2c-bc34-4123-bda0-408f4de0b9b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 71s 960ms/step - loss: 0.6999 - accuracy: 0.5315 - val_loss: 0.6905 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 66s 1s/step - loss: 0.6910 - accuracy: 0.5364 - val_loss: 0.6906 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 64s 972ms/step - loss: 0.6914 - accuracy: 0.5395 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 63s 953ms/step - loss: 0.6906 - accuracy: 0.5386 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 63s 956ms/step - loss: 0.6907 - accuracy: 0.5395 - val_loss: 0.6907 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f660ca2c1c0\u003e"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_fasttext, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7098,"status":"ok","timestamp":1678596469279,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"q9cWFjrkbAg7","outputId":"8ebf8713-3eeb-41ae-d253-96be8e869b0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 80ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24615,"status":"ok","timestamp":1678617367981,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"vC9VcTRMbAg7","outputId":"24e3137d-d201-47b1-c9ca-47be7b1b2b32"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 7s 65ms/step - loss: 0.6907 - accuracy: 0.5324 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 3s 52ms/step - loss: 0.6908 - accuracy: 0.5400 - val_loss: 0.6904 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 3s 46ms/step - loss: 0.6907 - accuracy: 0.5402 - val_loss: 0.6916 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 3s 43ms/step - loss: 0.6910 - accuracy: 0.5390 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 3s 45ms/step - loss: 0.6908 - accuracy: 0.5405 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc140518190\u003e"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_fasttext, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1736,"status":"ok","timestamp":1678617369680,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ACTQ5KLkbAg7","outputId":"d51998ba-b6a5-4938-9dc5-a9da075be233"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 16ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"p2Q8k7atIKbr"},"source":["### Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"5RQ_RVKRRZOw"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414769,"status":"ok","timestamp":1678597003067,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"TFTKzAhfbBJn","outputId":"1fcb0fdb-e57c-483f-dcad-63630e9ba74f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 85s 1s/step - loss: 0.8584 - accuracy: 0.5088 - val_loss: 0.7038 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 83s 1s/step - loss: 0.8061 - accuracy: 0.5117 - val_loss: 0.6930 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 86s 1s/step - loss: 0.7783 - accuracy: 0.5152 - val_loss: 0.6990 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 87s 1s/step - loss: 0.7776 - accuracy: 0.5032 - val_loss: 0.7083 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 71s 937ms/step - loss: 0.7578 - accuracy: 0.5131 - val_loss: 0.6990 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f660c3db160\u003e"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_word2vec, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5222,"status":"ok","timestamp":1678597008263,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"8gaRwwGlbBJn","outputId":"92a29186-5946-4a90-cde9-5a9f1d06ec75"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 59ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379136,"status":"ok","timestamp":1678597387394,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"bzdb-WkTbBJn","outputId":"1c4a3c83-2b96-4a20-b4a7-c89fc821fc32"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 82s 992ms/step - loss: 0.6987 - accuracy: 0.5086 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 74s 975ms/step - loss: 0.6929 - accuracy: 0.5227 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 74s 972ms/step - loss: 0.6929 - accuracy: 0.5210 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 74s 977ms/step - loss: 0.6925 - accuracy: 0.5206 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 74s 970ms/step - loss: 0.6929 - accuracy: 0.5165 - val_loss: 0.6926 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f660c48da30\u003e"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5423,"status":"ok","timestamp":1678597392790,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"_4J4YWhEbBJo","outputId":"9bb0033f-56b4-44e2-9554-32a7402818ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 73ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388962,"status":"ok","timestamp":1678597781729,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Hcudo7mMbBJo","outputId":"d09893ca-9d96-4669-f9d5-12e31503c32a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 77s 931ms/step - loss: 0.6997 - accuracy: 0.5063 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 71s 935ms/step - loss: 0.6926 - accuracy: 0.5227 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 68s 903ms/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 69s 916ms/step - loss: 0.6926 - accuracy: 0.5169 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 69s 902ms/step - loss: 0.6926 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f65fd2df940\u003e"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_word2vec, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5888,"status":"ok","timestamp":1678597787591,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"rZWEJN34bBJo","outputId":"8d6da0d4-fab3-468e-e79c-98c71decd3fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 6s 74ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21619,"status":"ok","timestamp":1678617422129,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"xzwt6EKnbBJo","outputId":"8794a438-25b2-403a-b96e-ea712e7c5372"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 7s 55ms/step - loss: 0.6938 - accuracy: 0.5107 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 4s 48ms/step - loss: 0.6934 - accuracy: 0.5125 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6931 - accuracy: 0.5212 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6928 - accuracy: 0.5113 - val_loss: 0.6929 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 3s 43ms/step - loss: 0.6924 - accuracy: 0.5274 - val_loss: 0.6932 - val_accuracy: 0.4773\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc1385da160\u003e"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_word2vec, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1714,"status":"ok","timestamp":1678617423827,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"-N8TBMytbBJo","outputId":"63092ce3-0be4-44b2-bd1c-bba4148ee54f"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 14ms/step\n","precision: [0.         0.47729469]\n","recall: [0. 1.]\n","fscore: [0.         0.64617397]\n","support: [1082  988]\n","accuracy: 0.47729468599033814\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1082\n","           1       0.48      1.00      0.65       988\n","\n","   micro avg       0.48      0.48      0.48      2070\n","   macro avg       0.24      0.50      0.32      2070\n","weighted avg       0.23      0.48      0.31      2070\n"," samples avg       0.48      0.48      0.48      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"SB97cTuPRZOx"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678597867083,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"BEIeAiOxOqpC","outputId":"0acebaf9-b7ff-439b-e629-de8f6231546f"},"outputs":[{"data":{"text/plain":["()"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["QueenslandFLD_df_embeddings_word2vec.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306222,"status":"ok","timestamp":1678607004718,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"vYlRfTIjbBug","outputId":"7ffce909-625b-4a30-923e-cb7031fadf68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 76s 1s/step - loss: 0.9159 - accuracy: 0.4985 - val_loss: 0.6911 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 59s 903ms/step - loss: 0.8399 - accuracy: 0.5089 - val_loss: 0.6989 - val_accuracy: 0.4601\n","Epoch 3/5\n","66/66 [==============================] - 59s 902ms/step - loss: 0.8124 - accuracy: 0.5120 - val_loss: 0.7306 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 56s 847ms/step - loss: 0.7985 - accuracy: 0.5065 - val_loss: 0.6959 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 55s 823ms/step - loss: 0.7784 - accuracy: 0.5053 - val_loss: 0.6910 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6ca8f8340\u003e"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_word2vec, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5149,"status":"ok","timestamp":1678607009851,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"IGRqL6AebBuh","outputId":"9a2eaa53-a339-4ae7-ebe1-15f27be63ced"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 5s 81ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330287,"status":"ok","timestamp":1678607340122,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"mkxAf8JlbBuh","outputId":"6453eac6-208e-4168-c217-a63e4dd3778c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 70s 959ms/step - loss: 0.6970 - accuracy: 0.5186 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 64s 969ms/step - loss: 0.6910 - accuracy: 0.5395 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 61s 932ms/step - loss: 0.6907 - accuracy: 0.5390 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 65s 979ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 65s 988ms/step - loss: 0.6906 - accuracy: 0.5395 - val_loss: 0.6912 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6467d4790\u003e"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4224,"status":"ok","timestamp":1678607344337,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"VMs-emMJbBuh","outputId":"4e69efec-b2c3-477e-a1ca-f66514aa816a"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 4s 69ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330011,"status":"ok","timestamp":1678607674340,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"7il4yys-bBuh","outputId":"f28605c3-e22b-4a46-d571-cfc1f2101248"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 66s 890ms/step - loss: 0.6993 - accuracy: 0.5179 - val_loss: 0.6904 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 58s 886ms/step - loss: 0.6913 - accuracy: 0.5381 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 58s 875ms/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6904 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 64s 968ms/step - loss: 0.6905 - accuracy: 0.5386 - val_loss: 0.6906 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 59s 888ms/step - loss: 0.6904 - accuracy: 0.5395 - val_loss: 0.6905 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6d14c0730\u003e"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_word2vec, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5696,"status":"ok","timestamp":1678607680017,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"zIgJ5UvqbBuh","outputId":"a6952448-99ca-4d45-e7dd-a82c94d0c1f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 4s 66ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24267,"status":"ok","timestamp":1678617488077,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"iGQd2FiAbBui","outputId":"0caf2ba9-6acf-4534-8347-322a50586eb6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 6s 57ms/step - loss: 0.6904 - accuracy: 0.5393 - val_loss: 0.6912 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 3s 52ms/step - loss: 0.6907 - accuracy: 0.5360 - val_loss: 0.6932 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 3s 43ms/step - loss: 0.6908 - accuracy: 0.5400 - val_loss: 0.6915 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 3s 47ms/step - loss: 0.6913 - accuracy: 0.5388 - val_loss: 0.6903 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 3s 53ms/step - loss: 0.6911 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fc130590190\u003e"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_word2vec, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":965,"status":"ok","timestamp":1678617489012,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"0ed8lxqGbBui","outputId":"2f0edad0-929b-4d67-bc1e-0e193eb1130d"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 16ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"pIc8fgxUIKiL"},"source":["### Glove"]},{"cell_type":"markdown","metadata":{"id":"UdKb9DA-RaKK"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31748,"status":"ok","timestamp":1679911900455,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"FzgNErcobCRv","outputId":"a74adb60-627d-4331-8d07-71aeb594aa79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","76/76 [==============================] - 13s 35ms/step - loss: 0.9039 - accuracy: 0.4939 - val_loss: 0.6951 - val_accuracy: 0.5227\n","Epoch 2/10\n","76/76 [==============================] - 2s 21ms/step - loss: 0.8216 - accuracy: 0.4943 - val_loss: 0.7021 - val_accuracy: 0.5227\n","Epoch 3/10\n","76/76 [==============================] - 1s 16ms/step - loss: 0.7973 - accuracy: 0.4891 - val_loss: 0.6979 - val_accuracy: 0.4773\n","Epoch 4/10\n","76/76 [==============================] - 1s 16ms/step - loss: 0.7666 - accuracy: 0.5026 - val_loss: 0.6937 - val_accuracy: 0.4773\n","Epoch 5/10\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7619 - accuracy: 0.4937 - val_loss: 0.6975 - val_accuracy: 0.4773\n","Epoch 6/10\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7413 - accuracy: 0.5156 - val_loss: 0.7163 - val_accuracy: 0.4773\n","Epoch 7/10\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7371 - accuracy: 0.4922 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 8/10\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7315 - accuracy: 0.4939 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 9/10\n","76/76 [==============================] - 1s 18ms/step - loss: 0.7189 - accuracy: 0.5125 - val_loss: 0.7235 - val_accuracy: 0.5227\n","Epoch 10/10\n","76/76 [==============================] - 1s 20ms/step - loss: 0.7131 - accuracy: 0.5016 - val_loss: 0.7312 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f11f708fb20\u003e"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_glove, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=10, batch_size=batch_size, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1752,"status":"ok","timestamp":1679911902202,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"uUW0IRQhbCRw","outputId":"6c84ba97-62dc-410b-9f18-48481b775283"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 6ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","auc: 0.5\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTMrmaaZbCRw"},"outputs":[],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n","from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n","from tensorflow.keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_glove, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=2, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1679638267639,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"desLp6AWPUq9","outputId":"a6c53260-5ec8-42bf-e88f-ea628be4fed1"},"outputs":[{"data":{"text/plain":["(4829, 300)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["X_text_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10626,"status":"ok","timestamp":1679627583613,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"EqZcnsYKbCRw","outputId":"525f479b-96ba-446a-c9cb-112061c9f13f"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 6s 92ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62536,"status":"ok","timestamp":1679913308484,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"mUuPDrvybCRw","outputId":"c510bb40-5d7e-4bd8-e9ca-5682d2f34b11"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Layer text_GRU will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","76/76 [==============================] - 166s 2s/step - loss: 0.6970 - accuracy: 0.5173 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 2/10\n","76/76 [==============================] - 166s 2s/step - loss: 0.6908 - accuracy: 0.5347 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 3/10\n","76/76 [==============================] - 162s 2s/step - loss: 0.6913 - accuracy: 0.5326 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 4/10\n","76/76 [==============================] - 162s 2s/step - loss: 0.6901 - accuracy: 0.5407 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 5/10\n","76/76 [==============================] - 165s 2s/step - loss: 0.6905 - accuracy: 0.5440 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 6/10\n","76/76 [==============================] - 164s 2s/step - loss: 0.6902 - accuracy: 0.5365 - val_loss: 0.6917 - val_accuracy: 0.5227\n","Epoch 7/10\n","76/76 [==============================] - 165s 2s/step - loss: 0.6901 - accuracy: 0.5413 - val_loss: 0.6918 - val_accuracy: 0.5227\n","Epoch 8/10\n","76/76 [==============================] - 163s 2s/step - loss: 0.6906 - accuracy: 0.5403 - val_loss: 0.6919 - val_accuracy: 0.5261\n","Epoch 9/10\n","76/76 [==============================] - 95s 1s/step - loss: 0.6902 - accuracy: 0.5384 - val_loss: 0.6943 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f11f605d8e0\u003e"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_embeddings_glove, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=10, batch_size=batch_size, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6962,"status":"ok","timestamp":1679913315445,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Ikr2fYWkbCRw","outputId":"dc80bfee-ed3e-4e14-adeb-b1b74dc5426d"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 69ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","auc: 0.5\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["\n","predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FiwjdEAWbCRx"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-4EhXlgbCRx"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"XnCpFYtzRaKL"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12192,"status":"ok","timestamp":1679913572116,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"MhDCg3i4bC4g","outputId":"adb5a868-c25e-46d5-b007-29a37e2f825e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","66/66 [==============================] - 6s 34ms/step - loss: 0.9013 - accuracy: 0.4970 - val_loss: 0.7132 - val_accuracy: 0.4601\n","Epoch 2/10\n","66/66 [==============================] - 1s 19ms/step - loss: 0.8522 - accuracy: 0.4977 - val_loss: 0.7166 - val_accuracy: 0.4601\n","Epoch 3/10\n","66/66 [==============================] - 1s 21ms/step - loss: 0.8059 - accuracy: 0.5058 - val_loss: 0.7704 - val_accuracy: 0.4601\n","Epoch 4/10\n","66/66 [==============================] - 1s 20ms/step - loss: 0.7784 - accuracy: 0.5115 - val_loss: 0.8392 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f14304e4d90\u003e"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_glove, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=10, batch_size=batch_size, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679913572117,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ko61eZ67bC4g","outputId":"be6d141f-c7e5-478e-c03f-40115b106400"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 5ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","auc: 0.5\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nN_TF48ebC4h"},"outputs":[],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_glove, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# # pad the text sequences\n","# X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","# X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(128, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 256\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=2, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK1WIEHybC4h"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1300648,"status":"ok","timestamp":1679915968423,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"X-D5kN0FbC4h","outputId":"e502c8d5-df35-453a-8f7d-a72caff721b3"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Layer text_GRU will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","66/66 [==============================] - 147s 2s/step - loss: 0.6967 - accuracy: 0.5300 - val_loss: 0.6903 - val_accuracy: 0.5399\n","Epoch 2/10\n","66/66 [==============================] - 142s 2s/step - loss: 0.6909 - accuracy: 0.5393 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 3/10\n","66/66 [==============================] - 145s 2s/step - loss: 0.6907 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 4/10\n","66/66 [==============================] - 165s 3s/step - loss: 0.6912 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 5/10\n","66/66 [==============================] - 183s 3s/step - loss: 0.6907 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 6/10\n","66/66 [==============================] - 174s 3s/step - loss: 0.6909 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 7/10\n","66/66 [==============================] - 181s 3s/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 8/10\n","66/66 [==============================] - 163s 2s/step - loss: 0.6906 - accuracy: 0.5395 - val_loss: 0.6915 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f13a861c220\u003e"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_embeddings_glove, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=10, batch_size=batch_size, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11204,"status":"ok","timestamp":1679915979623,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"AV1Bxc-sbC4h","outputId":"0d15a187-e362-407d-9e4b-c81587b1659e"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 7s 114ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","auc: 0.5\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cZM6AhlbC4h"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuHzx1UmbC4i"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"wnXgVAKAIKoc"},"source":["### Bert"]},{"cell_type":"markdown","metadata":{"id":"Ha9Vf87mRazK"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11688,"status":"ok","timestamp":1679915997757,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"egckIETLbDiJ","outputId":"e60af982-4c3e-406a-b257-e1b727f13fe5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","76/76 [==============================] - 4s 26ms/step - loss: 0.9064 - accuracy: 0.4962 - val_loss: 0.9592 - val_accuracy: 0.5227\n","Epoch 2/10\n","76/76 [==============================] - 1s 17ms/step - loss: 0.7786 - accuracy: 0.5100 - val_loss: 0.9317 - val_accuracy: 0.5227\n","Epoch 3/10\n","76/76 [==============================] - 1s 19ms/step - loss: 0.7302 - accuracy: 0.5252 - val_loss: 0.8075 - val_accuracy: 0.5227\n","Epoch 4/10\n","76/76 [==============================] - 1s 18ms/step - loss: 0.6982 - accuracy: 0.5411 - val_loss: 0.8575 - val_accuracy: 0.5227\n","Epoch 5/10\n","76/76 [==============================] - 2s 23ms/step - loss: 0.6716 - accuracy: 0.5925 - val_loss: 1.1048 - val_accuracy: 0.5227\n","Epoch 6/10\n","76/76 [==============================] - 1s 18ms/step - loss: 0.5835 - accuracy: 0.6995 - val_loss: 1.1147 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f13b369e400\u003e"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_bert_input, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=10, batch_size=batch_size, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":782,"status":"ok","timestamp":1679915998534,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"0UZIz_p3bDiK","outputId":"0d1f8bd3-685a-4757-f9ea-1dea3568c2cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 5ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","auc: 0.5\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210728,"status":"ok","timestamp":1679640774634,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"_MlKU-Y-bDiK","outputId":"41c9fa6f-2a2c-4706-f547-f13242e01b63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","76/76 [==============================] - 79s 953ms/step - loss: 0.5995 - accuracy: 0.6689 - val_loss: 0.6755 - val_accuracy: 0.7227\n","Epoch 2/2\n","76/76 [==============================] - 71s 935ms/step - loss: 0.4507 - accuracy: 0.7925 - val_loss: 0.6581 - val_accuracy: 0.6831\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f3986780040\u003e"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_bert_input, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=2, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5652,"status":"ok","timestamp":1679640780273,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"iyfXLHI_bDiK","outputId":"08a23dc0-968f-4673-d1ae-b698f48b7f67"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 65ms/step\n","precision: [0.62588652 0.93915344]\n","recall: [0.97874307 0.35931174]\n","fscore: [0.76351839 0.51976574]\n","support: [1082  988]\n","accuracy: 0.6830917874396135\n","auc: 0.6690274046412776\n","              precision    recall  f1-score   support\n","\n","           0       0.63      0.98      0.76      1082\n","           1       0.94      0.36      0.52       988\n","\n","   micro avg       0.68      0.68      0.68      2070\n","   macro avg       0.78      0.67      0.64      2070\n","weighted avg       0.78      0.68      0.65      2070\n"," samples avg       0.68      0.68      0.68      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78130,"status":"ok","timestamp":1679916805024,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"cz7emhBvbDiK","outputId":"025e0180-614e-4759-9ac7-e3182d1a7073"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Layer text_GRU will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 166s 2s/step - loss: 0.6219 - accuracy: 0.6420 - val_loss: 0.6763 - val_accuracy: 0.6502\n","Epoch 2/5\n","76/76 [==============================] - 161s 2s/step - loss: 0.4529 - accuracy: 0.7956 - val_loss: 0.6629 - val_accuracy: 0.5338\n","Epoch 3/5\n","76/76 [==============================] - 161s 2s/step - loss: 0.3560 - accuracy: 0.8457 - val_loss: 0.6478 - val_accuracy: 0.5633\n","Epoch 4/5\n","76/76 [==============================] - 157s 2s/step - loss: 0.2763 - accuracy: 0.8873 - val_loss: 0.5997 - val_accuracy: 0.7092\n","Epoch 5/5\n","76/76 [==============================] - 102s 1s/step - loss: 0.2177 - accuracy: 0.9128 - val_loss: 0.5739 - val_accuracy: 0.7043\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f1394da7070\u003e"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_bert_input, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5890,"status":"ok","timestamp":1679916810907,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"HC-s-UrnbDiL","outputId":"e75b84c6-e11b-467f-a1cd-bec362ea21c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 69ms/step\n","precision: [0.64798489 0.89004149]\n","recall: [0.95101664 0.43421053]\n","fscore: [0.77078652 0.58367347]\n","support: [1082  988]\n","accuracy: 0.7043478260869566\n","auc: 0.6926135810876545\n","              precision    recall  f1-score   support\n","\n","           0       0.65      0.95      0.77      1082\n","           1       0.89      0.43      0.58       988\n","\n","   micro avg       0.70      0.70      0.70      2070\n","   macro avg       0.77      0.69      0.68      2070\n","weighted avg       0.76      0.70      0.68      2070\n"," samples avg       0.70      0.70      0.70      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5p0cC6IbDiL"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_bert_input, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=3, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sW8BBbnwbDiL"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"0aM6-s7pRazL"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7207,"status":"ok","timestamp":1679916838207,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"J-bDFjxRbEJ7","outputId":"0caf37ac-983c-4dd6-d482-aeac03310e3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","66/66 [==============================] - 2s 19ms/step - loss: 0.8720 - accuracy: 0.5146 - val_loss: 0.6878 - val_accuracy: 0.6384\n","Epoch 2/10\n","66/66 [==============================] - 1s 15ms/step - loss: 0.6388 - accuracy: 0.6499 - val_loss: 1.4220 - val_accuracy: 0.4601\n","Epoch 3/10\n","66/66 [==============================] - 1s 17ms/step - loss: 0.2334 - accuracy: 0.9217 - val_loss: 2.7579 - val_accuracy: 0.4601\n","Epoch 4/10\n","66/66 [==============================] - 1s 17ms/step - loss: 0.1656 - accuracy: 0.9525 - val_loss: 1.4625 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f13902b9070\u003e"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_bert_input, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=10, batch_size=batch_size, callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":888,"status":"ok","timestamp":1679916839090,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"s07pk1wxbEJ8","outputId":"340195aa-7f44-4445-e4b6-4b6521422c54"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 7ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","auc: 0.5\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157190,"status":"ok","timestamp":1679641009061,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"JGvh1I1YbEJ8","outputId":"bc007b63-1103-4295-8095-1401201f5033"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","66/66 [==============================] - 75s 932ms/step - loss: 0.3204 - accuracy: 0.8652 - val_loss: 0.5826 - val_accuracy: 0.5482\n","Epoch 2/2\n","66/66 [==============================] - 56s 852ms/step - loss: 0.1125 - accuracy: 0.9670 - val_loss: 0.5510 - val_accuracy: 0.7464\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f396b9efeb0\u003e"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_bert_input, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=2, batch_size=batch_size, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5593,"status":"ok","timestamp":1679641014627,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"NqPP5TKsbEJ8","outputId":"379f57dd-281c-4edb-eb61-feb6bfe40d0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 4s 59ms/step\n","precision: [0.98950131 0.68140351]\n","recall: [0.45367028 0.99589744]\n","fscore: [0.62211221 0.80916667]\n","support: [831 975]\n","accuracy: 0.7464008859357697\n","auc: 0.7247838563362028\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.45      0.62       831\n","           1       0.68      1.00      0.81       975\n","\n","   micro avg       0.75      0.75      0.75      1806\n","   macro avg       0.84      0.72      0.72      1806\n","weighted avg       0.82      0.75      0.72      1806\n"," samples avg       0.75      0.75      0.75      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181345,"status":"ok","timestamp":1679918399015,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"vv0PxoB6bEJ8","outputId":"5a994475-791e-4e87-ea3d-9d1c9cdaea54"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Layer text_GRU will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 156s 2s/step - loss: 0.3587 - accuracy: 0.8493 - val_loss: 0.6156 - val_accuracy: 0.9408\n","Epoch 2/5\n","66/66 [==============================] - 139s 2s/step - loss: 0.1206 - accuracy: 0.9618 - val_loss: 0.5453 - val_accuracy: 0.9037\n","Epoch 3/5\n","66/66 [==============================] - 132s 2s/step - loss: 0.0758 - accuracy: 0.9758 - val_loss: 0.4806 - val_accuracy: 0.9313\n","Epoch 4/5\n","66/66 [==============================] - 75s 1s/step - loss: 0.0528 - accuracy: 0.9850 - val_loss: 0.4043 - val_accuracy: 0.9330\n","Epoch 5/5\n","66/66 [==============================] - 75s 1s/step - loss: 0.0418 - accuracy: 0.9886 - val_loss: 0.3184 - val_accuracy: 0.9480\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f135d523040\u003e"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_bert_input, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size, callbacks=[early_stopping])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3804,"status":"ok","timestamp":1679918402813,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ihfcq5K6bEJ9","outputId":"e73c13eb-3ec8-4600-d0bc-1c04ecda62ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 4s 68ms/step\n","precision: [0.95776398 0.94005994]\n","recall: [0.92779783 0.96512821]\n","fscore: [0.94254279 0.95242915]\n","support: [831 975]\n","accuracy: 0.9479512735326688\n","auc: 0.9464630195316115\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.93      0.94       831\n","           1       0.94      0.97      0.95       975\n","\n","   micro avg       0.95      0.95      0.95      1806\n","   macro avg       0.95      0.95      0.95      1806\n","weighted avg       0.95      0.95      0.95      1806\n"," samples avg       0.95      0.95      0.95      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"executionInfo":{"elapsed":10718,"status":"error","timestamp":1679639905091,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"CqL6zXRRbEJ9","outputId":"25e98379-5a99-4e62-edf6-b95d2e504ef5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n"]},{"ename":"InvalidArgumentError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-22-674c85c24d51\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 85\u001b[0;31m model.fit(x=np.array(X_text_train), y=np.array(y_train), \n\u001b[0m\u001b[1;32m     86\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m           epochs=3, batch_size=batch_size)\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNNV2' used by {{node model_8/text_bilstm1/forward_cu_dnnlstm/CudnnRNNV2}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\nRegistered devices: [CPU]\nRegistered kernels:\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in [DT_FLOAT]\n  device='GPU'; T in [DT_HALF]\n\n\t [[model_8/text_bilstm1/forward_cu_dnnlstm/CudnnRNNV2]] [Op:__inference_train_function_48264]"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_bert_input, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=3, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yte2TQo7bEJ9"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","from sklearn.metrics import roc_auc_score\n","\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","auc = roc_auc_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","print('auc: {}'.format(auc))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"G1i_TN1jIKt4"},"source":["### TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"OjaIHQauRbXb"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":247353,"status":"ok","timestamp":1678601336705,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"WkMU2tSFbE5-","outputId":"ea687638-c81c-495c-d931-f85946004c51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 54s 687ms/step - loss: 0.8501 - accuracy: 0.5065 - val_loss: 0.6974 - val_accuracy: 0.4773\n","Epoch 2/5\n","76/76 [==============================] - 46s 613ms/step - loss: 0.8127 - accuracy: 0.5040 - val_loss: 0.6991 - val_accuracy: 0.4773\n","Epoch 3/5\n","76/76 [==============================] - 46s 612ms/step - loss: 0.7767 - accuracy: 0.5111 - val_loss: 0.6958 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 50s 657ms/step - loss: 0.7716 - accuracy: 0.4945 - val_loss: 0.6983 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 47s 620ms/step - loss: 0.7532 - accuracy: 0.4962 - val_loss: 0.6923 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f65c78a1340\u003e"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_tfidf_1gram, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3104,"status":"ok","timestamp":1678601339785,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"qhxHmrkgbE5_","outputId":"0e26acd2-3f46-4a85-8ca7-3a0baf7739cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 3s 37ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":257879,"status":"ok","timestamp":1678601597659,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Du9he-LKbE5_","outputId":"26bdd4d9-d887-4b3c-efd5-2c11e8a73578"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 58s 697ms/step - loss: 0.6966 - accuracy: 0.5053 - val_loss: 0.6931 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 49s 644ms/step - loss: 0.6928 - accuracy: 0.5202 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 50s 665ms/step - loss: 0.6929 - accuracy: 0.5227 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 49s 643ms/step - loss: 0.6926 - accuracy: 0.5206 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 51s 665ms/step - loss: 0.6926 - accuracy: 0.5227 - val_loss: 0.6928 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f65c4c22040\u003e"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2913,"status":"ok","timestamp":1678601600544,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Xl1gUCOgbE5_","outputId":"c2b21a97-c1a8-4d93-8a0e-346d52c7face"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 3s 43ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268577,"status":"ok","timestamp":1678601869115,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ik5hOftWbE5_","outputId":"212f7b91-0147-4832-ae79-5ed4847e5ec7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 51s 616ms/step - loss: 0.6976 - accuracy: 0.5165 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 49s 651ms/step - loss: 0.6929 - accuracy: 0.5189 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 47s 616ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 47s 619ms/step - loss: 0.6928 - accuracy: 0.5225 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 47s 617ms/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6924 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f65a754ffa0\u003e"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_tfidf_1gram, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5400,"status":"ok","timestamp":1678601874498,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ycPV4grsbE6A","outputId":"89db6669-d04c-4060-d977-68f4a0bb1019"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 3s 39ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27222,"status":"ok","timestamp":1678617876397,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"aTK4ps1PbE6A","outputId":"0973a4cf-1c93-4b34-e46d-4c0daa0e5331"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 8s 58ms/step - loss: 0.6934 - accuracy: 0.5080 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 4s 57ms/step - loss: 0.6933 - accuracy: 0.5212 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6931 - accuracy: 0.5192 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6923 - accuracy: 0.5252 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6930 - accuracy: 0.5146 - val_loss: 0.6923 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f982ce59130\u003e"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_df_tfidf_1gram, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2916,"status":"ok","timestamp":1678617879296,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Ff3I5iY_bE6A","outputId":"f6e837fd-a8ce-4626-84c2-c334623cbd31"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 21ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"heKU6sv_RbXc"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208736,"status":"ok","timestamp":1678602181075,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"RrMoeXLEbFdi","outputId":"6fcff29a-0597-4309-d3a2-d81efb8292ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 43s 611ms/step - loss: 0.9174 - accuracy: 0.5015 - val_loss: 0.6993 - val_accuracy: 0.4601\n","Epoch 2/5\n","66/66 [==============================] - 41s 616ms/step - loss: 0.8445 - accuracy: 0.5053 - val_loss: 0.7143 - val_accuracy: 0.4601\n","Epoch 3/5\n","66/66 [==============================] - 41s 621ms/step - loss: 0.8155 - accuracy: 0.5030 - val_loss: 0.7933 - val_accuracy: 0.4601\n","Epoch 4/5\n","66/66 [==============================] - 41s 631ms/step - loss: 0.7955 - accuracy: 0.4980 - val_loss: 0.8792 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 41s 626ms/step - loss: 0.7813 - accuracy: 0.5013 - val_loss: 0.8127 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f65a6faa8e0\u003e"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_tfidf_1gram, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2511,"status":"ok","timestamp":1678602183558,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ntFGa-58bFdj","outputId":"4f3037ed-3dd3-40b9-ed78-d8144d4fe0f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 35ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":268007,"status":"ok","timestamp":1678602451559,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"oFU4nzjibFdj","outputId":"b08b4118-f88a-4321-edd1-591256f94256"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 48s 664ms/step - loss: 0.7001 - accuracy: 0.5208 - val_loss: 0.6901 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 44s 659ms/step - loss: 0.6914 - accuracy: 0.5350 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 42s 640ms/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 43s 657ms/step - loss: 0.6910 - accuracy: 0.5395 - val_loss: 0.6908 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 42s 639ms/step - loss: 0.6910 - accuracy: 0.5395 - val_loss: 0.6901 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f65a3c08f40\u003e"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1858,"status":"ok","timestamp":1678602453387,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"GGrnYsPkbFdj","outputId":"f6dfecdb-0fca-47b3-fb12-96eb98b05e10"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 42ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207604,"status":"ok","timestamp":1678608714831,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"A6fDZmHHbFdj","outputId":"95c9f9e2-5fad-4a67-dba8-09570e88e0ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 44s 616ms/step - loss: 0.6987 - accuracy: 0.5265 - val_loss: 0.6903 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 41s 621ms/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6905 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 39s 598ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6920 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 37s 557ms/step - loss: 0.6906 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 39s 594ms/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6908 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa63c1ae8b0\u003e"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_tfidf_1gram, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2369,"status":"ok","timestamp":1678608717177,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"UhZxURv3bFdj","outputId":"6b5b1a9a-62aa-4641-9758-7b1cd5d80bd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 36ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20539,"status":"ok","timestamp":1678617899832,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"HNWHFaNAbFdj","outputId":"0bb270ee-4f8f-4899-e451-1fd6db0eeaed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 7s 58ms/step - loss: 0.6913 - accuracy: 0.5333 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 3s 49ms/step - loss: 0.6915 - accuracy: 0.5338 - val_loss: 0.6910 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 5s 68ms/step - loss: 0.6915 - accuracy: 0.5381 - val_loss: 0.6903 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 3s 44ms/step - loss: 0.6908 - accuracy: 0.5350 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 3s 44ms/step - loss: 0.6908 - accuracy: 0.5383 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f982ca820d0\u003e"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_df_tfidf_1gram, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1221,"status":"ok","timestamp":1678617901051,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"6hFPQK6KbFdj","outputId":"2a3b8aa7-7f1f-4e6a-db3f-0f5a6dd8b9cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 14ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"r8Ft-RGjIKzY"},"source":["### TF-IDF + hashtags + Retweets + POS + FT"]},{"cell_type":"markdown","metadata":{"id":"bNvtbxB9RcCI"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271711,"status":"ok","timestamp":1678607965178,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"g31DSCp5bGG8","outputId":"7a05e236-4a47-4b6b-8698-08acdc2f1f2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 45s 575ms/step - loss: 0.9083 - accuracy: 0.4931 - val_loss: 0.7206 - val_accuracy: 0.4773\n","Epoch 2/5\n","76/76 [==============================] - 44s 577ms/step - loss: 0.8338 - accuracy: 0.5028 - val_loss: 0.6934 - val_accuracy: 0.4773\n","Epoch 3/5\n","76/76 [==============================] - 45s 587ms/step - loss: 0.8097 - accuracy: 0.4970 - val_loss: 0.7595 - val_accuracy: 0.4773\n","Epoch 4/5\n","76/76 [==============================] - 44s 584ms/step - loss: 0.7755 - accuracy: 0.5109 - val_loss: 0.7671 - val_accuracy: 0.4773\n","Epoch 5/5\n","76/76 [==============================] - 47s 611ms/step - loss: 0.7680 - accuracy: 0.4937 - val_loss: 0.7374 - val_accuracy: 0.4773\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6444b0ee0\u003e"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_FT, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2710,"status":"ok","timestamp":1678607967870,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"J-L3KawvbGG8","outputId":"a690aaab-5426-4d2a-a7a0-cc48d371a4d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 35ms/step\n","precision: [0.         0.47729469]\n","recall: [0. 1.]\n","fscore: [0.         0.64617397]\n","support: [1082  988]\n","accuracy: 0.47729468599033814\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1082\n","           1       0.48      1.00      0.65       988\n","\n","   micro avg       0.48      0.48      0.48      2070\n","   macro avg       0.24      0.50      0.32      2070\n","weighted avg       0.23      0.48      0.31      2070\n"," samples avg       0.48      0.48      0.48      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245679,"status":"ok","timestamp":1678608213544,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"V8yqtcQHbGG8","outputId":"c8262c8c-76c4-48b2-f511-f529df2e7eb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 52s 630ms/step - loss: 0.6977 - accuracy: 0.5096 - val_loss: 0.6932 - val_accuracy: 0.4773\n","Epoch 2/5\n","76/76 [==============================] - 47s 616ms/step - loss: 0.6924 - accuracy: 0.5194 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 50s 660ms/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 46s 610ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 49s 649ms/step - loss: 0.6928 - accuracy: 0.5198 - val_loss: 0.6924 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa6440fa670\u003e"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2281,"status":"ok","timestamp":1678608215800,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"aucjkf4DbGG8","outputId":"788e97ce-1625-42c4-9b25-9e9405e573bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 3s 41ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266471,"status":"ok","timestamp":1678608482267,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"HXfcI5KEbGG8","outputId":"84e4a083-f74f-415e-c3db-11f0245e12d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 52s 648ms/step - loss: 0.6995 - accuracy: 0.5061 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 48s 638ms/step - loss: 0.6925 - accuracy: 0.5227 - val_loss: 0.6934 - val_accuracy: 0.4773\n","Epoch 3/5\n","76/76 [==============================] - 47s 620ms/step - loss: 0.6930 - accuracy: 0.5098 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 44s 584ms/step - loss: 0.6926 - accuracy: 0.5227 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 43s 570ms/step - loss: 0.6927 - accuracy: 0.5185 - val_loss: 0.6924 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fa63c776fa0\u003e"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_FT, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2618,"status":"ok","timestamp":1678608484860,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"tpyTnnMSbGG8","outputId":"494d1391-77cd-4409-bdce-89470e774624"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 3s 37ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25882,"status":"ok","timestamp":1678617926932,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"WnhC9Wu8bGG8","outputId":"934f3434-813c-4001-dc1f-a4b4397b9090"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 8s 56ms/step - loss: 0.6935 - accuracy: 0.5067 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6935 - accuracy: 0.5160 - val_loss: 0.6950 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6935 - accuracy: 0.5198 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 3s 46ms/step - loss: 0.6929 - accuracy: 0.5202 - val_loss: 0.6926 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 3s 44ms/step - loss: 0.6928 - accuracy: 0.5189 - val_loss: 0.6934 - val_accuracy: 0.4773\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f982ca0b8e0\u003e"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_FT, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":957,"status":"ok","timestamp":1678617927873,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Ah19ObVubGG9","outputId":"95f03e17-dedc-4bf1-c046-0cac50238643"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 16ms/step\n","precision: [0.         0.47729469]\n","recall: [0. 1.]\n","fscore: [0.         0.64617397]\n","support: [1082  988]\n","accuracy: 0.47729468599033814\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1082\n","           1       0.48      1.00      0.65       988\n","\n","   micro avg       0.48      0.48      0.48      2070\n","   macro avg       0.24      0.50      0.32      2070\n","weighted avg       0.23      0.48      0.31      2070\n"," samples avg       0.48      0.48      0.48      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"QCJDla_RRcCJ"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388629,"status":"ok","timestamp":1678609934074,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"AA8DR9EubGqj","outputId":"a2296670-f60f-41d7-d649-720a3bd4d67c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 67s 975ms/step - loss: 0.9422 - accuracy: 0.5246 - val_loss: 0.7712 - val_accuracy: 0.4601\n","Epoch 2/5\n","66/66 [==============================] - 60s 904ms/step - loss: 0.8507 - accuracy: 0.4989 - val_loss: 0.8051 - val_accuracy: 0.4601\n","Epoch 3/5\n","66/66 [==============================] - 63s 955ms/step - loss: 0.8004 - accuracy: 0.5051 - val_loss: 0.8438 - val_accuracy: 0.4601\n","Epoch 4/5\n","66/66 [==============================] - 72s 1s/step - loss: 0.7800 - accuracy: 0.5056 - val_loss: 0.8437 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 72s 1s/step - loss: 0.7679 - accuracy: 0.5058 - val_loss: 0.8822 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c8803ca00\u003e"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_FT, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10385,"status":"ok","timestamp":1678609944435,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"trOwpIW3bGqj","outputId":"c5ae4604-c706-41a2-b3ac-cd94d931d519"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 91ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":437601,"status":"ok","timestamp":1678610382021,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"xR1VWATYbGqk","outputId":"fb94b9c7-b321-455c-f063-d692227e205d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 97s 1s/step - loss: 0.7009 - accuracy: 0.5144 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 88s 1s/step - loss: 0.6924 - accuracy: 0.5281 - val_loss: 0.6942 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 83s 1s/step - loss: 0.6909 - accuracy: 0.5395 - val_loss: 0.6907 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 84s 1s/step - loss: 0.6909 - accuracy: 0.5395 - val_loss: 0.6910 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 83s 1s/step - loss: 0.6911 - accuracy: 0.5395 - val_loss: 0.6904 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c8925fe20\u003e"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6018,"status":"ok","timestamp":1678610388016,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"ZGeTftWDbGqk","outputId":"4886da77-3ebb-44a2-b30f-81967b600530"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 5s 79ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455793,"status":"ok","timestamp":1678610843794,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"-WlbE1N6bGqk","outputId":"a17aebb1-b3c4-4cef-a8d2-9744df1b544f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 91s 1s/step - loss: 0.7023 - accuracy: 0.5193 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 84s 1s/step - loss: 0.6907 - accuracy: 0.5395 - val_loss: 0.6918 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 78s 1s/step - loss: 0.6909 - accuracy: 0.5395 - val_loss: 0.6913 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 75s 1s/step - loss: 0.6909 - accuracy: 0.5395 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 76s 1s/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c88c799a0\u003e"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_FT, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5176,"status":"ok","timestamp":1678610848945,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"dXNE_6TPbGqk","outputId":"f59eaede-bbbe-4424-8820-82fd8c8c4fd5"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 87ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20229,"status":"ok","timestamp":1678617948098,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"n_nbmELJbGqk","outputId":"5085e8fb-fe71-4e2d-d445-1102a211b121"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 8s 62ms/step - loss: 0.6919 - accuracy: 0.5345 - val_loss: 0.6909 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 3s 49ms/step - loss: 0.6909 - accuracy: 0.5400 - val_loss: 0.6905 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 3s 48ms/step - loss: 0.6908 - accuracy: 0.5402 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 3s 48ms/step - loss: 0.6897 - accuracy: 0.5371 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 3s 44ms/step - loss: 0.6912 - accuracy: 0.5362 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f97b40ea3a0\u003e"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_FT, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2207,"status":"ok","timestamp":1678617950301,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"j27u3pJtbGqk","outputId":"42c5517e-d51c-4b90-a707-ab474bef60c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 14ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"kMY7dv6UIK4W"},"source":["### TF-IDF + hashtags + Retweets + POS + word2vec"]},{"cell_type":"markdown","metadata":{"id":"dqe-DuLeRcjn"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269199,"status":"ok","timestamp":1678611136628,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"5xAg-G1ubHN7","outputId":"c46a2195-cb81-4358-962f-47e746f40436"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 49s 626ms/step - loss: 0.8996 - accuracy: 0.4883 - val_loss: 0.6973 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 44s 575ms/step - loss: 0.8279 - accuracy: 0.4939 - val_loss: 0.6955 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 43s 563ms/step - loss: 0.7872 - accuracy: 0.5125 - val_loss: 0.7087 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 44s 580ms/step - loss: 0.7755 - accuracy: 0.4960 - val_loss: 0.6980 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 41s 540ms/step - loss: 0.7548 - accuracy: 0.5036 - val_loss: 0.6922 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c885ca610\u003e"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_w2v, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3905,"status":"ok","timestamp":1678611140517,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"6HdGqxDvbHN8","outputId":"ede75b36-b4d8-42e0-9c60-5688eba13de5"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 56ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266332,"status":"ok","timestamp":1678611406825,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"u4DE11qIbHN8","outputId":"45459966-54b9-49cc-9f3a-a83abf053cba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 52s 640ms/step - loss: 0.6983 - accuracy: 0.5065 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 47s 616ms/step - loss: 0.6925 - accuracy: 0.5206 - val_loss: 0.6935 - val_accuracy: 0.4773\n","Epoch 3/5\n","76/76 [==============================] - 49s 639ms/step - loss: 0.6928 - accuracy: 0.5148 - val_loss: 0.6925 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 47s 624ms/step - loss: 0.6928 - accuracy: 0.5194 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 47s 619ms/step - loss: 0.6928 - accuracy: 0.5210 - val_loss: 0.6922 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c88cf3040\u003e"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5166,"status":"ok","timestamp":1678611411965,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"JUQ5-5FmbHN8","outputId":"3feb5149-4d49-4dca-ec14-73d4d734f6cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 57ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271801,"status":"ok","timestamp":1678611683743,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"vPClhmpfbHN8","outputId":"42c85da9-ea83-4764-c994-1e81459665e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 46s 566ms/step - loss: 0.6996 - accuracy: 0.5127 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 42s 560ms/step - loss: 0.6930 - accuracy: 0.5148 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 49s 642ms/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 43s 571ms/step - loss: 0.6928 - accuracy: 0.5227 - val_loss: 0.6923 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 44s 578ms/step - loss: 0.6927 - accuracy: 0.5227 - val_loss: 0.6924 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c88216f40\u003e"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_w2v, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1252,"status":"ok","timestamp":1678611684970,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"7NlDikQ3bHN8","outputId":"46935240-0a6c-4c05-d955-c79d01e80bb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 3s 45ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63137,"status":"ok","timestamp":1678618293870,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"DvdzuOnhbHN8","outputId":"ed8d24ac-9615-495f-f56d-8c5c1e314b2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 28s 170ms/step - loss: 0.6940 - accuracy: 0.5179 - val_loss: 0.6950 - val_accuracy: 0.4773\n","Epoch 2/5\n","76/76 [==============================] - 7s 87ms/step - loss: 0.6934 - accuracy: 0.5156 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 7s 97ms/step - loss: 0.6929 - accuracy: 0.5208 - val_loss: 0.6950 - val_accuracy: 0.4773\n","Epoch 4/5\n","76/76 [==============================] - 5s 70ms/step - loss: 0.6933 - accuracy: 0.5136 - val_loss: 0.6922 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 8s 100ms/step - loss: 0.6932 - accuracy: 0.5150 - val_loss: 0.6925 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7fdcec4ec3d0\u003e"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_w2v, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2442,"status":"ok","timestamp":1678618296296,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Pszuxtz5bHN8","outputId":"16d5b666-c641-4b0d-8158-09db950d095f"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 3s 24ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"o-6u4nnjRcjo"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204626,"status":"ok","timestamp":1678611889593,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"FWcd_m_cbHyt","outputId":"19c23ece-a3ff-4a48-e230-1c0219cbc659"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 40s 583ms/step - loss: 0.9023 - accuracy: 0.4996 - val_loss: 0.6975 - val_accuracy: 0.4601\n","Epoch 2/5\n","66/66 [==============================] - 39s 586ms/step - loss: 0.8392 - accuracy: 0.5061 - val_loss: 0.7036 - val_accuracy: 0.4601\n","Epoch 3/5\n","66/66 [==============================] - 40s 613ms/step - loss: 0.8111 - accuracy: 0.5087 - val_loss: 0.7632 - val_accuracy: 0.4601\n","Epoch 4/5\n","66/66 [==============================] - 39s 591ms/step - loss: 0.7770 - accuracy: 0.5312 - val_loss: 0.7057 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 39s 598ms/step - loss: 0.7798 - accuracy: 0.5011 - val_loss: 0.6988 - val_accuracy: 0.4601\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c73cdbac0\u003e"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_w2v, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3545,"status":"ok","timestamp":1678611893111,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"gy-1qsnzbHyu","outputId":"59b6149a-1313-46f0-f98a-e2aa399fa8c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 35ms/step\n","precision: [0.46013289 0.        ]\n","recall: [1. 0.]\n","fscore: [0.63026166 0.        ]\n","support: [831 975]\n","accuracy: 0.4601328903654485\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       0.00      0.00      0.00       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.23      0.50      0.32      1806\n","weighted avg       0.21      0.46      0.29      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266127,"status":"ok","timestamp":1678612159225,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"AlKbuX5pbHyu","outputId":"2f6a6302-2598-4f4f-ba4d-fa3633dd0fb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 46s 634ms/step - loss: 0.6970 - accuracy: 0.5215 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 42s 630ms/step - loss: 0.6908 - accuracy: 0.5395 - val_loss: 0.6903 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 41s 622ms/step - loss: 0.6904 - accuracy: 0.5395 - val_loss: 0.6948 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 42s 632ms/step - loss: 0.6911 - accuracy: 0.5395 - val_loss: 0.6915 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 42s 633ms/step - loss: 0.6912 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c73951c40\u003e"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2902,"status":"ok","timestamp":1678612162099,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"-sAmTtSfbHyu","outputId":"8d484e25-7151-43b9-f645-68094474bbac"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 40ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206473,"status":"ok","timestamp":1678612368567,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Dswqs4JzbHyu","outputId":"5c11b14b-ffdd-44d6-e1bd-1fced57282b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 41s 559ms/step - loss: 0.6967 - accuracy: 0.5139 - val_loss: 0.6901 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 36s 540ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 38s 571ms/step - loss: 0.6910 - accuracy: 0.5395 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 33s 505ms/step - loss: 0.6910 - accuracy: 0.5395 - val_loss: 0.6901 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 37s 563ms/step - loss: 0.6907 - accuracy: 0.5395 - val_loss: 0.6906 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c6e2e40a0\u003e"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_w2v, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3714,"status":"ok","timestamp":1678612372257,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"kRQljDOXbHyu","outputId":"dcba3b7b-94ea-4bab-9710-3ad92bf6cbad"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 54ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60639,"status":"ok","timestamp":1678618543555,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"PBGwQb7dbHyu","outputId":"f28ef6f8-b7e8-457a-e478-2e66abf99d80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 17s 78ms/step - loss: 0.6904 - accuracy: 0.5352 - val_loss: 0.6900 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 4s 55ms/step - loss: 0.6908 - accuracy: 0.5383 - val_loss: 0.6938 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 6s 86ms/step - loss: 0.6912 - accuracy: 0.5386 - val_loss: 0.6902 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 4s 53ms/step - loss: 0.6902 - accuracy: 0.5345 - val_loss: 0.6912 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 4s 55ms/step - loss: 0.6912 - accuracy: 0.5333 - val_loss: 0.6910 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f204ef90fa0\u003e"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_w2v, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3487,"status":"ok","timestamp":1678618547018,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"TDjCenn0bHyu","outputId":"68628e87-7ed1-4905-91c9-075eab96b224"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 21ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"uRRxL5BsIK94"},"source":["### TF-IDF + hashtags + Retweets + POS + Glove"]},{"cell_type":"markdown","metadata":{"id":"vP-PLJhcRdJj"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":804258,"status":"ok","timestamp":1678530231014,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"YffDfPqZbK0G","outputId":"06bba554-129c-4e19-cfb3-617a7fed9910"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 37s 468ms/step - loss: 0.8629 - accuracy: 0.5022 - val_loss: 0.7125 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 39s 517ms/step - loss: 0.8132 - accuracy: 0.4929 - val_loss: 0.6959 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 37s 493ms/step - loss: 0.7889 - accuracy: 0.5018 - val_loss: 0.7035 - val_accuracy: 0.4773\n","Epoch 4/20\n","76/76 [==============================] - 40s 522ms/step - loss: 0.7559 - accuracy: 0.5146 - val_loss: 0.7922 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 37s 487ms/step - loss: 0.7611 - accuracy: 0.5001 - val_loss: 0.7142 - val_accuracy: 0.4773\n","Epoch 6/20\n","76/76 [==============================] - 38s 499ms/step - loss: 0.7501 - accuracy: 0.4926 - val_loss: 0.7083 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 38s 502ms/step - loss: 0.7379 - accuracy: 0.4935 - val_loss: 0.6973 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 34s 449ms/step - loss: 0.7242 - accuracy: 0.5119 - val_loss: 0.7004 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 38s 501ms/step - loss: 0.7196 - accuracy: 0.5063 - val_loss: 0.7242 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 36s 475ms/step - loss: 0.7110 - accuracy: 0.5111 - val_loss: 0.7234 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 38s 500ms/step - loss: 0.7119 - accuracy: 0.5007 - val_loss: 0.7256 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 37s 488ms/step - loss: 0.7056 - accuracy: 0.5071 - val_loss: 0.7190 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 39s 511ms/step - loss: 0.7009 - accuracy: 0.5202 - val_loss: 0.7076 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 35s 461ms/step - loss: 0.6994 - accuracy: 0.5144 - val_loss: 0.7036 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 37s 489ms/step - loss: 0.6996 - accuracy: 0.5057 - val_loss: 0.7005 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 37s 483ms/step - loss: 0.6955 - accuracy: 0.5173 - val_loss: 0.6963 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 39s 510ms/step - loss: 0.6954 - accuracy: 0.5146 - val_loss: 0.6969 - val_accuracy: 0.4773\n","Epoch 18/20\n","76/76 [==============================] - 36s 475ms/step - loss: 0.6979 - accuracy: 0.4982 - val_loss: 0.6933 - val_accuracy: 0.4797\n","Epoch 19/20\n","76/76 [==============================] - 38s 498ms/step - loss: 0.6944 - accuracy: 0.5107 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 40s 533ms/step - loss: 0.6960 - accuracy: 0.4995 - val_loss: 0.6924 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f98730e44f0\u003e"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_glove, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2203,"status":"ok","timestamp":1678530233208,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"VqDX4DgCbK0H","outputId":"0951fa15-4292-4376-8126-a69738951cdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 31ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":786020,"status":"ok","timestamp":1678531236446,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"33GnTkSqbK0H","outputId":"bd27cdc1-5d0a-4dc8-e4e2-d22dab85ed19"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 55s 603ms/step - loss: 0.6959 - accuracy: 0.5169 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 40s 523ms/step - loss: 0.6909 - accuracy: 0.5241 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 38s 503ms/step - loss: 0.6865 - accuracy: 0.5129 - val_loss: 0.6909 - val_accuracy: 0.5232\n","Epoch 4/20\n","76/76 [==============================] - 37s 490ms/step - loss: 0.6832 - accuracy: 0.5260 - val_loss: 0.6900 - val_accuracy: 0.5237\n","Epoch 5/20\n","76/76 [==============================] - 39s 519ms/step - loss: 0.6823 - accuracy: 0.5156 - val_loss: 0.6893 - val_accuracy: 0.5246\n","Epoch 6/20\n","76/76 [==============================] - 36s 479ms/step - loss: 0.6815 - accuracy: 0.5202 - val_loss: 0.6891 - val_accuracy: 0.5246\n","Epoch 7/20\n","76/76 [==============================] - 37s 485ms/step - loss: 0.6802 - accuracy: 0.5225 - val_loss: 0.6881 - val_accuracy: 0.5232\n","Epoch 8/20\n","76/76 [==============================] - 37s 492ms/step - loss: 0.6803 - accuracy: 0.5223 - val_loss: 0.6884 - val_accuracy: 0.5261\n","Epoch 9/20\n","76/76 [==============================] - 40s 520ms/step - loss: 0.6790 - accuracy: 0.5196 - val_loss: 0.6899 - val_accuracy: 0.5261\n","Epoch 10/20\n","76/76 [==============================] - 39s 518ms/step - loss: 0.6775 - accuracy: 0.5266 - val_loss: 0.6925 - val_accuracy: 0.5251\n","Epoch 11/20\n","76/76 [==============================] - 38s 500ms/step - loss: 0.6780 - accuracy: 0.5274 - val_loss: 0.6967 - val_accuracy: 0.4947\n","Epoch 12/20\n","76/76 [==============================] - 39s 508ms/step - loss: 0.6770 - accuracy: 0.5243 - val_loss: 0.6968 - val_accuracy: 0.5256\n","Epoch 13/20\n","76/76 [==============================] - 40s 522ms/step - loss: 0.6769 - accuracy: 0.5258 - val_loss: 0.6981 - val_accuracy: 0.4957\n","Epoch 14/20\n","76/76 [==============================] - 36s 477ms/step - loss: 0.6758 - accuracy: 0.5252 - val_loss: 0.7026 - val_accuracy: 0.5251\n","Epoch 15/20\n","76/76 [==============================] - 40s 525ms/step - loss: 0.6761 - accuracy: 0.5208 - val_loss: 0.6993 - val_accuracy: 0.5261\n","Epoch 16/20\n","76/76 [==============================] - 38s 501ms/step - loss: 0.6748 - accuracy: 0.5293 - val_loss: 0.7042 - val_accuracy: 0.4952\n","Epoch 17/20\n","76/76 [==============================] - 38s 504ms/step - loss: 0.6746 - accuracy: 0.5202 - val_loss: 0.7030 - val_accuracy: 0.5242\n","Epoch 18/20\n","76/76 [==============================] - 38s 496ms/step - loss: 0.6748 - accuracy: 0.5258 - val_loss: 0.7075 - val_accuracy: 0.5251\n","Epoch 19/20\n","76/76 [==============================] - 39s 515ms/step - loss: 0.6741 - accuracy: 0.5283 - val_loss: 0.7093 - val_accuracy: 0.5246\n","Epoch 20/20\n","76/76 [==============================] - 39s 517ms/step - loss: 0.6745 - accuracy: 0.5256 - val_loss: 0.7076 - val_accuracy: 0.5246\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f968e8ceb80\u003e"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2712,"status":"ok","timestamp":1678531239150,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"JQjf_XU7bK0H","outputId":"aa04e901-39fd-478d-f92e-9a5e8b9c333c"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 33ms/step\n","precision: [0.52437811 0.53333333]\n","recall: [0.974122   0.03238866]\n","fscore: [0.68175938 0.0610687 ]\n","support: [1082  988]\n","accuracy: 0.5246376811594203\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.97      0.68      1082\n","           1       0.53      0.03      0.06       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.53      0.50      0.37      2070\n","weighted avg       0.53      0.52      0.39      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":745519,"status":"ok","timestamp":1678533908690,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"vdPauMMWbK0H","outputId":"cfc8bd2b-2f9b-425d-c8b0-596587bba8ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 52s 656ms/step - loss: 0.7008 - accuracy: 0.4939 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 36s 479ms/step - loss: 0.6896 - accuracy: 0.5138 - val_loss: 0.6918 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 36s 467ms/step - loss: 0.6855 - accuracy: 0.5283 - val_loss: 0.6911 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 35s 467ms/step - loss: 0.6819 - accuracy: 0.5247 - val_loss: 0.6902 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 38s 497ms/step - loss: 0.6781 - accuracy: 0.5250 - val_loss: 0.6905 - val_accuracy: 0.4957\n","Epoch 6/20\n","76/76 [==============================] - 33s 434ms/step - loss: 0.6768 - accuracy: 0.5289 - val_loss: 0.6893 - val_accuracy: 0.5242\n","Epoch 7/20\n","76/76 [==============================] - 37s 488ms/step - loss: 0.6744 - accuracy: 0.5347 - val_loss: 0.7210 - val_accuracy: 0.5251\n","Epoch 8/20\n","76/76 [==============================] - 35s 462ms/step - loss: 0.6738 - accuracy: 0.5276 - val_loss: 0.6955 - val_accuracy: 0.5232\n","Epoch 9/20\n","76/76 [==============================] - 35s 456ms/step - loss: 0.6736 - accuracy: 0.5260 - val_loss: 0.7009 - val_accuracy: 0.5246\n","Epoch 10/20\n","76/76 [==============================] - 37s 483ms/step - loss: 0.6715 - accuracy: 0.5272 - val_loss: 0.7082 - val_accuracy: 0.5237\n","Epoch 11/20\n","76/76 [==============================] - 33s 432ms/step - loss: 0.6720 - accuracy: 0.5347 - val_loss: 0.7191 - val_accuracy: 0.4937\n","Epoch 12/20\n","76/76 [==============================] - 37s 480ms/step - loss: 0.6717 - accuracy: 0.5320 - val_loss: 0.7282 - val_accuracy: 0.4952\n","Epoch 13/20\n","76/76 [==============================] - 35s 458ms/step - loss: 0.6709 - accuracy: 0.5171 - val_loss: 0.7260 - val_accuracy: 0.5232\n","Epoch 14/20\n","76/76 [==============================] - 35s 464ms/step - loss: 0.6694 - accuracy: 0.5382 - val_loss: 0.7321 - val_accuracy: 0.5242\n","Epoch 15/20\n","76/76 [==============================] - 37s 481ms/step - loss: 0.6694 - accuracy: 0.5320 - val_loss: 0.7360 - val_accuracy: 0.5232\n","Epoch 16/20\n","76/76 [==============================] - 33s 436ms/step - loss: 0.6687 - accuracy: 0.5328 - val_loss: 0.7387 - val_accuracy: 0.5232\n","Epoch 17/20\n","76/76 [==============================] - 36s 475ms/step - loss: 0.6689 - accuracy: 0.5343 - val_loss: 0.7412 - val_accuracy: 0.5232\n","Epoch 18/20\n","76/76 [==============================] - 36s 469ms/step - loss: 0.6693 - accuracy: 0.5328 - val_loss: 0.7432 - val_accuracy: 0.5232\n","Epoch 19/20\n","76/76 [==============================] - 35s 460ms/step - loss: 0.6682 - accuracy: 0.5374 - val_loss: 0.7430 - val_accuracy: 0.5237\n","Epoch 20/20\n","76/76 [==============================] - 37s 486ms/step - loss: 0.6671 - accuracy: 0.5330 - val_loss: 0.7466 - val_accuracy: 0.5232\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f968d627df0\u003e"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_glove, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2467,"status":"ok","timestamp":1678533911134,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"kuiJNpcCbK0H","outputId":"ea723838-2d16-4bbf-a90e-2aa391acc174"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 32ms/step\n","precision: [0.5235732  0.50909091]\n","recall: [0.97504621 0.02834008]\n","fscore: [0.68130449 0.05369128]\n","support: [1082  988]\n","accuracy: 0.5231884057971015\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.98      0.68      1082\n","           1       0.51      0.03      0.05       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.52      0.50      0.37      2070\n","weighted avg       0.52      0.52      0.38      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cO6P83aMbK0I"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_glove, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FmXGMfiFbK0I"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"fNgn-jpBRdJk"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":804258,"status":"ok","timestamp":1678530231014,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"5VG1E5qtbLZz","outputId":"06bba554-129c-4e19-cfb3-617a7fed9910"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 37s 468ms/step - loss: 0.8629 - accuracy: 0.5022 - val_loss: 0.7125 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 39s 517ms/step - loss: 0.8132 - accuracy: 0.4929 - val_loss: 0.6959 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 37s 493ms/step - loss: 0.7889 - accuracy: 0.5018 - val_loss: 0.7035 - val_accuracy: 0.4773\n","Epoch 4/20\n","76/76 [==============================] - 40s 522ms/step - loss: 0.7559 - accuracy: 0.5146 - val_loss: 0.7922 - val_accuracy: 0.4773\n","Epoch 5/20\n","76/76 [==============================] - 37s 487ms/step - loss: 0.7611 - accuracy: 0.5001 - val_loss: 0.7142 - val_accuracy: 0.4773\n","Epoch 6/20\n","76/76 [==============================] - 38s 499ms/step - loss: 0.7501 - accuracy: 0.4926 - val_loss: 0.7083 - val_accuracy: 0.5227\n","Epoch 7/20\n","76/76 [==============================] - 38s 502ms/step - loss: 0.7379 - accuracy: 0.4935 - val_loss: 0.6973 - val_accuracy: 0.4773\n","Epoch 8/20\n","76/76 [==============================] - 34s 449ms/step - loss: 0.7242 - accuracy: 0.5119 - val_loss: 0.7004 - val_accuracy: 0.4773\n","Epoch 9/20\n","76/76 [==============================] - 38s 501ms/step - loss: 0.7196 - accuracy: 0.5063 - val_loss: 0.7242 - val_accuracy: 0.4773\n","Epoch 10/20\n","76/76 [==============================] - 36s 475ms/step - loss: 0.7110 - accuracy: 0.5111 - val_loss: 0.7234 - val_accuracy: 0.4773\n","Epoch 11/20\n","76/76 [==============================] - 38s 500ms/step - loss: 0.7119 - accuracy: 0.5007 - val_loss: 0.7256 - val_accuracy: 0.4773\n","Epoch 12/20\n","76/76 [==============================] - 37s 488ms/step - loss: 0.7056 - accuracy: 0.5071 - val_loss: 0.7190 - val_accuracy: 0.4773\n","Epoch 13/20\n","76/76 [==============================] - 39s 511ms/step - loss: 0.7009 - accuracy: 0.5202 - val_loss: 0.7076 - val_accuracy: 0.4773\n","Epoch 14/20\n","76/76 [==============================] - 35s 461ms/step - loss: 0.6994 - accuracy: 0.5144 - val_loss: 0.7036 - val_accuracy: 0.4773\n","Epoch 15/20\n","76/76 [==============================] - 37s 489ms/step - loss: 0.6996 - accuracy: 0.5057 - val_loss: 0.7005 - val_accuracy: 0.4773\n","Epoch 16/20\n","76/76 [==============================] - 37s 483ms/step - loss: 0.6955 - accuracy: 0.5173 - val_loss: 0.6963 - val_accuracy: 0.4773\n","Epoch 17/20\n","76/76 [==============================] - 39s 510ms/step - loss: 0.6954 - accuracy: 0.5146 - val_loss: 0.6969 - val_accuracy: 0.4773\n","Epoch 18/20\n","76/76 [==============================] - 36s 475ms/step - loss: 0.6979 - accuracy: 0.4982 - val_loss: 0.6933 - val_accuracy: 0.4797\n","Epoch 19/20\n","76/76 [==============================] - 38s 498ms/step - loss: 0.6944 - accuracy: 0.5107 - val_loss: 0.6927 - val_accuracy: 0.5227\n","Epoch 20/20\n","76/76 [==============================] - 40s 533ms/step - loss: 0.6960 - accuracy: 0.4995 - val_loss: 0.6924 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f98730e44f0\u003e"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_glove, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2203,"status":"ok","timestamp":1678530233208,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"jKta8hwSbLZ0","outputId":"0951fa15-4292-4376-8126-a69738951cdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 31ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":786020,"status":"ok","timestamp":1678531236446,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"YChlSBoLbLZ0","outputId":"bd27cdc1-5d0a-4dc8-e4e2-d22dab85ed19"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 55s 603ms/step - loss: 0.6959 - accuracy: 0.5169 - val_loss: 0.6924 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 40s 523ms/step - loss: 0.6909 - accuracy: 0.5241 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 38s 503ms/step - loss: 0.6865 - accuracy: 0.5129 - val_loss: 0.6909 - val_accuracy: 0.5232\n","Epoch 4/20\n","76/76 [==============================] - 37s 490ms/step - loss: 0.6832 - accuracy: 0.5260 - val_loss: 0.6900 - val_accuracy: 0.5237\n","Epoch 5/20\n","76/76 [==============================] - 39s 519ms/step - loss: 0.6823 - accuracy: 0.5156 - val_loss: 0.6893 - val_accuracy: 0.5246\n","Epoch 6/20\n","76/76 [==============================] - 36s 479ms/step - loss: 0.6815 - accuracy: 0.5202 - val_loss: 0.6891 - val_accuracy: 0.5246\n","Epoch 7/20\n","76/76 [==============================] - 37s 485ms/step - loss: 0.6802 - accuracy: 0.5225 - val_loss: 0.6881 - val_accuracy: 0.5232\n","Epoch 8/20\n","76/76 [==============================] - 37s 492ms/step - loss: 0.6803 - accuracy: 0.5223 - val_loss: 0.6884 - val_accuracy: 0.5261\n","Epoch 9/20\n","76/76 [==============================] - 40s 520ms/step - loss: 0.6790 - accuracy: 0.5196 - val_loss: 0.6899 - val_accuracy: 0.5261\n","Epoch 10/20\n","76/76 [==============================] - 39s 518ms/step - loss: 0.6775 - accuracy: 0.5266 - val_loss: 0.6925 - val_accuracy: 0.5251\n","Epoch 11/20\n","76/76 [==============================] - 38s 500ms/step - loss: 0.6780 - accuracy: 0.5274 - val_loss: 0.6967 - val_accuracy: 0.4947\n","Epoch 12/20\n","76/76 [==============================] - 39s 508ms/step - loss: 0.6770 - accuracy: 0.5243 - val_loss: 0.6968 - val_accuracy: 0.5256\n","Epoch 13/20\n","76/76 [==============================] - 40s 522ms/step - loss: 0.6769 - accuracy: 0.5258 - val_loss: 0.6981 - val_accuracy: 0.4957\n","Epoch 14/20\n","76/76 [==============================] - 36s 477ms/step - loss: 0.6758 - accuracy: 0.5252 - val_loss: 0.7026 - val_accuracy: 0.5251\n","Epoch 15/20\n","76/76 [==============================] - 40s 525ms/step - loss: 0.6761 - accuracy: 0.5208 - val_loss: 0.6993 - val_accuracy: 0.5261\n","Epoch 16/20\n","76/76 [==============================] - 38s 501ms/step - loss: 0.6748 - accuracy: 0.5293 - val_loss: 0.7042 - val_accuracy: 0.4952\n","Epoch 17/20\n","76/76 [==============================] - 38s 504ms/step - loss: 0.6746 - accuracy: 0.5202 - val_loss: 0.7030 - val_accuracy: 0.5242\n","Epoch 18/20\n","76/76 [==============================] - 38s 496ms/step - loss: 0.6748 - accuracy: 0.5258 - val_loss: 0.7075 - val_accuracy: 0.5251\n","Epoch 19/20\n","76/76 [==============================] - 39s 515ms/step - loss: 0.6741 - accuracy: 0.5283 - val_loss: 0.7093 - val_accuracy: 0.5246\n","Epoch 20/20\n","76/76 [==============================] - 39s 517ms/step - loss: 0.6745 - accuracy: 0.5256 - val_loss: 0.7076 - val_accuracy: 0.5246\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f968e8ceb80\u003e"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2712,"status":"ok","timestamp":1678531239150,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"C1EGb9TubLZ0","outputId":"aa04e901-39fd-478d-f92e-9a5e8b9c333c"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 33ms/step\n","precision: [0.52437811 0.53333333]\n","recall: [0.974122   0.03238866]\n","fscore: [0.68175938 0.0610687 ]\n","support: [1082  988]\n","accuracy: 0.5246376811594203\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.97      0.68      1082\n","           1       0.53      0.03      0.06       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.53      0.50      0.37      2070\n","weighted avg       0.53      0.52      0.39      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":745519,"status":"ok","timestamp":1678533908690,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"q4ZtVYiMbLZ0","outputId":"cfc8bd2b-2f9b-425d-c8b0-596587bba8ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","76/76 [==============================] - 52s 656ms/step - loss: 0.7008 - accuracy: 0.4939 - val_loss: 0.6920 - val_accuracy: 0.5227\n","Epoch 2/20\n","76/76 [==============================] - 36s 479ms/step - loss: 0.6896 - accuracy: 0.5138 - val_loss: 0.6918 - val_accuracy: 0.5227\n","Epoch 3/20\n","76/76 [==============================] - 36s 467ms/step - loss: 0.6855 - accuracy: 0.5283 - val_loss: 0.6911 - val_accuracy: 0.5227\n","Epoch 4/20\n","76/76 [==============================] - 35s 467ms/step - loss: 0.6819 - accuracy: 0.5247 - val_loss: 0.6902 - val_accuracy: 0.5227\n","Epoch 5/20\n","76/76 [==============================] - 38s 497ms/step - loss: 0.6781 - accuracy: 0.5250 - val_loss: 0.6905 - val_accuracy: 0.4957\n","Epoch 6/20\n","76/76 [==============================] - 33s 434ms/step - loss: 0.6768 - accuracy: 0.5289 - val_loss: 0.6893 - val_accuracy: 0.5242\n","Epoch 7/20\n","76/76 [==============================] - 37s 488ms/step - loss: 0.6744 - accuracy: 0.5347 - val_loss: 0.7210 - val_accuracy: 0.5251\n","Epoch 8/20\n","76/76 [==============================] - 35s 462ms/step - loss: 0.6738 - accuracy: 0.5276 - val_loss: 0.6955 - val_accuracy: 0.5232\n","Epoch 9/20\n","76/76 [==============================] - 35s 456ms/step - loss: 0.6736 - accuracy: 0.5260 - val_loss: 0.7009 - val_accuracy: 0.5246\n","Epoch 10/20\n","76/76 [==============================] - 37s 483ms/step - loss: 0.6715 - accuracy: 0.5272 - val_loss: 0.7082 - val_accuracy: 0.5237\n","Epoch 11/20\n","76/76 [==============================] - 33s 432ms/step - loss: 0.6720 - accuracy: 0.5347 - val_loss: 0.7191 - val_accuracy: 0.4937\n","Epoch 12/20\n","76/76 [==============================] - 37s 480ms/step - loss: 0.6717 - accuracy: 0.5320 - val_loss: 0.7282 - val_accuracy: 0.4952\n","Epoch 13/20\n","76/76 [==============================] - 35s 458ms/step - loss: 0.6709 - accuracy: 0.5171 - val_loss: 0.7260 - val_accuracy: 0.5232\n","Epoch 14/20\n","76/76 [==============================] - 35s 464ms/step - loss: 0.6694 - accuracy: 0.5382 - val_loss: 0.7321 - val_accuracy: 0.5242\n","Epoch 15/20\n","76/76 [==============================] - 37s 481ms/step - loss: 0.6694 - accuracy: 0.5320 - val_loss: 0.7360 - val_accuracy: 0.5232\n","Epoch 16/20\n","76/76 [==============================] - 33s 436ms/step - loss: 0.6687 - accuracy: 0.5328 - val_loss: 0.7387 - val_accuracy: 0.5232\n","Epoch 17/20\n","76/76 [==============================] - 36s 475ms/step - loss: 0.6689 - accuracy: 0.5343 - val_loss: 0.7412 - val_accuracy: 0.5232\n","Epoch 18/20\n","76/76 [==============================] - 36s 469ms/step - loss: 0.6693 - accuracy: 0.5328 - val_loss: 0.7432 - val_accuracy: 0.5232\n","Epoch 19/20\n","76/76 [==============================] - 35s 460ms/step - loss: 0.6682 - accuracy: 0.5374 - val_loss: 0.7430 - val_accuracy: 0.5237\n","Epoch 20/20\n","76/76 [==============================] - 37s 486ms/step - loss: 0.6671 - accuracy: 0.5330 - val_loss: 0.7466 - val_accuracy: 0.5232\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f968d627df0\u003e"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_glove, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2467,"status":"ok","timestamp":1678533911134,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"gFgoILbtbLZ0","outputId":"ea723838-2d16-4bbf-a90e-2aa391acc174"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 32ms/step\n","precision: [0.5235732  0.50909091]\n","recall: [0.97504621 0.02834008]\n","fscore: [0.68130449 0.05369128]\n","support: [1082  988]\n","accuracy: 0.5231884057971015\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.98      0.68      1082\n","           1       0.51      0.03      0.05       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.52      0.50      0.37      2070\n","weighted avg       0.52      0.52      0.38      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMGz6yWfbLZ0"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgN2gkdRbLZ0"},"outputs":[],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"GjbnUKQGILED"},"source":["### TF-IDF + hashtags + Retweets + POS + bert"]},{"cell_type":"markdown","metadata":{"id":"QqHRI2aMRdpp"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267995,"status":"ok","timestamp":1678612672004,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"kkktjFGcbMD1","outputId":"d5b73cf4-57d8-4bb8-ae1f-e8cff6ddfd8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 45s 560ms/step - loss: 0.9012 - accuracy: 0.4926 - val_loss: 0.7384 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 42s 560ms/step - loss: 0.8043 - accuracy: 0.5109 - val_loss: 0.7097 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 43s 568ms/step - loss: 0.7556 - accuracy: 0.5212 - val_loss: 0.7708 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 43s 562ms/step - loss: 0.7193 - accuracy: 0.5361 - val_loss: 0.7665 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 40s 529ms/step - loss: 0.6868 - accuracy: 0.5670 - val_loss: 0.8257 - val_accuracy: 0.5227\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c688b7fd0\u003e"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_bert, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3626,"status":"ok","timestamp":1678612675609,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"XRFcqPdrbMD1","outputId":"dd34e178-04c0-4b05-f0d6-5274230f061f"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 2s 35ms/step\n","precision: [0.52270531 0.        ]\n","recall: [1. 0.]\n","fscore: [0.68654822 0.        ]\n","support: [1082  988]\n","accuracy: 0.5227053140096618\n","              precision    recall  f1-score   support\n","\n","           0       0.52      1.00      0.69      1082\n","           1       0.00      0.00      0.00       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.26      0.50      0.34      2070\n","weighted avg       0.27      0.52      0.36      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264274,"status":"ok","timestamp":1678612939872,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"KdScRtMxbMD1","outputId":"ec18f762-0be7-4725-8ca2-944b552c32e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 51s 624ms/step - loss: 0.6178 - accuracy: 0.6571 - val_loss: 0.6780 - val_accuracy: 0.7483\n","Epoch 2/5\n","76/76 [==============================] - 45s 593ms/step - loss: 0.4465 - accuracy: 0.7960 - val_loss: 0.6577 - val_accuracy: 0.7618\n","Epoch 3/5\n","76/76 [==============================] - 44s 579ms/step - loss: 0.3614 - accuracy: 0.8443 - val_loss: 0.6326 - val_accuracy: 0.7246\n","Epoch 4/5\n","76/76 [==============================] - 45s 592ms/step - loss: 0.2958 - accuracy: 0.8772 - val_loss: 0.6096 - val_accuracy: 0.7507\n","Epoch 5/5\n","76/76 [==============================] - 45s 589ms/step - loss: 0.2384 - accuracy: 0.9016 - val_loss: 0.5757 - val_accuracy: 0.7304\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f4c685ea940\u003e"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5599,"status":"ok","timestamp":1678612945449,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"P8GLPvVlbMD1","outputId":"a721ff5c-78b3-49cb-833d-c33e6e16d63a"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 56ms/step\n","precision: [0.69818457 0.78743316]\n","recall: [0.85304991 0.59615385]\n","fscore: [0.76788686 0.67857143]\n","support: [1082  988]\n","accuracy: 0.7304347826086957\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.85      0.77      1082\n","           1       0.79      0.60      0.68       988\n","\n","   micro avg       0.73      0.73      0.73      2070\n","   macro avg       0.74      0.72      0.72      2070\n","weighted avg       0.74      0.73      0.73      2070\n"," samples avg       0.73      0.73      0.73      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":461943,"status":"ok","timestamp":1678613606635,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"MltdCGmibMD1","outputId":"429eac43-40cb-4ed1-8f1e-ad9fffa8970a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 97s 1s/step - loss: 0.6197 - accuracy: 0.6513 - val_loss: 0.6782 - val_accuracy: 0.6995\n","Epoch 2/5\n","76/76 [==============================] - 88s 1s/step - loss: 0.4460 - accuracy: 0.7964 - val_loss: 0.6587 - val_accuracy: 0.6367\n","Epoch 3/5\n","76/76 [==============================] - 86s 1s/step - loss: 0.3495 - accuracy: 0.8552 - val_loss: 0.6382 - val_accuracy: 0.7285\n","Epoch 4/5\n","76/76 [==============================] - 86s 1s/step - loss: 0.2751 - accuracy: 0.8898 - val_loss: 0.6074 - val_accuracy: 0.7386\n","Epoch 5/5\n","76/76 [==============================] - 83s 1s/step - loss: 0.2150 - accuracy: 0.9157 - val_loss: 0.5773 - val_accuracy: 0.7256\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f5400f6bb50\u003e"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_bert, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6859,"status":"ok","timestamp":1678613613471,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"_R6Yb4kcbMD1","outputId":"997c1258-c559-4a10-f737-3b637e628b17"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 6s 71ms/step\n","precision: [0.80093677 0.67269737]\n","recall: [0.63216266 0.82793522]\n","fscore: [0.70661157 0.74228675]\n","support: [1082  988]\n","accuracy: 0.7256038647342995\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.63      0.71      1082\n","           1       0.67      0.83      0.74       988\n","\n","   micro avg       0.73      0.73      0.73      2070\n","   macro avg       0.74      0.73      0.72      2070\n","weighted avg       0.74      0.73      0.72      2070\n"," samples avg       0.73      0.73      0.73      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27096,"status":"ok","timestamp":1678618581728,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"3U78S23SbMD1","outputId":"c8d9575e-568e-476a-b2ae-3688f2928652"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 8s 64ms/step - loss: 0.6303 - accuracy: 0.6455 - val_loss: 0.5266 - val_accuracy: 0.7377\n","Epoch 2/5\n","76/76 [==============================] - 3s 45ms/step - loss: 0.4646 - accuracy: 0.7865 - val_loss: 0.5088 - val_accuracy: 0.7522\n","Epoch 3/5\n","76/76 [==============================] - 3s 46ms/step - loss: 0.4056 - accuracy: 0.8285 - val_loss: 0.5218 - val_accuracy: 0.7628\n","Epoch 4/5\n","76/76 [==============================] - 4s 48ms/step - loss: 0.3468 - accuracy: 0.8592 - val_loss: 0.5330 - val_accuracy: 0.7536\n","Epoch 5/5\n","76/76 [==============================] - 5s 60ms/step - loss: 0.3056 - accuracy: 0.8770 - val_loss: 0.5531 - val_accuracy: 0.7449\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f2038369bb0\u003e"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_RT_POS_bert, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1735,"status":"ok","timestamp":1678618583442,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"AoTs3mefbMD1","outputId":"159992cc-39fd-4e98-ba0c-8a944892f13e"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 13ms/step\n","precision: [0.78556701 0.70909091]\n","recall: [0.70425139 0.78947368]\n","fscore: [0.74269006 0.74712644]\n","support: [1082  988]\n","accuracy: 0.744927536231884\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.70      0.74      1082\n","           1       0.71      0.79      0.75       988\n","\n","   micro avg       0.74      0.74      0.74      2070\n","   macro avg       0.75      0.75      0.74      2070\n","weighted avg       0.75      0.74      0.74      2070\n"," samples avg       0.74      0.74      0.74      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"C03KyIFeRdpq"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":390904,"status":"ok","timestamp":1678614004373,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"s6bg2ZuIbMlX","outputId":"3c7113b3-910a-4ea4-e800-d0d98e01aa82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 76s 1s/step - loss: 0.8707 - accuracy: 0.5215 - val_loss: 0.6806 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 66s 998ms/step - loss: 0.4848 - accuracy: 0.7586 - val_loss: 2.4375 - val_accuracy: 0.4601\n","Epoch 3/5\n","66/66 [==============================] - 71s 1s/step - loss: 0.1842 - accuracy: 0.9478 - val_loss: 2.5569 - val_accuracy: 0.4601\n","Epoch 4/5\n","66/66 [==============================] - 65s 982ms/step - loss: 0.1508 - accuracy: 0.9594 - val_loss: 1.9678 - val_accuracy: 0.4601\n","Epoch 5/5\n","66/66 [==============================] - 74s 1s/step - loss: 0.1342 - accuracy: 0.9642 - val_loss: 0.8499 - val_accuracy: 0.4629\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f5400f6bd00\u003e"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_bert, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10930,"status":"ok","timestamp":1678614015279,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"o87hAVnZbMlX","outputId":"9ee093e6-31be-4789-9ce7-be2696a0de08"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 94ms/step\n","precision: [0.46141033 1.        ]\n","recall: [1.         0.00512821]\n","fscore: [0.63145897 0.01020408]\n","support: [831 975]\n","accuracy: 0.4629014396456257\n","              precision    recall  f1-score   support\n","\n","           0       0.46      1.00      0.63       831\n","           1       1.00      0.01      0.01       975\n","\n","   micro avg       0.46      0.46      0.46      1806\n","   macro avg       0.73      0.50      0.32      1806\n","weighted avg       0.75      0.46      0.30      1806\n"," samples avg       0.46      0.46      0.46      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":451423,"status":"ok","timestamp":1678614466678,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"l8WCJJ7GbMlX","outputId":"18087279-bc30-41a7-dc92-c9c8fe987650"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 88s 1s/step - loss: 0.3254 - accuracy: 0.8737 - val_loss: 0.5950 - val_accuracy: 0.7935\n","Epoch 2/5\n","66/66 [==============================] - 79s 1s/step - loss: 0.1078 - accuracy: 0.9675 - val_loss: 0.5501 - val_accuracy: 0.9147\n","Epoch 3/5\n","66/66 [==============================] - 82s 1s/step - loss: 0.0698 - accuracy: 0.9777 - val_loss: 0.4795 - val_accuracy: 0.9352\n","Epoch 4/5\n","66/66 [==============================] - 86s 1s/step - loss: 0.0486 - accuracy: 0.9829 - val_loss: 0.3908 - val_accuracy: 0.8937\n","Epoch 5/5\n","66/66 [==============================] - 77s 1s/step - loss: 0.0382 - accuracy: 0.9886 - val_loss: 0.3082 - val_accuracy: 0.9358\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f5401435430\u003e"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5295,"status":"ok","timestamp":1678614471949,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"cENweDJ-bMlX","outputId":"cda105e2-8d15-4c75-9a12-4851f01a6bdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 6s 87ms/step\n","precision: [0.96010296 0.91739553]\n","recall: [0.8977136  0.96820513]\n","fscore: [0.9278607  0.94211577]\n","support: [831 975]\n","accuracy: 0.9357696566998892\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.90      0.93       831\n","           1       0.92      0.97      0.94       975\n","\n","   micro avg       0.94      0.94      0.94      1806\n","   macro avg       0.94      0.93      0.93      1806\n","weighted avg       0.94      0.94      0.94      1806\n"," samples avg       0.94      0.94      0.94      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329316,"status":"ok","timestamp":1678614801242,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"M4wzf0TnbMlX","outputId":"9b70e606-43e7-4012-97d6-663433545106"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 81s 1s/step - loss: 0.3207 - accuracy: 0.8723 - val_loss: 0.6101 - val_accuracy: 0.8610\n","Epoch 2/5\n","66/66 [==============================] - 71s 1s/step - loss: 0.1080 - accuracy: 0.9653 - val_loss: 0.5391 - val_accuracy: 0.9214\n","Epoch 3/5\n","66/66 [==============================] - 58s 875ms/step - loss: 0.0701 - accuracy: 0.9784 - val_loss: 0.4730 - val_accuracy: 0.9601\n","Epoch 4/5\n","66/66 [==============================] - 54s 825ms/step - loss: 0.0518 - accuracy: 0.9843 - val_loss: 0.3941 - val_accuracy: 0.9535\n","Epoch 5/5\n","66/66 [==============================] - 58s 870ms/step - loss: 0.0358 - accuracy: 0.9896 - val_loss: 0.3923 - val_accuracy: 0.8068\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f54014a83d0\u003e"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_bert, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4780,"status":"ok","timestamp":1678614805991,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"WNFEwo1DbMlX","outputId":"6a3c2652-a044-49bf-c05e-6f426f1e0e14"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 5s 76ms/step\n","precision: [0.9898374  0.73820396]\n","recall: [0.58604091 0.99487179]\n","fscore: [0.73620559 0.84753167]\n","support: [831 975]\n","accuracy: 0.8067552602436323\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.59      0.74       831\n","           1       0.74      0.99      0.85       975\n","\n","   micro avg       0.81      0.81      0.81      1806\n","   macro avg       0.86      0.79      0.79      1806\n","weighted avg       0.85      0.81      0.80      1806\n"," samples avg       0.81      0.81      0.81      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26014,"status":"ok","timestamp":1678618612291,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Zs3WkqiwbMlX","outputId":"e8c3d3fa-c207-4335-981b-33a7c6650378"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 7s 54ms/step - loss: 0.3672 - accuracy: 0.8403 - val_loss: 0.1641 - val_accuracy: 0.9502\n","Epoch 2/5\n","66/66 [==============================] - 3s 47ms/step - loss: 0.1320 - accuracy: 0.9601 - val_loss: 0.1418 - val_accuracy: 0.9607\n","Epoch 3/5\n","66/66 [==============================] - 3s 44ms/step - loss: 0.0864 - accuracy: 0.9770 - val_loss: 0.1618 - val_accuracy: 0.9563\n","Epoch 4/5\n","66/66 [==============================] - 3s 52ms/step - loss: 0.0654 - accuracy: 0.9824 - val_loss: 0.1647 - val_accuracy: 0.9485\n","Epoch 5/5\n","66/66 [==============================] - 3s 47ms/step - loss: 0.0497 - accuracy: 0.9869 - val_loss: 0.1733 - val_accuracy: 0.9535\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f20302d1850\u003e"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_RT_POS_bert, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1205,"status":"ok","timestamp":1678618613472,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"-vXipw9cbMlX","outputId":"e0d0b9d8-bfa5-4766-bf17-dd47ab6c786b"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 16ms/step\n","precision: [0.94411415 0.96165803]\n","recall: [0.95547533 0.95179487]\n","fscore: [0.94976077 0.95670103]\n","support: [831 975]\n","accuracy: 0.9534883720930233\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.96      0.95       831\n","           1       0.96      0.95      0.96       975\n","\n","   micro avg       0.95      0.95      0.95      1806\n","   macro avg       0.95      0.95      0.95      1806\n","weighted avg       0.95      0.95      0.95      1806\n"," samples avg       0.95      0.95      0.95      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"ZR2twrSj75Au"},"source":["### HT + POS + TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"OENZJrjW8H7l"},"source":["#### Nepal"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":332023,"status":"ok","timestamp":1678615165004,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"NtL0jDH48H7m","outputId":"80c85ede-0dcb-4de7-bc2e-d2ccca2c72a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 63s 786ms/step - loss: 0.9669 - accuracy: 0.4910 - val_loss: 0.7918 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 60s 789ms/step - loss: 0.8557 - accuracy: 0.4984 - val_loss: 0.7451 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 64s 842ms/step - loss: 0.8162 - accuracy: 0.5074 - val_loss: 0.7516 - val_accuracy: 0.5227\n","Epoch 4/5\n","76/76 [==============================] - 63s 834ms/step - loss: 0.7994 - accuracy: 0.5049 - val_loss: 0.6928 - val_accuracy: 0.5227\n","Epoch 5/5\n","76/76 [==============================] - 63s 834ms/step - loss: 0.7819 - accuracy: 0.5024 - val_loss: 0.7302 - val_accuracy: 0.4773\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f540063a760\u003e"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_POS, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5510,"status":"ok","timestamp":1678615170492,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"RdTugR0f8H7n","outputId":"84db093c-f0c1-4bd4-c63d-51a59015d9f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 68ms/step\n","precision: [0.         0.47729469]\n","recall: [0. 1.]\n","fscore: [0.         0.64617397]\n","support: [1082  988]\n","accuracy: 0.47729468599033814\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00      1082\n","           1       0.48      1.00      0.65       988\n","\n","   micro avg       0.48      0.48      0.48      2070\n","   macro avg       0.24      0.50      0.32      2070\n","weighted avg       0.23      0.48      0.31      2070\n"," samples avg       0.48      0.48      0.48      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":388458,"status":"ok","timestamp":1678615558944,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"DhjTxsuI8H7n","outputId":"42ae5ee1-729b-46c6-cded-c53075fda9b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 78s 962ms/step - loss: 0.6928 - accuracy: 0.5256 - val_loss: 0.6936 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 71s 934ms/step - loss: 0.6813 - accuracy: 0.5635 - val_loss: 0.6914 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 72s 950ms/step - loss: 0.6754 - accuracy: 0.5749 - val_loss: 0.6892 - val_accuracy: 0.5275\n","Epoch 4/5\n","76/76 [==============================] - 72s 943ms/step - loss: 0.6712 - accuracy: 0.5796 - val_loss: 0.6871 - val_accuracy: 0.5242\n","Epoch 5/5\n","76/76 [==============================] - 69s 911ms/step - loss: 0.6688 - accuracy: 0.5869 - val_loss: 0.6852 - val_accuracy: 0.5242\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f54003b9970\u003e"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5008,"status":"ok","timestamp":1678615563926,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"QRBn_rbJ8H7n","outputId":"44deb84d-176b-4108-fc91-4c5c5c0f0113"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 5s 66ms/step\n","precision: [0.52376286 0.55172414]\n","recall: [0.98798521 0.01619433]\n","fscore: [0.68459814 0.03146509]\n","support: [1082  988]\n","accuracy: 0.5241545893719807\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.99      0.68      1082\n","           1       0.55      0.02      0.03       988\n","\n","   micro avg       0.52      0.52      0.52      2070\n","   macro avg       0.54      0.50      0.36      2070\n","weighted avg       0.54      0.52      0.37      2070\n"," samples avg       0.52      0.52      0.52      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":215423,"status":"ok","timestamp":1678615779327,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"qKqEBZhP8H7n","outputId":"53726688-c1b9-400c-b5fd-1607345a3da8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 46s 560ms/step - loss: 0.6925 - accuracy: 0.5330 - val_loss: 0.6914 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 41s 536ms/step - loss: 0.6756 - accuracy: 0.5730 - val_loss: 0.6921 - val_accuracy: 0.5227\n","Epoch 3/5\n","76/76 [==============================] - 42s 557ms/step - loss: 0.6690 - accuracy: 0.5751 - val_loss: 0.6897 - val_accuracy: 0.5232\n","Epoch 4/5\n","76/76 [==============================] - 43s 561ms/step - loss: 0.6648 - accuracy: 0.5831 - val_loss: 0.6866 - val_accuracy: 0.5246\n","Epoch 5/5\n","76/76 [==============================] - 41s 534ms/step - loss: 0.6621 - accuracy: 0.5912 - val_loss: 0.6847 - val_accuracy: 0.5256\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f53ee144d90\u003e"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_POS, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5840,"status":"ok","timestamp":1678615785145,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"RRcRlq2F8H7n","outputId":"f7dcd70e-e651-4bac-a7e4-68a8ec0602d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 4s 53ms/step\n","precision: [0.52439024 0.65      ]\n","recall: [0.9935305  0.01315789]\n","fscore: [0.68646232 0.02579365]\n","support: [1082  988]\n","accuracy: 0.5256038647342995\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.99      0.69      1082\n","           1       0.65      0.01      0.03       988\n","\n","   micro avg       0.53      0.53      0.53      2070\n","   macro avg       0.59      0.50      0.36      2070\n","weighted avg       0.58      0.53      0.37      2070\n"," samples avg       0.53      0.53      0.53      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25626,"status":"ok","timestamp":1678618649790,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"yiC2W1zy8H7o","outputId":"167b3b87-3337-4ccc-9179-0113a4eb0c8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","76/76 [==============================] - 7s 52ms/step - loss: 0.6926 - accuracy: 0.5189 - val_loss: 0.6917 - val_accuracy: 0.5227\n","Epoch 2/5\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6925 - accuracy: 0.5183 - val_loss: 0.6910 - val_accuracy: 0.5203\n","Epoch 3/5\n","76/76 [==============================] - 4s 55ms/step - loss: 0.6877 - accuracy: 0.5430 - val_loss: 0.6882 - val_accuracy: 0.5415\n","Epoch 4/5\n","76/76 [==============================] - 4s 46ms/step - loss: 0.6823 - accuracy: 0.5610 - val_loss: 0.6883 - val_accuracy: 0.5396\n","Epoch 5/5\n","76/76 [==============================] - 3s 45ms/step - loss: 0.6799 - accuracy: 0.5624 - val_loss: 0.6850 - val_accuracy: 0.5386\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f2020163670\u003e"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(NepalEQ_TFIDF_HT_POS, NepalEQ_df_target,test_size=0.3,stratify=NepalEQ_df_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1678618650753,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"E8oW7cOP8H7o","outputId":"3d2b97a0-136c-4081-a43c-242f1b92e44e"},"outputs":[{"name":"stdout","output_type":"stream","text":["65/65 [==============================] - 1s 15ms/step\n","precision: [0.55200655 0.51943463]\n","recall: [0.62292052 0.44635628]\n","fscore: [0.58532349 0.48013065]\n","support: [1082  988]\n","accuracy: 0.538647342995169\n","              precision    recall  f1-score   support\n","\n","           0       0.55      0.62      0.59      1082\n","           1       0.52      0.45      0.48       988\n","\n","   micro avg       0.54      0.54      0.54      2070\n","   macro avg       0.54      0.53      0.53      2070\n","weighted avg       0.54      0.54      0.54      2070\n"," samples avg       0.54      0.54      0.54      2070\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"markdown","metadata":{"id":"qMf-t7_n8H7o"},"source":["#### Queensland"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":205424,"status":"ok","timestamp":1678616000036,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"wiDMB39q8H7o","outputId":"d5142c86-20da-4055-e6d9-eb9875cb3288"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 37s 542ms/step - loss: 0.8780 - accuracy: 0.5039 - val_loss: 0.6930 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 38s 579ms/step - loss: 0.8377 - accuracy: 0.4968 - val_loss: 0.6907 - val_accuracy: 0.5399\n","Epoch 3/5\n","66/66 [==============================] - 34s 517ms/step - loss: 0.8006 - accuracy: 0.4932 - val_loss: 0.7077 - val_accuracy: 0.4601\n","Epoch 4/5\n","66/66 [==============================] - 37s 558ms/step - loss: 0.7792 - accuracy: 0.5087 - val_loss: 0.7190 - val_accuracy: 0.5399\n","Epoch 5/5\n","66/66 [==============================] - 35s 539ms/step - loss: 0.7633 - accuracy: 0.4951 - val_loss: 0.7039 - val_accuracy: 0.5399\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f53ec379c40\u003e"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["###### CNN\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_POS, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_dropout1 = Dropout(0.5, name='text_dropout1')(text_embed)\n","text_Conv1 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv1')(text_dropout1)\n","text_pool1 = MaxPooling1D(pool_size=5, name= 'text_pool1')(text_Conv1)\n","text_dropout2 = Dropout(0.5, name='text_dropout2')(text_pool1)\n","text_batchnorm1 = BatchNormalization(name='text_batchnorm1')(text_dropout2)\n","text_Conv2 = Conv1D(128, kernel_size=5, activation='relu', name='text_Conv2')(text_batchnorm1)\n","text_pool2 = MaxPooling1D(pool_size=5, name='text_pool2')(text_Conv2)\n","text_dropout3 = Dropout(0.5, name='text_dropout3')(text_pool2)\n","text_batchnorm2 = BatchNormalization(name='text_batchnorm2')(text_dropout3)\n","text_flatten = Flatten(name='text_flatten')(text_batchnorm2)\n","text_dense = Dense(128, activation='relu', name='text_dense')(text_flatten)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dense)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","# model.fit(x=[X_text_train, X_image_train], y=[y_train], \n","#           validation_data=([X_text_test, X_image_test], y_test), \n","#           epochs=2, batch_size=batch_size)\n","\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5288,"status":"ok","timestamp":1678616005297,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"4Bq_uROk8H7o","outputId":"5cabe3e0-c08d-4273-d95e-bc6af06af8f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 3s 56ms/step\n","precision: [0.         0.53986711]\n","recall: [0. 1.]\n","fscore: [0.         0.70118662]\n","support: [831 975]\n","accuracy: 0.5398671096345515\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00       831\n","           1       0.54      1.00      0.70       975\n","\n","   micro avg       0.54      0.54      0.54      1806\n","   macro avg       0.27      0.50      0.35      1806\n","weighted avg       0.29      0.54      0.38      1806\n"," samples avg       0.54      0.54      0.54      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":205596,"status":"ok","timestamp":1678616210878,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"mYez-dnm8H7o","outputId":"7ffe7c43-98a3-4f79-b797-d350f833a5d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 39s 541ms/step - loss: 0.6616 - accuracy: 0.5946 - val_loss: 0.6860 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 37s 561ms/step - loss: 0.6055 - accuracy: 0.6596 - val_loss: 0.6812 - val_accuracy: 0.6489\n","Epoch 3/5\n","66/66 [==============================] - 39s 586ms/step - loss: 0.5906 - accuracy: 0.6741 - val_loss: 0.6750 - val_accuracy: 0.5399\n","Epoch 4/5\n","66/66 [==============================] - 38s 572ms/step - loss: 0.5851 - accuracy: 0.6834 - val_loss: 0.6667 - val_accuracy: 0.6606\n","Epoch 5/5\n","66/66 [==============================] - 38s 574ms/step - loss: 0.5791 - accuracy: 0.6860 - val_loss: 0.6565 - val_accuracy: 0.5764\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f53ec0477c0\u003e"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["####### LSTM\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_LSTM = LSTM(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_LSTM')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_LSTM)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4001,"status":"ok","timestamp":1678616214855,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"LzC0TBYb8H7p","outputId":"838b2ccf-bf1c-44be-dd0b-3d099b5017c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 4s 64ms/step\n","precision: [0.79464286 0.56198347]\n","recall: [0.10709988 0.97641026]\n","fscore: [0.18875928 0.7133758 ]\n","support: [831 975]\n","accuracy: 0.5764119601328903\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.11      0.19       831\n","           1       0.56      0.98      0.71       975\n","\n","   micro avg       0.58      0.58      0.58      1806\n","   macro avg       0.68      0.54      0.45      1806\n","weighted avg       0.67      0.58      0.47      1806\n"," samples avg       0.58      0.58      0.58      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208117,"status":"ok","timestamp":1678616422966,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"KQwJGbYL8H7p","outputId":"1dbfce3b-333e-4e66-f074-93f3fab2922c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 39s 554ms/step - loss: 0.6525 - accuracy: 0.6046 - val_loss: 0.6836 - val_accuracy: 0.5399\n","Epoch 2/5\n","66/66 [==============================] - 34s 517ms/step - loss: 0.5997 - accuracy: 0.6689 - val_loss: 0.6771 - val_accuracy: 0.5703\n","Epoch 3/5\n","66/66 [==============================] - 36s 547ms/step - loss: 0.5912 - accuracy: 0.6746 - val_loss: 0.6727 - val_accuracy: 0.6700\n","Epoch 4/5\n","66/66 [==============================] - 33s 501ms/step - loss: 0.5848 - accuracy: 0.6850 - val_loss: 0.6636 - val_accuracy: 0.5731\n","Epoch 5/5\n","66/66 [==============================] - 36s 541ms/step - loss: 0.5813 - accuracy: 0.6800 - val_loss: 0.6532 - val_accuracy: 0.6694\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f54006e6610\u003e"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["####### GRU - 2DCNN\n","\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_POS, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_GRU = GRU(32, input_shape=(None,1), activation='relu', recurrent_activation='hard_sigmoid', return_sequences=True, name='text_GRU')(text_embed)\n","text_dropout1 = Dropout(0.2, name='text_dropout1')(text_GRU)\n","text_norm = BatchNormalization(name='text_norm')(text_dropout1)\n","text_flatten = Flatten(name='text_flatten')(text_norm)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_flatten)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":763,"status":"ok","timestamp":1678616423714,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"TcObu-hM8H7p","outputId":"e25b0ffb-3d45-48db-9742-e6f08b175a7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 2s 34ms/step\n","precision: [0.72941176 0.64583333]\n","recall: [0.44765343 0.85846154]\n","fscore: [0.55480984 0.73712021]\n","support: [831 975]\n","accuracy: 0.6694352159468439\n","              precision    recall  f1-score   support\n","\n","           0       0.73      0.45      0.55       831\n","           1       0.65      0.86      0.74       975\n","\n","   micro avg       0.67      0.67      0.67      1806\n","   macro avg       0.69      0.65      0.65      1806\n","weighted avg       0.68      0.67      0.65      1806\n"," samples avg       0.67      0.67      0.67      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21727,"status":"ok","timestamp":1678619649681,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"Vhj4pKh78H7p","outputId":"dc90e155-fc36-4126-9df2-d4bd013898c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","66/66 [==============================] - 7s 70ms/step - loss: 0.6881 - accuracy: 0.5471 - val_loss: 0.6786 - val_accuracy: 0.5670\n","Epoch 2/5\n","66/66 [==============================] - 3s 43ms/step - loss: 0.6640 - accuracy: 0.6022 - val_loss: 0.6346 - val_accuracy: 0.6451\n","Epoch 3/5\n","66/66 [==============================] - 3s 47ms/step - loss: 0.6346 - accuracy: 0.6411 - val_loss: 0.6215 - val_accuracy: 0.6617\n","Epoch 4/5\n","66/66 [==============================] - 3s 46ms/step - loss: 0.6271 - accuracy: 0.6558 - val_loss: 0.6214 - val_accuracy: 0.6672\n","Epoch 5/5\n","66/66 [==============================] - 4s 59ms/step - loss: 0.6225 - accuracy: 0.6610 - val_loss: 0.6183 - val_accuracy: 0.6700\n"]},{"data":{"text/plain":["\u003ckeras.callbacks.History at 0x7f1f68251340\u003e"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import *\n","from keras.utils.np_utils import to_categorical\n","from keras.initializers import Constant\n","import re\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Import Libraries\n","import sys, os, re, csv, codecs, numpy as np, pandas as pd\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n","from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras import initializers, regularizers, constraints, optimizers, layers\n","from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n","from keras.layers import Conv1D, MaxPooling1D, Embedding\n","from keras.models import Sequential\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","# split the data into train and test sets\n","X_text_train, X_text_test, y_train, y_test =train_test_split(QueenslandFLD_TFIDF_HT_POS, QueenslandFLD_target,test_size=0.3,stratify=QueenslandFLD_target)\n","\n","num_classes = 2\n","\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","MAX_NB_WORDS = 200000 # Maximum size of the vocabulary\n","MAX_SEQUENCE_LENGTH = 300 # Maximum length of the text sequence\n","EMBEDDING_DIM = 100 # Size of the word embeddings\n","NUM_CLASSES = 2 # Number of unique classes\n","\n","# pad the text sequences\n","X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n","X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","\n","# define the text input layer\n","text_input = Input(shape=(MAX_SEQUENCE_LENGTH,), name='text_input')\n","text_embed = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, name='text_embedding')(text_input)\n","text_spdropout = SpatialDropout1D(0.2, name='text_spdropout')(text_embed)\n","text_bilstm1 = Bidirectional(CuDNNLSTM(64, return_sequences=True), name='text_bilstm1')(text_spdropout)\n","text_bilstm2 = Bidirectional(CuDNNLSTM(32), name='text_bilstm2')(text_bilstm1)\n","text_dropout1 = Dropout(0.25, name='text_dropout')(text_bilstm2)\n","output = Dense(NUM_CLASSES, activation='softmax', name='output')(text_dropout1)\n","\n","# define the model\n","model = Model(inputs=[text_input], outputs=[output])\n","\n","# set batch size\n","batch_size = 64\n","\n","# initiate RMSprop optimizer\n","opt = keras.optimizers.legacy.RMSprop(lr=0.0005, decay=1e-6)\n","\n","# Let's train the model using RMSprop\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=['accuracy'])\n","\n","\n","# train the model\n","model.fit(x=np.array(X_text_train), y=np.array(y_train), \n","          validation_data=(np.array(X_text_test), np.array(y_test)),\n","          epochs=5, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":911,"status":"ok","timestamp":1678619650573,"user":{"displayName":"Musa Phiri","userId":"10907548191956596892"},"user_tz":-120},"id":"hg-7kgKF8H7p","outputId":"b8d8ea89-5a0f-4d6d-dfb6-149a2afdd338"},"outputs":[{"name":"stdout","output_type":"stream","text":["57/57 [==============================] - 1s 13ms/step\n","precision: [0.67563528 0.66666667]\n","recall: [0.54392298 0.7774359 ]\n","fscore: [0.60266667 0.71780303]\n","support: [831 975]\n","accuracy: 0.6699889258028793\n","              precision    recall  f1-score   support\n","\n","           0       0.68      0.54      0.60       831\n","           1       0.67      0.78      0.72       975\n","\n","   micro avg       0.67      0.67      0.67      1806\n","   macro avg       0.67      0.66      0.66      1806\n","weighted avg       0.67      0.67      0.66      1806\n"," samples avg       0.67      0.67      0.67      1806\n","\n"]}],"source":["predicted=model.predict(np.array(X_text_test))\n","\n","import sklearn\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n","\n","predicted_labels = predicted.round()  # Round the predicted values to get binary predictions\n","precision, recall, fscore, support = score(np.array(y_test), predicted_labels)\n","accuracy = accuracy_score(np.array(y_test), predicted_labels)\n","\n","print('precision: {}'.format(precision))\n","print('recall: {}'.format(recall))\n","print('fscore: {}'.format(fscore))\n","print('support: {}'.format(support))\n","print('accuracy: {}'.format(accuracy))\n","\n","print(sklearn.metrics.classification_report(np.array(y_test), predicted.round()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LA-vQW-L8CqS"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOmCwoQBYh5qMsEtMKfdFZf","name":"","provenance":[{"file_id":"1x7f9TEtF-_7_Ah6nuh7mf2gDwEg6Me18","timestamp":1677065638606}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}